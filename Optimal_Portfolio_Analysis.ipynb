{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e3c8cb",
   "metadata": {},
   "source": [
    "# ETF Portfolio Optimization & Dynamic Asset Allocation\n",
    "\n",
    "**Last updated:** 2025-08-03\n",
    "\n",
    "This script provides a deep dive into portfolio optimization and aims to answer one key question: *What portfolio of ETFs should I invest in?*  Financial theory offers guidance on how to approach this problem, and here we apply that theory to a selection of Vanguard ETFs to determine what long-only strategy an investor should follow—assuming she is willing to take on the same level of risk as the overall market.\n",
    "\n",
    "But what is the “overall market” portfolio?  For our purposes, the benchmark will be the Vanguard ETF tracking the S&P 500, [VOO](https://investor.vanguard.com/investment-products/etfs/profile/voo), one of the largest ETFs in the world.\n",
    "\n",
    "We will walk through how to collect relevant data from Yahoo Finance, how to estimate mean returns and covariances of ETF returns, and how to construct the mean-variance efficient frontier.  We will also explore the constraints needed to make these strategies viable for retail investors, and discuss Bayesian-style techniques that can help us estimate inputs more robustly.\n",
    "\n",
    "Next, we introduce several advanced portfolio optimization techniques—including Black-Litterman, Hierarchical Risk Parity (HRP), DCC-GARCH, and Markov regime-switching models—and use them to re-estimate optimal portfolios that match VOO’s risk.  We then compare their performance both in-sample and through simulation.\n",
    "\n",
    "For the two best-performing methods in-sample—the simple mean-variance optimization and the Markov regime-switching model—we conduct an out-of-sample backtest to evaluate their real-world performance.  This leads us to a perhaps surprising conclusion: **the best-performing portfolio is found through simple mean-variance optimization**.  This is not only encouraging for practitioners, but also means we can offer a simple, intuitive tool to help investors choose the optimal portfolio based on their individual risk and return preferences.\n",
    "\n",
    "Even more interesting: the optimal portfolio almost always turns out to be a combination of a high-risk Growth ETF and a Treasury ETF.  This outcome is consistent with a classic insight from financial theory—**diversification works best when blending assets with low correlation**, which is precisely what we find here.\n",
    "\n",
    "I hope you enjoy the journey outlined in this notebook as much as I enjoyed writing it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ae96f9",
   "metadata": {},
   "source": [
    "### 1. Setup & Configuration\n",
    "\n",
    "The first step is to import required Python libraries, set global display options, and defines helper functions used throughout the analysis.  I will also disable some warnings that otherwise are distracting.  \n",
    "\n",
    "There is a lot we will do in this scrip, so there are a lot of libraries to load!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f7d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library Imports ---\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# --- Third-Party Library Imports ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxopt as opt\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import statsmodels.api as sm\n",
    "from scipy.optimize import minimize\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, leaves_list\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from arch.univariate import ConstantMean, GARCH\n",
    "from statsmodels.stats.diagnostic import het_arch\n",
    "from statsmodels.tsa.regime_switching.markov_regression import MarkovRegression\n",
    "from pandas_datareader.data import DataReader\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "from arch.univariate.base import DataScaleWarning, ConvergenceWarning \n",
    "import plotly.graph_objects as go\n",
    "from ipywidgets import FloatSlider, VBox, HTML, Button, Dropdown, HBox\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# --- Global Settings & Constants ---\n",
    "# Set the working directory.\n",
    "# NOTE: You may need to change this path to your project's root directory.\n",
    "DIRECTORY = \".\"\n",
    "os.chdir(DIRECTORY)\n",
    "\n",
    "# Set the seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Analysis Period\n",
    "ANALYSIS_YEARS = 15\n",
    "\n",
    "# Optimization & Simulation Parameters\n",
    "FRONTIER_POINTS = 50  # Number of points to calculate on the efficient frontier.\n",
    "MC_SIM_SCENARIOS = 10000  # Number of scenarios for Monte Carlo simulation.\n",
    "MC_SIM_HORIZON_MONTHS = 120  # 10-year horizon for simulation.\n",
    "RESAMPLE_ITERATIONS = 100  # Number of bootstrap iterations for resampled frontier.\n",
    "ROLLING_WINDOW_MONTHS = 60  # 5-year rolling window for dynamic weight analysis.\n",
    "\n",
    "# Regime Modeling Parameters\n",
    "MIN_OBS_PER_REGIME = 6  # Minimum data points required to consider a regime valid.\n",
    "MAX_REGIMES_TO_TEST = 4  # Test models with 2 up to this number of regimes.\n",
    "\n",
    "# --- Initial Setup ---\n",
    "# Configure pandas for better display\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.3f}\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Configure the CVXOPT solver to not display progress messages\n",
    "opt.solvers.options[\"show_progress\"] = False\n",
    "\n",
    "# ─── Warning filters ───\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)            # generic statsmodels\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)     # sklearn / statsmodels\n",
    "warnings.filterwarnings(\"ignore\", category=DataScaleWarning)       # ARCH data-scale\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)         # ARCH convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a544782",
   "metadata": {},
   "source": [
    "### 2. Data Loading & Preparation\n",
    "\n",
    "What we will do in this section:\n",
    "* **ETF universe:** Scrape the latest list of Vanguard ETFs.\n",
    "* **Historical prices:** Retrieve closing prices from Yahoo Finance via [`yfinance`](https://ranaroussi.github.io/yfinance/).\n",
    "* **Return series:** Compute monthly returns and calculate mean and covariances.\n",
    "* **Diversification** We will introduce the concept of diversification, which will provide guidance for the rest of the analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c26d18",
   "metadata": {},
   "source": [
    "We will need a systematic way to identify ETFs we can invest into.  Luckily, Vanguard is one of the leading ETF brokers and provides a great list of available ETFs [here](https://advisors.vanguard.com/investments/etfs).  We can just download the ETF list as csv-file, or we can scrape the table from the website directly.  I go with the latter solution so I always have the latest ETF list ready for the program.\n",
    "\n",
    "The approach is quite straightforward: we use a webdriver to identify the table on the website, collect the relevant cells in the table via their HTML tags (in our case, ticker, ETF name, and expense ration), and click to the next page to do the same again.  You only need to make sure to add the path to the webdriver, in my case chromedriver. \n",
    "\n",
    "Web scraping is a useful skill to know, but we are focused on portfolio optimization.  Check out one of the many tutorials on working with Selenium to perform task similar to the one below, for example the [ScrapingBee Tutorial](https://www.scrapingbee.com/blog/selenium-python/) or [Selenium with Python](https://selenium-python.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d642df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Selenium to scrape the Vanguard advisor site for an up-to-date list of\n",
    "# tickers, names, and expense ratios.\n",
    "def extract_etf_data_from_page(driver_instance):\n",
    "    \"\"\"\n",
    "    Extracts ETF data (Symbol, Name, Expense Ratio) from the currently\n",
    "    viewed table on the Vanguard website.\n",
    "\n",
    "    Args:\n",
    "        driver_instance: The active Selenium WebDriver instance.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary represents an ETF.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        table_body = driver_instance.find_element(By.CSS_SELECTOR, \"table tbody\")\n",
    "        rows = table_body.find_elements(By.TAG_NAME, \"tr\")\n",
    "        for row in rows:\n",
    "            try:\n",
    "                cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                symbol = cells[0].text.strip()\n",
    "                # Fund name is in a nested div; clean up extraneous text like \"NEW FUND\"\n",
    "                raw_name = row.find_elements(By.TAG_NAME, \"div\")[1].text.strip()\n",
    "                fund_name = re.sub(\n",
    "                    r\"\\s*(NEW FUND)?\\s*$\", \"\", raw_name.replace(\"\\n\", \" \")\n",
    "                ).strip()\n",
    "                # Expense ratio is in the 8th column; clean and convert to float\n",
    "                expense_text = cells[7].text.strip().replace(\"%\", \"\").strip()\n",
    "                expense_ratio = float(expense_text) / 100 if expense_text else None\n",
    "                data.append(\n",
    "                    {\n",
    "                        \"Symbol\": symbol,\n",
    "                        \"Fund name\": fund_name,\n",
    "                        \"Expense ratio\": expense_ratio,\n",
    "                    }\n",
    "                )\n",
    "            except (IndexError, ValueError):\n",
    "                # Skip row if any element is missing or fails to parse\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting table data: {e}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"Scraping ETF data from Vanguard website...\")\n",
    "    # Setup Selenium Chrome driver in headless mode (no visible browser window)\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "    # IMPORTANT: Update this path to your local chromedriver executable.\n",
    "    # For better portability, consider using webdriver-manager:\n",
    "    # from webdriver_manager.chrome import ChromeDriverManager\n",
    "    # service = Service(ChromeDriverManager().install())\n",
    "    chrome_service = Service(executable_path=\"/Users/dominikjurek/Library/CloudStorage/Dropbox/Personal/Investment/chromedriver\")\n",
    "    \n",
    "    driver = webdriver.Chrome(service=chrome_service, options=chrome_options)\n",
    "\n",
    "    # Load the Vanguard ETF page and wait for it to render\n",
    "    driver.get(\"https://advisors.vanguard.com/investments/etfs\")\n",
    "    time.sleep(6)  # Allow time for JavaScript to load the table\n",
    "\n",
    "    # Extract data from the first page\n",
    "    all_etf_data = extract_etf_data_from_page(driver)\n",
    "\n",
    "    # Click the \"Next page\" button to load the remaining ETFs\n",
    "    try:\n",
    "        next_button = driver.find_element(\n",
    "            By.XPATH, '//button[@aria-label[contains(., \"Forward one page\")]]'\n",
    "        )\n",
    "        next_button.click()\n",
    "        time.sleep(5)  # Wait for the second page to load\n",
    "        all_etf_data += extract_etf_data_from_page(driver)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not navigate to the second page (or it doesn't exist): {e}\")\n",
    "\n",
    "    # Process the scraped data\n",
    "    df_etf_metadata = pd.DataFrame(all_etf_data)\n",
    "    etf_name_map = dict(zip(df_etf_metadata[\"Symbol\"], df_etf_metadata[\"Fund name\"]))\n",
    "    etf_expense_map = dict(\n",
    "        zip(df_etf_metadata[\"Symbol\"], df_etf_metadata[\"Expense ratio\"])\n",
    "    )\n",
    "    etf_symbols = list(etf_name_map.keys())\n",
    "    print(f\"Successfully extracted metadata for {len(etf_symbols)} ETFs.\")\n",
    "    print(\"Sample of extracted ETF data:\")\n",
    "    print(df_etf_metadata.head())\n",
    "except Exception as e:\n",
    "    # If scraping fails, fall back to a predefined list of core ETFs\n",
    "    print(f\"Could not complete web scraping. Reason: {e}\")\n",
    "    print(\"Falling back to a predefined list of core ETFs.\")\n",
    "    etf_symbols = [\n",
    "        \"VOO\", \"VTI\", \"VEA\", \"VWO\", \"BND\", \"BNDX\", \"VGIT\", \"VGLT\", \"VTIP\", \"MUB\",\n",
    "    ]\n",
    "    etf_name_map = {s: s for s in etf_symbols}\n",
    "    etf_expense_map = {s: 0.0003 for s in etf_symbols}  # Use a reasonable default\n",
    "finally:\n",
    "    if \"driver\" in locals() and driver:\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d6033",
   "metadata": {},
   "source": [
    "I want to make sure my investment strategy is supported by broadly diversified ETFs, so I manually remove all industry-specific ETFs.  This is not really necessary to run the code or identify optimal portfolios, especially since the industry indices are themselves relatively well diversified across dozen or even hundreds of stocks.  My concern is rather that a particularly well performing sector over a few year, such as IT, will dominate the optimal portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a diversified portfolio of broad asset classes, we remove\n",
    "# specialized, sector-specific ETFs and redundant funds.\n",
    "industry_keywords = [\n",
    "    \"Energy\", \"Health Care\", \"Consumer\", \"Materials\", \"Financials\",\n",
    "    \"Utilities\", \"Real Estate\", \"Industrials\", \"Communication\", \"Information Technology\",\n",
    "]\n",
    "# List of specific ETFs to remove (often sector-focused or overlapping)\n",
    "remove_symbols = [\n",
    "    \"VGT\", \"VHT\", \"VPU\", \"VDC\", \"VAW\", \"VIS\", \"VFH\", \"VNQ\", \"VOX\", \"VDE\", \"VCR\",\n",
    "]\n",
    "\n",
    "\n",
    "def is_industry_or_redundant(symbol, name_map):\n",
    "    \"\"\"Checks if an ETF is sector-specific or on the removal list.\"\"\"\n",
    "    name = name_map.get(symbol, \"\")\n",
    "    is_industry = any(keyword in name for keyword in industry_keywords)\n",
    "    is_redundant = symbol in remove_symbols\n",
    "    return is_industry or is_redundant\n",
    "\n",
    "\n",
    "etf_symbols = [s for s in etf_symbols if not is_industry_or_redundant(s, etf_name_map)]\n",
    "etf_symbols = list(dict.fromkeys(etf_symbols))  # Ensure unique symbols\n",
    "print(f\"\\nFiltered down to {len(etf_symbols)} ETFs for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23445f",
   "metadata": {},
   "source": [
    "Now it is time to download the price history for each of the ETFs from Yahoo Finance via [`yfinance`](https://ranaroussi.github.io/yfinance/).  We focus on data up to the last full month-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57602d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_total_return_series(ticker):\n",
    "    \"\"\"\n",
    "    Fetches maximum available historical prices for a ticker from Yahoo Finance,\n",
    "    adjusted for dividends and splits to represent total return.\n",
    "\n",
    "    Args:\n",
    "        ticker (str): The stock or ETF symbol.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame of historical adjusted closing prices.\n",
    "                      Returns an empty DataFrame on failure.\n",
    "    \"\"\"\n",
    "    print(f\"Downloading data for {ticker}...\")\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        # 'back_adjust=True' provides a total return series by adjusting historical\n",
    "        # prices for both dividends and stock splits. 'auto_adjust=False' is required.\n",
    "        df = stock.history(period=\"max\", auto_adjust=False, back_adjust=True)[\n",
    "            [\"Close\"]\n",
    "        ].rename(columns={\"Close\": ticker})\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Could not fetch data for {ticker}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Combine all price series into one DataFrame\n",
    "all_prices_list = [get_total_return_series(ticker) for ticker in etf_symbols]\n",
    "all_prices = pd.concat([df for df in all_prices_list if not df.empty], axis=1)\n",
    "\n",
    "# Standardize the index to datetime objects without timezone information\n",
    "all_prices.index = pd.to_datetime(all_prices.index).tz_localize(None)\n",
    "\n",
    "# Resample daily prices to month-end, then calculate monthly percentage returns\n",
    "returns_monthly = all_prices.resample(\"ME\").last().pct_change()\n",
    "\n",
    "# Drop the last row if it's from the current (incomplete) month\n",
    "last_date = returns_monthly.index[-1]\n",
    "today = pd.Timestamp.today()\n",
    "\n",
    "# Check if last observation is in the current month and year\n",
    "if last_date.month == today.month and last_date.year == today.year:\n",
    "    returns_monthly = returns_monthly.iloc[:-1]\n",
    "\n",
    "# Limit data to the last N years for a more relevant analysis window\n",
    "cutoff_date = returns_monthly.index.max() - pd.DateOffset(years=ANALYSIS_YEARS)\n",
    "returns_monthly = returns_monthly[returns_monthly.index > cutoff_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240a44e9",
   "metadata": {},
   "source": [
    "We will be using monthly observations for the most part, but we also need to remove NA values from the return table.  This is tricky, since the length of the ETF price history differs quite substantially with some ET like the Total Treasury ETF (VTG) having no observations at all as it was created just recently.  Dropping all rows with NA values thus is not a solution.  Instead, we can require ETFs to have a minimum number of observations, in this case 10 years of data.  Since the ETF list on the website only contains currently available ETFs, we will drop all ETFs that do not have at least 10 years of recent history.  We lose around half of our initial ETF list, which is one of the main critiques of portfolio optimization: you need enough price history and basically have to exclude assets for which you cannot estimate covariances and expected returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6606a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning:\n",
    "# Drop ETFs that do not have at least 10 years of non-NA observations\n",
    "MIN_OBSERVATIONS = 10 * 12\n",
    "returns_monthly = returns_monthly.dropna(axis=1, thresh=MIN_OBSERVATIONS)\n",
    "\n",
    "# Drop any month (row) that still has missing values\n",
    "returns_monthly = returns_monthly.dropna(axis=0)\n",
    "\n",
    "# Update final list of ETFs and related data based on the cleaned DataFrame\n",
    "etf_symbols = returns_monthly.columns.tolist()\n",
    "\n",
    "# The S&P 500 ETF (VOO) is our primary benchmark; it's required for the analysis.\n",
    "if \"VOO\" not in etf_symbols:\n",
    "    raise ValueError(\n",
    "        \"VOO data is missing or was dropped. It is required for benchmark comparison.\"\n",
    "    )\n",
    "\n",
    "# Create a NumPy array of expense ratios in the same order as our final ETF symbols\n",
    "expense_vector = np.array([etf_expense_map.get(sym, 0.0) for sym in etf_symbols])\n",
    "\n",
    "print(\n",
    "    f\"\\nFinal analysis will use {len(etf_symbols)} ETFs over {len(returns_monthly)} months.\"\n",
    ")\n",
    "print(\n",
    "    f\"Analysis period: {returns_monthly.index.min().date()} to {returns_monthly.index.max().date()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfa58be",
   "metadata": {},
   "source": [
    "Now we can explore the statistical properties of the ETFs we have.  With enough data, the [Central Limit Theorem](https://www.probabilitycourse.com/chapter7/7_1_2_central_limit_theorem.php) implies that the sampling distribution of the sample mean of returns approaches a normal distribution, which can then be described by its mean and standard deviation, so let's have a look at the means and volatilities, i.e., the standard deviations of our ETFs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c6e147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate annualized mean return and standard deviation\n",
    "mean_returns = returns_monthly.mean() * 12  # 12 months in a year\n",
    "volatility = returns_monthly.std() * np.sqrt(12)\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(volatility, mean_returns, color='steelblue', edgecolors='black', s=100, alpha=0.8)\n",
    "\n",
    "# Annotate each ETF\n",
    "for i, symbol in enumerate(etf_symbols):\n",
    "    plt.annotate(symbol,\n",
    "                 (volatility.iloc[i], mean_returns.iloc[i]),\n",
    "                 textcoords=\"offset points\",\n",
    "                 xytext=(5, 5),\n",
    "                 ha='left', fontsize=9)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.title(\"ETF Mean-Variance Diagram (Annualized)\", fontsize=16)\n",
    "plt.xlabel(\"Annualized Volatility (Standard Deviation)\", fontsize=12)\n",
    "plt.ylabel(\"Annualized Mean Return\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9525b3b7",
   "metadata": {},
   "source": [
    "There is quite a big variation between ETFs.  Some ETFs have very low historic returns, like VCIT, the intermediate corporate bonds ETFs, but also low volatility.  That means, these ETFs are expected to have a small put relatively consistent return over time.  On the other hand, ETFs like MGK, Mega Cap Growth, have experienced very high returns over the past 10 year (more than 17% p.a. on average!), but they were also enormously volatile and risky.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f760aaf1",
   "metadata": {},
   "source": [
    "The individual volatility of assets is only partially useful.  What is more interesting is how assets co-move with other assets over time.  So, let us have a look at the covariance between ETFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute covariance matrix (annualized)\n",
    "covariance_matrix = returns_monthly.cov() * 12\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(covariance_matrix,\n",
    "            annot=False, fmt=\".3f\",\n",
    "            cmap=\"YlGnBu\",\n",
    "            xticklabels=etf_symbols,\n",
    "            yticklabels=etf_symbols,\n",
    "            square=True, linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": 0.75})\n",
    "\n",
    "plt.title(\"Annualized Covariance Matrix of ETF Returns\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea8620e",
   "metadata": {},
   "source": [
    "The covariance is quite widespread, just like the variance in the diagonal.  Looking at the matrix, we actually can understand why portfolio optimization (and asset pricing in general) is focused on the covariance of assets.  There are only 55 elements in the diagonal (the variance of assets), but 55*54 elements everywhere else representing how each asset co-moves with each other assets.  So, it is much more important how asset co-move with other assets rather than how volatile it is by itself, especially when we combine assets to portfolio. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbee7eef",
   "metadata": {},
   "source": [
    "Covariances are less intuitive than correlations, especially since correlations are bound between -1 and 1.  So, let's also have a look at the correlation matrix of the ETFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91eeb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "correlation_matrix = returns_monthly.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix,\n",
    "            annot=False, fmt=\".2f\",\n",
    "            cmap=\"coolwarm\", center=0,\n",
    "            xticklabels=etf_symbols,\n",
    "            yticklabels=etf_symbols,\n",
    "            square=True, linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": 0.75})\n",
    "\n",
    "plt.title(\"Correlation Matrix of Monthly ETF Returns\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb935e09",
   "metadata": {},
   "source": [
    "We already know assets have varying degrees of correlation from the covariance matrix, but the correlation matrix tells us one additional fact: no assets are perfectly correlated with each other (they would have a deep red color like the diagonal with correlation coefficient 1).  \n",
    "\n",
    "This means that combinations of assets will have a lower variance than the weighted average of the variances.  To see why, consider the standard deviation of ETF 1 and ETF 2, $\\sigma_1$ and $\\sigma_2$, and portfolio weights $w_1$ and $w_2$, where the assets are not perfectly correlated.\n",
    "\n",
    "The portfolio variance of two assets is given by:\n",
    "\n",
    "$$\n",
    "\\sigma_p^2 = w_1^2 \\sigma_1^2 + w_2^2 \\sigma_2^2 + 2w_1 w_2 \\rho_{12} \\sigma_1 \\sigma_2\n",
    "$$\n",
    "\n",
    "Since the correlation $\\rho_{12} < 1$, i.e., ETF 1 and ETF 2 do not move perfectly together, the portfolio variance is **smaller** than the square of the weighted average of the standard deviations of the two ETFs, and thus also less than the average of the individual variances in most cases: \n",
    "\n",
    "$$\n",
    "\\sigma_p^2 < w_1^2 \\sigma_1^2 + w_2^2 \\sigma_2^2  + 2w_1 w_2 \\sigma_1 \\sigma_2 = (w_1 \\sigma_1 + w_2 \\sigma_2)^2\n",
    "$$\n",
    "\n",
    "For the expected returns, this is not true.  The expected return is a weighted average of the individual expected returns of the ETFs, $\\mu_1$ and $\\mu_2$.\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[R_p] = w_1 \\mu_1 + w_2 \\mu_2\n",
    "$$\n",
    "\n",
    "This is the idea behind **diversification**: by combining assets that are not perfectly correlated, the portfolio’s overall variance (risk) becomes lower than the average of the individual asset variances.  In other words, diversification helps smooth out the ups and downs, because not all assets move together.\n",
    "\n",
    "Now we understand why diversification helps us when building portfolios to reduce risk and we can try to find optimal portfolios that reduce the variance for a given expected return target, or maximize the expected return for a volatility/risk level we are willing to accept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb9af0",
   "metadata": {},
   "source": [
    "### 3. Mean‑Variance Optimization\n",
    "\n",
    "In this section, we will define the efficient frontier based on the historic average returns and covariances of the ETF and find our optimal portfolio that matches VOO.  We will explore different regularization and shrinkage estimators for the mean and covariance.  Our take-away from this section: the optimal portfolio is a combination of short-term Treasury ETF and the Russell 1000 Growth ETF. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62055867",
   "metadata": {},
   "source": [
    "The basic problem of finding the best portfolios for a given expected return goes back to the classic [Markowitz (1952)](https://www.jstor.org/stable/2975974) problem:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}} \\; \\tfrac{1}{2} \\mathbf{w}^\\top \\Sigma \\mathbf{w}\n",
    "\\quad \\text{subject to} \\quad\n",
    "\\mathbf{w}^\\top \\mathbf{1} = 1, \\quad\n",
    "\\mathbf{w}^\\top \\mu = \\mu^*.\n",
    "$$\n",
    "\n",
    "Quite straightforward: to objective is to find portfolio weights $\\mathbf{w}$ for the ETFs that minimize the variance of the portfolio while matching the expected return $\\mu^*$.  The only other constraint is that the weights sum up to one.  That's all.  We can build a efficient frontier with this approach by defining a sequence of target expected returns $\\mu^*$ for the portfolios, between the smallest and largest returns of the ETFs, and solve for the portfolio with minimum variance for each target $\\mu^*$.  Since we are only dealing with first and second moments, so nothing of higher order than to the power of two, the minimization problem is convex and we can find unique portfolio weights that minimize the variance.\n",
    "\n",
    "Note that since we have a nice quadratic problem, we can use the [CVXOPT Quadratic Programming solver](https://cvxopt.org/userguide/coneprog.html?highlight=cvxopt%20solvers%20qp#quadratic-programming) to find the solution to the program above.  We only need to translate the conditions into the proper vectors and matrices that the optimizer expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4cb029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the two key inputs for Markowitz portfolio optimization.\n",
    "# 1. Expected Returns (mu): The anticipated annualized return for each asset.\n",
    "# 2. Covariance Matrix (Sigma): A measure of how asset returns move together.\n",
    "\n",
    "# Calculate historical annualized mean returns, net of expense ratios\n",
    "annual_mu_sample = (returns_monthly.mean().values * 12) - expense_vector\n",
    "\n",
    "# The sample covariance matrix is calculated from historical returns and annualized\n",
    "sample_cov = returns_monthly.cov().values\n",
    "annual_cov_sample = sample_cov * 12\n",
    "\n",
    "\n",
    "def efficient_frontier_markowitz(expected_returns, covariance_matrix, n_points=50):\n",
    "    \"\"\"\n",
    "    Computes the mean-variance efficient frontier using classic Markowitz optimization.\n",
    "\n",
    "    Args:\n",
    "        expected_returns (np.array): Vector of expected returns (annualized).\n",
    "        covariance_matrix (np.array): Covariance matrix of returns (annualized).\n",
    "        n_points (int): Number of portfolios to calculate along the frontier.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains lists of 'mu', 'sigma', and 'weights' for each portfolio.\n",
    "    \"\"\"\n",
    "    n_assets = len(expected_returns)\n",
    "    frontier = {\"mu\": [], \"sigma\": [], \"weights\": []}\n",
    "\n",
    "    # Define QP parameters that don't change in the loop\n",
    "    P = opt.matrix(covariance_matrix)\n",
    "    q = opt.matrix(np.zeros(n_assets))\n",
    "    \n",
    "    # Equality constraint matrix A: one row for expected return, one for weights summing to 1\n",
    "    A = opt.matrix(np.vstack([expected_returns, np.ones(n_assets)]))\n",
    "    \n",
    "    # Range of target returns\n",
    "    target_mus = np.linspace(expected_returns.min(), expected_returns.max(), n_points)\n",
    "\n",
    "    for mu_target in target_mus:\n",
    "        b = opt.matrix([mu_target, 1.0])  # RHS of constraints\n",
    "        try:\n",
    "            sol = opt.solvers.qp(P, q, None, None, A, b)\n",
    "            if sol['status'] == 'optimal':\n",
    "                w = np.array(sol['x']).flatten()\n",
    "                mu = w @ expected_returns\n",
    "                sigma = np.sqrt(w @ covariance_matrix @ w)\n",
    "                frontier[\"mu\"].append(mu)\n",
    "                frontier[\"sigma\"].append(sigma)\n",
    "                frontier[\"weights\"].append(w)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return frontier\n",
    "\n",
    "# Compute the frontier\n",
    "frontier_data = efficient_frontier_markowitz(annual_mu_sample, annual_cov_sample)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(frontier_data[\"sigma\"], frontier_data[\"mu\"], lw=2, label=\"Efficient Frontier\")\n",
    "plt.xlabel(\"Volatility (σ)\", fontsize=12)\n",
    "plt.ylabel(\"Expected Return (μ)\", fontsize=12)\n",
    "plt.title(\"Mean-Variance Efficient Frontier\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d34ec",
   "metadata": {},
   "source": [
    "This looks exactly as expected!  The efficient frontier has the shape of a parabola on it's side, because eventually the variance of a portfolio is a sum over squared deviations from the mean.  We also see that for some volatility level we actually have two solution, a lower and an upper arm of the frontier (due to the quadratic nature of the problem).  The lower arm economically makes no sense, so we will prune the results later to remove it; practically, if an investor is willing to accept a level of risk (volatility) that has two optimal portfolios, he would only choose portfolios from the upper arm with highest return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638275be",
   "metadata": {},
   "source": [
    "We eventually want to select optimal portfolios from the efficient frontier that minimize the variance for a specific target expected return, or that maximize the return for a given level of risk.  As benchmark for our optimal portfolios, we can look at VOO, the S&P 500 ETF.  VOO is one of the largest ETFs worldwide and the S&P 500 is frequently used as a proxy for the overall market portfolio, so it makes sense to use the returns and volatility of VOO as targets for our portfolio optimization.  \n",
    "\n",
    "Let's first find the portfolio weights of the efficient frontier portfolio that has the same expected return as VOO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05519015",
   "metadata": {},
   "outputs": [],
   "source": [
    "voo_returns_monthly = returns_monthly[\"VOO\"]\n",
    "voo_mu_annual = voo_returns_monthly.mean() * 12 - etf_expense_map.get(\"VOO\", 0.0)\n",
    "voo_sigma_annual = voo_returns_monthly.std() * np.sqrt(12)\n",
    "\n",
    "\n",
    "def select_portfolio(frontier, target_metric, target_value):\n",
    "    \"\"\"\n",
    "    Selects a portfolio from the efficient frontier closest to a target value.\n",
    "\n",
    "    Args:\n",
    "        frontier (dict): The efficient frontier dictionary.\n",
    "        target_metric (str): The metric to match ('mu' or 'sigma').\n",
    "        target_value (float): The target return or volatility.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Index and weights of the selected portfolio, or (None, None).\n",
    "    \"\"\"\n",
    "    if not frontier[target_metric]:\n",
    "        return None, None\n",
    "    diffs = np.abs(np.array(frontier[target_metric]) - target_value)\n",
    "    idx = diffs.argmin()\n",
    "    return idx, frontier[\"weights\"][idx]\n",
    "\n",
    "# Select matching portfolio\n",
    "idx, weights_voo_match = select_portfolio(frontier_data, target_metric=\"mu\", target_value=voo_mu_annual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a6ff82",
   "metadata": {},
   "source": [
    "To get a sense of how the optimal portfolio looks like, we can print the top and bottom three portfolio weights, the smallest weights in absolute terms, and plot weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6135f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if weights_voo_match is not None:\n",
    "    selected_mu = frontier_data[\"mu\"][idx]\n",
    "    selected_sigma = frontier_data[\"sigma\"][idx]\n",
    "\n",
    "    print(f\"\\nComparison with VOO:\")\n",
    "    print(f\"VOO - Expected Return: {voo_mu_annual:.2%}, Volatility: {voo_sigma_annual:.2%}\")\n",
    "    print(f\"Selected Portfolio - Expected Return: {selected_mu:.2%}, Volatility: {selected_sigma:.2%}\")\n",
    "\n",
    "    # Zip symbols and weights, then sort by weight\n",
    "    weights_named = list(zip(etf_symbols, weights_voo_match))\n",
    "    sorted_weights = sorted(weights_named, key=lambda x: x[1], reverse=True)\n",
    "    sorted_by_abs = sorted(weights_named, key=lambda x: abs(x[1]))\n",
    "\n",
    "    print(\"\\nTop 3 Weights in Selected Portfolio:\")\n",
    "    for symbol, weight in sorted_weights[:3]:\n",
    "        print(f\"{symbol}: {weight:.2%}\")\n",
    "\n",
    "    print(\"\\nBottom 3 Weights in Selected Portfolio:\")\n",
    "    for symbol, weight in sorted_weights[-3:]:\n",
    "        print(f\"{symbol}: {weight:.2%}\")\n",
    "\n",
    "    print(\"\\n3 Smallest Absolute Weights in Selected Portfolio:\")\n",
    "    for symbol, weight in sorted_by_abs[:3]:\n",
    "        print(f\"{symbol}: {weight:.2%}\")\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(etf_symbols, weights_voo_match)\n",
    "    plt.title(f\"Portfolio Weights Matching VOO's Expected Return ({voo_mu_annual:.2%})\")\n",
    "    plt.ylabel(\"Weight\")\n",
    "    plt.xlabel(\"ETF\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No matching portfolio found on the frontier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b079c8c3",
   "metadata": {},
   "source": [
    "This is unfortunately a typical result for simple portfolio optimization: the expected volatility for the optimal portfolio is much lower than for VOOs, but it is practically infeasible as there are many assets with extreme weights in the optimal portfolio.  The Total Bond Market ETF (BND), for example, should have a weight of more than -600%, while the Intermediate Corporate Bond ETF (VCIT) should have a weight of more than 300%!  In other words, if you want to invest $1, you should short-sell $6 of BND and use the proceeds to purchase $3 of VCIT, etc.  This is not really feasible for a retail investor.  \n",
    "\n",
    "We could also encounter a second problem: there are many portfolios with quite small weights, such as the Short-Term Inflation-Protected Securities ETF (VTIP) having a weight of around 3%.  This is not very large and it makes the implementation of trading strategies difficult since we have to purchase many ETFs with tiny weights.\n",
    "\n",
    "A third problem we have not mentioned yet is estimation error.  Given how extreme the weights are, even small errors in the estimated expected return or the covariance matrix can lead to substantial changes in the optimal portfolio allocations.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b338b1c",
   "metadata": {},
   "source": [
    "To account for these problem, we can try the following:\n",
    "- **Add long-only constraints**: we can add an additional constraint to make sure each portfolio weight is between zero and one.  This is much more realistic for an individual investor.\n",
    "\n",
    "- **Apply L1 regularization**: we can adapt the idea from the LASSO regression and add a penalty for large and non-zero weights, making it more likely that the optimization shrinks more weights to zero, reducing the number of assets in the optimal portfolio.  We need to decide on a the meta-parameter for the penalty, $\\lambda$, which we can do via a simple grid search.  Choosing to small of a $\\lambda$ and we might have too many small ETFs in the optimal portfolio, choosing to large a $\\lambda$ and we introduce too much of a bias and the portfolio is not optimal anymore.  So, let's look for the $\\lambda$ that maximizes the Sharpe ratio, expected return over sigma, for the optimal portfolio that matches VOO's expected return.  The Sharpe ratio is a measure of the price of risk, so it makes sense to choose the highest possible return that we can get for one unit of risk, i.e., volatility.\n",
    "\n",
    "- **Apply shrinkage methods to $\\mu$ and $\\sigma$**: to reduce the effect of estimation error in the expected return and the covariance matrix, we can apply shrinkage methods for both.  We can try James-Stein for the expected return and Ledoit-Wolf for the covariance matrix.  Both shrink the sample estimates to a reasonable target, thus reducing estimation error and noise of the estimates.  James-Stein shrinks the sample means of the ETFs to the grand mean over all ETFs, and Ledoit-Wolf shrinks the covariance matrix to the identify matrix.\n",
    "\n",
    "Useful sources on implementing the LASSO regularization in quadratic programming are [Stéphane Caron's blog](https://scaron.info/blog/lasso-regularization-in-quadratic-programming.html) and the original paper by Rovert Tibshirani, [*Regression Shrinkage and Selection via the Lasso*](https://academic.oup.com/jrsssb/article-pdf/58/1/267/49098631/jrsssb_58_1_267.pdf), ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b111ce88",
   "metadata": {},
   "source": [
    "We can now expand the optimization problem as follows, with $\\mu$ and $\\Sigma$ either the sample mean and covariance or by the shrunk versions.\n",
    "\n",
    "#### Objective\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}, \\mathbf{u}} \\quad \\tfrac{1}{2} \\mathbf{w}^\\top \\Sigma \\mathbf{w} + \\lambda \\sum_{i=1}^n u_i\n",
    "$$\n",
    "\n",
    "\n",
    "#### Constraints\n",
    "\n",
    "1. **L1 Reformulation (Enforce $ u_i \\ge |w_i| $)**\n",
    "\n",
    "$$\n",
    "w_i - u_i \\le 0 \\quad \\text{(i.e., } u_i \\ge w_i \\text{)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "-w_i - u_i \\le 0 \\quad \\text{(i.e., } u_i \\ge -w_i \\text{)}\n",
    "$$\n",
    "\n",
    "2. **Long-only Constraints**\n",
    "\n",
    "$$\n",
    "0 \\le w_i \\le 1\n",
    "$$\n",
    "\n",
    "3. **Budget Constraint**\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n w_i = 1\n",
    "$$\n",
    "\n",
    "4. **Target Return Constraint**\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n w_i \\mu_i = \\mu^*\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b758aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_frontier(\n",
    "    covariance_matrix, expected_returns, n_points=50, lambda_l1=0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates the efficient frontier using the Markowitz model with optional L1\n",
    "    regularization (LASSO). This encourages sparse portfolios by driving the\n",
    "    weights of less important assets to exactly zero.\n",
    "\n",
    "    This function reformulates the L1-regularized problem into a standard\n",
    "    Quadratic Program (QP) that can be solved efficiently by CVXOPT.\n",
    "\n",
    "    Args:\n",
    "        covariance_matrix (np.array): Annualized covariance matrix of asset returns.\n",
    "        expected_returns (np.array): Annualized vector of expected asset returns.\n",
    "        n_points (int): The number of points to calculate along the frontier.\n",
    "        lambda_l1 (float): The regularization strength. Higher values lead to\n",
    "                           more sparsity (more zero weights).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing returns ('mu'), volatilities ('sigma'),\n",
    "              and portfolio weights ('weights') for each point on the frontier.\n",
    "    \"\"\"\n",
    "    n_assets = len(expected_returns)\n",
    "\n",
    "    # We solve for a combined vector x = [w, u] of size 2*n_assets, where:\n",
    "    # w: the standard portfolio weights (n_assets)\n",
    "    # u: auxiliary variables to handle the absolute value |w_i| (n_assets)\n",
    "    # The objective becomes: minimize 0.5*w'.Cov.w + lambda*1'.u\n",
    "    # Subject to: w-u <= 0, -w-u <= 0 (which implies u >= |w|)\n",
    "\n",
    "    # 1. The Quadratic Term P\n",
    "    # Only involves 'w', so P_new has the original covariance_matrix in the\n",
    "    # top-left block and zeros elsewhere.\n",
    "    P_l1 = opt.matrix(\n",
    "        np.block(\n",
    "            [\n",
    "                [covariance_matrix, np.zeros((n_assets, n_assets))],\n",
    "                [np.zeros((n_assets, n_assets)), np.zeros((n_assets, n_assets))],\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 2. The Linear Term q\n",
    "    # The L1 penalty `lambda * sum(|w_i|)` is reformulated as `lambda * sum(u_i)`.\n",
    "    # This becomes the linear part of the objective, q'.x.\n",
    "    q_l1 = opt.matrix(np.concatenate([np.zeros(n_assets), lambda_l1 * np.ones(n_assets)]))\n",
    "\n",
    "    # 3. The Inequality Constraints G and h (for Gx <= h)\n",
    "    # These enforce u_i >= |w_i| and the original box constraints 0 <= w_i <= 1.\n",
    "    I = np.eye(n_assets)\n",
    "    Z = np.zeros((n_assets, n_assets))\n",
    "    G_l1 = opt.matrix(\n",
    "        np.block(\n",
    "            [\n",
    "                [I, -I],  # For w_i - u_i <= 0\n",
    "                [-I, -I],  # For -w_i - u_i <= 0\n",
    "                [-I, Z],  # For -w_i <= 0 (w_i >= 0)\n",
    "                [I, Z],  # For w_i <= 1\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    h_l1 = opt.matrix(np.concatenate([np.zeros(3 * n_assets), np.ones(n_assets)]))\n",
    "\n",
    "    # 4. The Equality Constraints A and b (for Ax = b)\n",
    "    # These constraints (sum of weights = 1, portfolio return = target)\n",
    "    # only apply to the 'w' part of our variable vector 'x'.\n",
    "    A_l1 = opt.matrix(\n",
    "        np.block(\n",
    "            [\n",
    "                [expected_returns, np.zeros(n_assets)],\n",
    "                [np.ones(n_assets), np.zeros(n_assets)],\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Iterate through a range of target returns to trace the frontier\n",
    "    target_mus = np.linspace(expected_returns.min(), expected_returns.max(), n_points)\n",
    "    frontier = {\"mu\": [], \"sigma\": [], \"weights\": []}\n",
    "    for target_mu in target_mus:\n",
    "        # The RHS of the equality constraint\n",
    "        b_l1 = opt.matrix([target_mu, 1.0])\n",
    "        try:\n",
    "            # Solve the larger, reformulated QP problem\n",
    "            solution = opt.solvers.qp(P_l1, q_l1, G_l1, h_l1, A_l1, b_l1)\n",
    "            if solution[\"status\"] == \"optimal\":\n",
    "                # Extract only the weights 'w' from the solution vector 'x'.\n",
    "                weights = np.array(solution[\"x\"][:n_assets]).flatten()\n",
    "                # Clean up tiny weights due to solver precision\n",
    "                weights[np.abs(weights) < 1e-7] = 0\n",
    "                # Re-normalize to ensure sum is exactly 1 after cleanup\n",
    "                if np.sum(weights) > 0:\n",
    "                    weights /= np.sum(weights)\n",
    "\n",
    "                sigma = np.sqrt(weights.T @ covariance_matrix @ weights)\n",
    "                actual_mu = weights.T @ expected_returns\n",
    "                frontier[\"mu\"].append(actual_mu)\n",
    "                frontier[\"sigma\"].append(sigma)\n",
    "                frontier[\"weights\"].append(weights)\n",
    "        except ValueError:\n",
    "            # Solver may fail if no feasible solution exists for a target return\n",
    "            pass\n",
    "    return frontier\n",
    "\n",
    "\n",
    "def prune_frontier(frontier):\n",
    "    \"\"\"\n",
    "    Removes dominated portfolios from a mean-variance frontier, keeping only\n",
    "    the efficient upper arm where return increases with volatility.\n",
    "\n",
    "    Args:\n",
    "        frontier (dict): A dict with keys 'sigma', 'mu', 'weights'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A pruned frontier with the same structure.\n",
    "    \"\"\"\n",
    "    vol = np.asarray(frontier[\"sigma\"])\n",
    "    ret = np.asarray(frontier[\"mu\"])\n",
    "    wlist = list(frontier[\"weights\"])\n",
    "\n",
    "    # 1) Sort by volatility (ascending)\n",
    "    order = np.argsort(vol)\n",
    "    vol, ret = vol[order], ret[order]\n",
    "    wlist = [wlist[i] for i in order]\n",
    "\n",
    "    # 2) Walk from left to right, keeping only points with strictly increasing returns\n",
    "    keep_idx = []\n",
    "    last_best_ret = -np.inf\n",
    "    for i in range(len(ret)):\n",
    "        if ret[i] > last_best_ret + 1e-12:\n",
    "            keep_idx.append(i)\n",
    "            last_best_ret = ret[i]\n",
    "\n",
    "    # 3) Assemble the pruned frontier\n",
    "    return {\n",
    "        \"sigma\": vol[keep_idx].tolist(),\n",
    "        \"mu\": ret[keep_idx].tolist(),\n",
    "        \"weights\": [wlist[i] for i in keep_idx],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43c91d1",
   "metadata": {},
   "source": [
    "We need to still define the optimal L1 penalty term for regularization, $\\lambda$.  Too large a parameter promotes sparsity but introduces too much bias, while too small a parameter being ineffective in removing small weights.  We will do a grid search and find the parameter that maximizes the Sharpe ratio for the optimal portfolio that matches VOO's volatility.  We can add robustness to the search by using a training-test split to compare $\\lambda$'s based on out-of-sample performance.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c6c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Grid-Search for the \"Best\" L1-Penalty (lambda_1)\n",
    "# - A simple train/validation split is used (last 20% = validation).\n",
    "# - The metric for \"best\" is the out-of-sample Sharpe Ratio.\n",
    "# - The portfolio on the frontier is chosen by matching VOO's volatility.\n",
    "print(\"\\n--- Grid-searching for optimal L1 penalty (lambda) ---\")\n",
    "lambda_grid = np.logspace(-6, -1, 11)  # 11 points: 1e-6 to 1e-1\n",
    "val_frac = 0.20\n",
    "target_sig = voo_mu_annual\n",
    "\n",
    "# Split data into training and validation sets\n",
    "T = len(returns_monthly)\n",
    "split_idx = int((1 - val_frac) * T)\n",
    "ret_train = returns_monthly.iloc[:split_idx]\n",
    "ret_val = returns_monthly.iloc[split_idx:]\n",
    "\n",
    "mu_train = ret_train.mean().values * 12\n",
    "cov_train = ret_train.cov().values * 12\n",
    "mu_val = ret_val.mean().values * 12\n",
    "cov_val = ret_val.cov().values * 12\n",
    "\n",
    "\n",
    "def sharpe_ratio(mu, sigma):\n",
    "    \"\"\"\n",
    "    Calculates annualized Sharpe ratio from annualized decimal returns.\n",
    "    (Assumes risk-free rate = 0).\n",
    "    \"\"\"\n",
    "    return mu / sigma if sigma > 0 else np.nan\n",
    "\n",
    "\n",
    "def eval_lambda(lam):\n",
    "    \"\"\"Fit frontier with lambda_1=lam on train-set, score on validation.\"\"\"\n",
    "    front = efficient_frontier(cov_train, mu_train, n_points=30, lambda_l1=lam)\n",
    "    _, w = select_portfolio(front, \"mu\", target_sig)\n",
    "    if w is None:\n",
    "        return np.nan\n",
    "    mu_out_of_sample = w @ mu_val\n",
    "    sigma_out_of_sample = np.sqrt(w @ cov_val @ w)\n",
    "    return sharpe_ratio(mu_out_of_sample, sigma_out_of_sample)\n",
    "\n",
    "\n",
    "# Perform grid search\n",
    "scores = [eval_lambda(lam) for lam in lambda_grid]\n",
    "best_idx = int(np.nanargmax(scores))\n",
    "best_lambda = float(lambda_grid[best_idx])\n",
    "\n",
    "# Report results\n",
    "print(f\"lambda_1 candidates: {[f'{l:.5g}' for l in lambda_grid]}\")\n",
    "print(f\"Validation Sharpe: {[f'{s:.3f}' for s in scores]}\")\n",
    "print(f\"\\n-> Selected optimal lambda_1 = {best_lambda:.5g}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28364488",
   "metadata": {},
   "source": [
    "Before we implement the portfolio optimization with shrinkage estimates for $\\mu$ and $\\Sigma$, we should understand what the shrinkage estimators are and see how they compare to the sample mean and covariance.\n",
    "\n",
    "There are many sources for shrinkage estimators, for James-Stein, for example, these [lecture notes by Maxime Cauchois](https://web.stanford.edu/class/stats50/files/STATS_50_Regression_to_the_mean.pdf) provide a nice summary, or the paper by Eric Bennett Rasmusen, [*Understanding Shrinkage Estimators: From Zero to Oracle to James-Stein*](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2675681).  For Ledoit-Wolf, the original paper, [*A well-conditioned estimator for large-dimensional covariance matrices*](https://perso.ens-lyon.fr/patrick.flandrin/LedoitWolf_JMA2004.pdf), is a solid source, but also the [sklean documentation about covariance shrinkage](https://scikit-learn.org/stable/modules/covariance.html#shrunk-covariance) is very useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa0455",
   "metadata": {},
   "source": [
    "The James-Stein estimator is a famous and straightforward approach to use Bayesian-style shrinkage to the expected return.  It's formula is data-driven and basically tries to shrink the sample means for each asset towards the grand mean.  I use the following formula:\n",
    "\n",
    "$$\n",
    "\\hat\\mu^{\\text{JS}}_i \\;=\\; (1-\\lambda)\\,\\hat\\mu_i \\;+\\; \\lambda\\,\\bar\\mu,\n",
    "\\qquad\n",
    "\\lambda \\;=\\; \\min\\!\\Bigl[1,\\;\\frac{(p-2)\\,s^{2}}{\\sum_{j=1}^{p}(\\hat\\mu_j-\\bar\\mu)^{2}}\\Bigr]\n",
    "$$\n",
    "\n",
    "with $\\bar\\mu=\\tfrac{1}{p}\\sum_{j=1}^{p}\\hat\\mu_j$ and $s^{2}=\\tfrac{1}{p-1}\\sum_{j=1}^{p}(\\hat\\mu_j-\\bar\\mu)^{2}$.  $\\bar\\mu$ is nothing but the grand mean, the average return across all expected ETF returns, and $s^{2}$ is the estimated variance of the individual ETF means.  The James-Stein estimator thus intuitively shrinks more towards the grand mean if there is more uncertainty about the each estimators (larger $s^{2}$) or when the estimates for $\\mu$ are closer around the grand mean (smaller $\\sum_{j=1}^{p}(\\hat\\mu_j-\\bar\\mu)^{2}$).\n",
    "\n",
    "One assumptions here is that all estimators for the mean return have the same variance, thus we can use one $s^{2}$ for the approach.  This is probably not correct, but a useful and simple starting point.  We further see that $\\lambda$ simplifies to $\\frac{p-2}{p-1}$, since $s^{2}$ is estimated from the same data as the denominator.  This is a big simplification, more accurate would be to use the estimated variance of each ETF's mean, so that ETFs with more uncertainty about the expected return are shrunk closer to the grand mean, but for initial understanding how shrinkage affects our portfolio optimization, this is enough.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c91e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def james_stein_shrinkage(mu_hat):\n",
    "    \"\"\"\n",
    "    Applies James–Stein shrinkage to the vector of sample means.\n",
    "\n",
    "    Args:\n",
    "        mu_hat (np.ndarray): Sample mean returns (shape: [n_assets]).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Shrunk mean returns.\n",
    "    \"\"\"\n",
    "    p = len(mu_hat)\n",
    "    mu_bar = np.mean(mu_hat)\n",
    "    numerator = (p - 2) * np.var(mu_hat, ddof=1)\n",
    "    denominator = np.sum((mu_hat - mu_bar) ** 2)\n",
    "    \n",
    "    # Guard against division by zero\n",
    "    if denominator == 0:\n",
    "        lambda_js = 0\n",
    "    else:\n",
    "        lambda_js = min(1, max(0, numerator / denominator))\n",
    "\n",
    "    mu_shrunk = (1 - lambda_js) * mu_hat + lambda_js * mu_bar\n",
    "    return mu_shrunk\n",
    "\n",
    "\n",
    "# Mean shrinkage\n",
    "annual_mu_shrunk = james_stein_shrinkage(annual_mu_sample)\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "df_mu = pd.DataFrame({\n",
    "    \"ETF\": etf_symbols,\n",
    "    \"Sample Mean\": annual_mu_sample,\n",
    "    \"James–Stein Mean\": annual_mu_shrunk\n",
    "})\n",
    "\n",
    "# Melt to long format for seaborn\n",
    "df_mu_melted = df_mu.melt(id_vars=\"ETF\", var_name=\"Type\", value_name=\"Return\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_mu_melted, x=\"ETF\", y=\"Return\", hue=\"Type\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Annualized Expected Returns: Sample vs. James–Stein Shrinkage\")\n",
    "plt.ylabel(\"Expected Return\")\n",
    "plt.grid(axis='y', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe22ad8",
   "metadata": {},
   "source": [
    "James-Stein is too aggressive and shrinks the annual mean to severely towards the grand mean.  I appears that since the sample means are no too widely distributed, James-Stein shrinks them relatively quickly towards the grand mean.\n",
    "\n",
    "Instead of the harsh shrinkage by James-Stein, let's use a more gentle shrinkage parameter to preserves more of the dispersion of the individual expected returns of the ETFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6069d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_small = 0.2  # small shrinkage strength\n",
    "mu_grand = np.mean(annual_mu_sample)\n",
    "annual_mu_shrunk_partial = (1 - lambda_small) * annual_mu_sample + lambda_small * mu_grand\n",
    "\n",
    "# Build DataFrame\n",
    "df_mu_partial = pd.DataFrame({\n",
    "    \"ETF\": etf_symbols,\n",
    "    \"Sample Mean\": annual_mu_sample,\n",
    "    f\"Shrunk (λ={lambda_small})\": annual_mu_shrunk_partial\n",
    "})\n",
    "\n",
    "# Melt to long format for plotting\n",
    "df_mu_melted = df_mu_partial.melt(id_vars=\"ETF\", var_name=\"Type\", value_name=\"Return\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_mu_melted, x=\"ETF\", y=\"Return\", hue=\"Type\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(f\"Annualized Expected Returns: Sample vs. Shrunk (λ = {lambda_small})\")\n",
    "plt.ylabel(\"Expected Return\")\n",
    "plt.grid(axis='y', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03dd15e",
   "metadata": {},
   "source": [
    "Next, we can shrink the covariance using Ledoit-Wolf.  The basic approach here can also be viewed as Bayesian-like shrinkage for the covariance matrix towards the a grand-mean type identity matrix that has the average variance in the diagonal.  The precise formula for the weights is quite complicated, but it is worth noting that the weight on the diagonal target matrix decreases as the sample covariance matrix becomes more reliable.  The formula looks like this:\n",
    "\n",
    "$$\n",
    "\\Sigma_{\\text{LW}} = (1-\\delta)\\,S + \\delta F,\n",
    "\\qquad\n",
    "F = \\bar{s}\\,I,\\qquad\n",
    "\\bar{s} = \\frac{1}{p}\\operatorname{tr}(S).\n",
    "$$\n",
    "\n",
    "* $S$ = sample covariance matrix\n",
    "* $F$ = scaled identity matrix with the same average variance as $S$\n",
    "* $0\\le\\delta\\le1$ = data-driven shrinkage weight picked by the Ledoit-Wolf rule to minimize estimation error\n",
    "\n",
    "Luckily, the implementation in Python is straightforward with the sklearn library.  We can compare the heat map of the raw and the shrunk estimators to see how much shrinkage affects our covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbef4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance shrinkage\n",
    "lw = LedoitWolf().fit(returns_monthly.values)\n",
    "annual_cov_shrunk = lw.covariance_ * 12\n",
    "\n",
    "# Convert to DataFrames for heatmap comparison\n",
    "df_cov_sample = pd.DataFrame(annual_cov_sample, index=etf_symbols, columns=etf_symbols)\n",
    "df_cov_shrunk = pd.DataFrame(annual_cov_shrunk, index=etf_symbols, columns=etf_symbols)\n",
    "\n",
    "# Plot side-by-side heatmaps\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6), constrained_layout=True)\n",
    "\n",
    "sns.heatmap(df_cov_sample, cmap=\"YlGnBu\", ax=axs[0], square=True, cbar_kws={\"shrink\": 0.7})\n",
    "axs[0].set_title(\"Sample Covariance\")\n",
    "\n",
    "sns.heatmap(df_cov_shrunk, cmap=\"YlGnBu\", ax=axs[1], square=True, cbar_kws={\"shrink\": 0.7})\n",
    "axs[1].set_title(\"Ledoit–Wolf Shrunk Covariance\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "\n",
    "plt.suptitle(\"Covariance Matrices: Sample vs. Ledoit–Wolf Shrinkage\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd72816",
   "metadata": {},
   "source": [
    "Ledoit-Wolf barely changes the covariance matrix.  It seems the sample covariance matrix is already quite stable due to the 120 monthly observations for each ETF, so Ledoit-Wolf does not change much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cf6c89",
   "metadata": {},
   "source": [
    "Finally, let us generate the efficient frontiers for each variant (raw, with L1 regularization, with mean shrunk, with covariance shrunk) and compare optimal portfolios that match VOO's expected mean or volatility.  For this, we can print the top weights for each optimal portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b50f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frontier using simple sample estimates\n",
    "ef_raw = prune_frontier(\n",
    "    efficient_frontier(annual_cov_sample, annual_mu_sample, n_points=FRONTIER_POINTS)\n",
    ")\n",
    "# Frontier using L1 regularization\n",
    "ef_reg_l1 = prune_frontier(\n",
    "    efficient_frontier(\n",
    "        annual_cov_shrunk, annual_mu_sample, n_points=FRONTIER_POINTS, lambda_l1=best_lambda\n",
    "    )\n",
    ")\n",
    "# Frontier using shrinkage-adjusted covariance\n",
    "ef_shrunk = prune_frontier(\n",
    "    efficient_frontier(annual_cov_shrunk, annual_mu_sample, n_points=FRONTIER_POINTS)\n",
    ")\n",
    "# Frontier using also shrinkage-adjusted means\n",
    "ef_shrunk_mean = prune_frontier(\n",
    "    efficient_frontier(annual_cov_shrunk, annual_mu_shrunk_partial, n_points=FRONTIER_POINTS)\n",
    ")\n",
    "\n",
    "# Find portfolios on each frontier matching the VOO benchmark's risk or return\n",
    "_, w_mu_raw = select_portfolio(ef_raw, \"mu\", voo_mu_annual)\n",
    "_, w_sigma_raw = select_portfolio(ef_raw, \"sigma\", voo_sigma_annual)\n",
    "_, w_mu_reg_l1 = select_portfolio(ef_reg_l1, \"mu\", voo_mu_annual)\n",
    "_, w_sigma_reg_l1 = select_portfolio(ef_reg_l1, \"sigma\", voo_sigma_annual)\n",
    "_, w_mu_shrunk = select_portfolio(ef_shrunk, \"mu\", voo_mu_annual)\n",
    "_, w_sigma_shrunk = select_portfolio(ef_shrunk, \"sigma\", voo_sigma_annual)\n",
    "_, w_mu_shrunk_mean = select_portfolio(ef_shrunk_mean, \"mu\", voo_mu_annual)\n",
    "_, w_sigma_shrunk_mean = select_portfolio(ef_shrunk_mean, \"sigma\", voo_sigma_annual)\n",
    "\n",
    "# Display the composition of selected portfolios\n",
    "portfolios_to_display = {\n",
    "    \"Raw (Return-Matched)\": w_mu_raw,\n",
    "    \"Raw (Risk-Matched)\": w_sigma_raw,\n",
    "    \"L1 Regularized (Return-Matched)\": w_mu_reg_l1,\n",
    "    \"L1 Regularized (Risk-Matched)\": w_sigma_reg_l1,\n",
    "    \"Shrunk Covariance (Return-Matched)\": w_mu_shrunk,\n",
    "    \"Shrunk Covariance (Risk-Matched)\": w_sigma_shrunk,\n",
    "    \"Shrunk Mean (Return-Matched)\": w_mu_shrunk_mean,\n",
    "    \"Shrunk Mean (Risk-Matched)\": w_sigma_shrunk_mean,\n",
    "}\n",
    "\n",
    "for label, weights in portfolios_to_display.items():\n",
    "    if weights is not None:\n",
    "        print(f\"\\nTop 3 ETFs for {label} Portfolio:\")\n",
    "        top_indices = np.argsort(weights)[-3:][::-1]\n",
    "        for i in top_indices:\n",
    "            if weights[i] > 0.001:  # Only show assets with meaningful weight\n",
    "                symbol = etf_symbols[i]\n",
    "                name = etf_name_map.get(symbol, \"Unknown\")\n",
    "                print(f\"  {symbol} ({name}): {weights[i]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e449d89",
   "metadata": {},
   "source": [
    "We can also visually compare the efficient frontiers and how close the optimal portfolios are to the target VOO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d2c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(ef_raw[\"sigma\"], ef_raw[\"mu\"], \"o-\", label=\"Raw Estimate Frontier\", alpha=0.7)\n",
    "plt.plot(ef_reg_l1[\"sigma\"], ef_reg_l1[\"mu\"], \"o-\", label=\"L1 Regularized Frontier\", lw=2)\n",
    "plt.plot(\n",
    "    ef_shrunk[\"sigma\"], ef_shrunk[\"mu\"], \"o-\", label=\"Shrunk Covariance Frontier\", lw=2\n",
    ")\n",
    "plt.plot(\n",
    "    ef_shrunk_mean[\"sigma\"], ef_shrunk_mean[\"mu\"], \"o-\", label=\"Shrunk Mean Frontier\", lw=2\n",
    ")\n",
    "# VOO benchmark\n",
    "plt.scatter([voo_sigma_annual], [voo_mu_annual], color=\"black\", marker=\"X\", s=200, label=\"VOO Benchmark\", zorder=5)\n",
    "\n",
    "# Define portfolio styles\n",
    "portfolios = [\n",
    "    (\"Raw\", w_sigma_raw, w_mu_raw, annual_mu_sample, \"blue\"),\n",
    "    (\"L1 Regularized\", w_sigma_reg_l1, w_mu_reg_l1, annual_mu_sample, \"orange\"),\n",
    "    (\"Shrunk Covariance\", w_sigma_shrunk, w_mu_shrunk, annual_mu_sample, \"green\"),\n",
    "    (\"Shrunk Mean\", w_sigma_shrunk_mean, w_mu_shrunk_mean, annual_mu_shrunk_partial, \"purple\"),\n",
    "]\n",
    "\n",
    "# Plot sigma- and mu-matched portfolios\n",
    "for label, w_sigma, w_mu, mu_vec, color in portfolios:\n",
    "    if w_sigma is not None:\n",
    "        mu = w_sigma @ mu_vec\n",
    "        sigma = np.sqrt(w_sigma @ annual_cov_shrunk @ w_sigma)\n",
    "        plt.scatter(sigma, mu, marker=\"D\", color=color, edgecolor=\"black\", s=100, label=f\"{label} (σ-matched)\", zorder=4)\n",
    "\n",
    "    if w_mu is not None:\n",
    "        mu = w_mu @ mu_vec\n",
    "        sigma = np.sqrt(w_mu @ annual_cov_shrunk @ w_mu)\n",
    "        plt.scatter(sigma, mu, marker=\"s\", color=color, edgecolor=\"black\", s=100, label=f\"{label} (μ-matched)\", zorder=4)\n",
    "\n",
    "# Plot settings\n",
    "plt.title(\"Efficient Frontiers with σ- and μ-Matched Portfolios\", fontsize=16)\n",
    "plt.xlabel(\"Annualized Volatility (σ)\", fontsize=12)\n",
    "plt.ylabel(\"Annualized Expected Return (μ)\", fontsize=12)\n",
    "plt.grid(True, linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234cfde4",
   "metadata": {},
   "source": [
    "Basically all efficient frontiers suggest to put almost 90% of the portfolio weight into Growth ETFs and the rest into the short-term Treasury ETF to match VOO's volatility.   To match VOO's expected return, we would invest 3/4 in Growth ETFs and the rest in short-term Treasury.  This does make economic sense: short-term Treasuries have low return, but also low volatility, while the Russel 1000 Growth ETFs has one of the highest returns in the sample, but also one of the highest volatilities.  The short-term Treasury ETF is also not much correlated with other ETFs (as we can see in the heat map), so some combination of a Growth ETFs with high return/high volatility and a largely uncorrelated low return/low volatility treasury ETFs maximized the diversification potential while still matching VOO's return or risk.\n",
    "\n",
    "Graphically, we see that the different estimation variants for the efficient frontier perform quite similar, except for the mean-shrinkage method.  Ledoit-Wolf barely changes the covariance matrix, so it virtually delivers the same results as using the sample covariance.  Without regularization, only two ETFs are selected anyways, so we don't have an issue with many small weights.  The L1 regularization only penalizes the large weight on the Russel 1000 Growth ETFs and leads to the addition of another Growth ETF instead, which overall doesn't change much as the Mega Cap Growth ETF has a similar risk-return profile as the Russel 1000 Growth ETF.\n",
    "\n",
    "Mean shrinkage leads to generally inferior frontier estimates and perform worse than no shrinkage at all.  Since we have 120 monthly return observations per ETF, the estimates for the mean should be quite reliable with such a large sample, so shrinkage might not be needed.  Thus, I will not consider mean shrinkage further for now.  In later iterations of this notebook, we might need to come back to mean shrinkage if the out-of-sample performance for the portfolio optimization without it is terrible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b918e",
   "metadata": {},
   "source": [
    "Using the efficient frontier with raw sample mean and covariance without regularization, we can build a nice little dashboard that allows to select points along the efficient frontier and calculates the optimal weights for that point.  As expected, mostly only the weighting between the short-term Treasury ETf and Growth ETF change; the more risk I am willing to accept, the more weight goes into the Growth ETF and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2461c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas   = np.array(ef_raw[\"sigma\"])\n",
    "mus      = np.array(ef_raw[\"mu\"])\n",
    "weights  = np.array(ef_raw[\"weights\"])          # (n_points, n_assets)\n",
    "symbols  = etf_symbols                          # ticker list\n",
    "name_map = etf_name_map                         # dict ticker → long name\n",
    "\n",
    "fig = go.FigureWidget()\n",
    "fig.layout.hovermode = \"closest\"\n",
    "fig.layout.clickmode = \"event+select\"\n",
    "\n",
    "fig.add_scatter(x=sigmas, y=mus, mode=\"lines\",\n",
    "                line=dict(color=\"lightgray\"), hoverinfo=\"skip\", showlegend=False)\n",
    "\n",
    "fig.add_scatter(\n",
    "    x=sigmas,\n",
    "    y=mus,\n",
    "    mode=\"markers\",\n",
    "    marker=dict(size=10, color=\"black\"),\n",
    "    name=\"Raw Frontier\",\n",
    "    hovertemplate=\"σ: %{x:.2%}<br>μ: %{y:.2%}<br>(click to see top weights)<extra></extra>\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Efficient Frontier (Raw Estimates)\",\n",
    "    xaxis_title=\"Volatility (σ)\",\n",
    "    yaxis_title=\"Expected Return (μ)\",\n",
    "    height=480,\n",
    ")\n",
    "\n",
    "out = HTML()\n",
    "\n",
    "def format_html(idx: int) -> str:\n",
    "    \"\"\"Return HTML string listing the three largest weights.\"\"\"\n",
    "    w = weights[idx]\n",
    "    top_idx = np.argsort(w)[-3:][::-1]\n",
    "    \n",
    "    rows = []\n",
    "    for i in top_idx:\n",
    "        if w[i] > 0.001:  # ≥ 0.1 %\n",
    "            sym  = symbols[i]\n",
    "            name = name_map.get(sym, \"Unknown\")\n",
    "            rows.append(\n",
    "                f\"<li><b>{sym}</b> <span style='color:#666'>({name})</span>\"\n",
    "                f\"<span style='float:right'>{w[i]:.2%}</span></li>\"\n",
    "            )\n",
    "    \n",
    "    html = (\n",
    "        f\"<div style='font-family:Arial, sans-serif; font-size:14px;\"\n",
    "        f\"line-height:1.4; max-width:420px;'>\"\n",
    "        f\"<h4 style='margin:4px 0 8px 0; font-size:15px;'>\"\n",
    "        f\"Top 3 ETFs – Frontier Point {idx+1}</h4>\"\n",
    "        f\"<ul style='list-style:none; padding-left:0; margin:0;'>\"\n",
    "        + \"\\n\".join(rows) +\n",
    "        \"</ul></div>\"\n",
    "    )\n",
    "    return html\n",
    "\n",
    "def show_top_three(idx: int):\n",
    "    out.value = format_html(idx)\n",
    "\n",
    "def handle_click(trace, points, selector):\n",
    "    if points.point_inds:\n",
    "        show_top_three(points.point_inds[0])\n",
    "\n",
    "fig.data[1].on_click(handle_click)\n",
    "\n",
    "display(VBox([fig, out]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b650caf",
   "metadata": {},
   "source": [
    "### 4. Advanced Static Models & Robustness Checks\n",
    "\n",
    "In this section, we will implement more advance portfolio optimization model and robustness checks.  Specifically, we will run the following models:\n",
    "\n",
    "* **Resampled Efficient Frontier** \n",
    "* **Rolling Estimation**\n",
    "* **Black–Litterman** \n",
    "* **Risk Parity** \n",
    "* **Hierarchical Risk Parity** \n",
    "* **DCC‑GARCH** \n",
    "\n",
    "All of these models make economic sense, and there are situations in which they are useful (that's why I'm writing about them).  However, if there is one key take-away from this section, it is that for the setting we are looking at, long-only retail investor portfolios, it the simple mean-variance optimization from the previous section is hard to beat.  And more broadly, VOO itself is quite hard to beat as well, especially since many of the methods below implicitly or explicitly minimize portfolio volatility leading to low expected portfolio returns.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f2bc83",
   "metadata": {},
   "source": [
    "#### Resampling observations - Boostrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6318c2",
   "metadata": {},
   "source": [
    "Bootstrapping tackles uncertainty about the optimal portfolio estimation by repeatedly drawing bootstrap samples of the historical return matrix, estimate the efficient frontier for each sample, and averaging across the resulting weight vectors.  This Monte-Carlo overlay delivers two key benefits: robustness, because it dampens the impact of noisy single-sample estimates of the expected mean and covariance, and diversification, because extreme corner solutions are naturally pulled toward the centre.  Given our ETF universe—only a decade of monthly data spread across many assets—resampling provides a pragmatic hedge against estimation error without forcing Bayesian priors. The trade-off is extra compute time and the risk of diluting genuine alpha if the true optimum is highly concentrated.  I am less worried about missing the optimum, but the computational burden leads me to only estimate 30 points for each efficient frontier.  For the optimal portfolio, I average across the bootstrapped optimal portfolios that match VOO's volatility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69553ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrapping  addresses \"estimation error\" by creating many new return\n",
    "# datasets via bootstrapping. The final portfolio is the average of all optimal\n",
    "# portfolios found, leading to a more diversified and stable allocation.\n",
    "print(f\"Running Resampled Frontier with {RESAMPLE_ITERATIONS} iterations...\")\n",
    "n_obs, n_assets = returns_monthly.shape\n",
    "resampled_weights_list = []\n",
    "\n",
    "for i in range(RESAMPLE_ITERATIONS):\n",
    "    if (i + 1) % 25 == 0:\n",
    "        print(f\"  Resample iteration {i + 1}/{RESAMPLE_ITERATIONS}...\")\n",
    "\n",
    "    # Create a bootstrap sample of the monthly returns\n",
    "    boot_indices = np.random.choice(n_obs, size=n_obs, replace=True)\n",
    "    returns_boot = returns_monthly.iloc[boot_indices]\n",
    "\n",
    "    # Recalculate parameters for the bootstrap sample\n",
    "    mu_boot = (returns_boot.mean().values * 12) - expense_vector\n",
    "    try:\n",
    "        # Use shrunk covariance for better stability in each resampled data set\n",
    "        cov_boot = LedoitWolf().fit(returns_boot.values).covariance_ * 12\n",
    "        # Generate frontier and select the risk-matched portfolio\n",
    "        ef_boot = efficient_frontier(cov_boot, mu_boot, n_points=30)\n",
    "        _, w_boot = select_portfolio(ef_boot, \"sigma\", voo_sigma_annual)\n",
    "        if w_boot is not None:\n",
    "            resampled_weights_list.append(w_boot)\n",
    "    except (ValueError, np.linalg.LinAlgError):\n",
    "        # Skip iteration if the solver or covariance estimation fails\n",
    "        continue\n",
    "\n",
    "# The final resampled portfolio is the average of weights from all iterations\n",
    "if resampled_weights_list:\n",
    "    w_resampled = np.mean(resampled_weights_list, axis=0)\n",
    "    w_resampled /= w_resampled.sum()           # <-- ensures Σw = 1\n",
    "    \n",
    "    print(\"\\nTop 3 ETFs for Resampled Portfolio:\")\n",
    "    top_indices = np.argsort(w_resampled)[-3:][::-1]\n",
    "    for i in top_indices:\n",
    "        symbol = etf_symbols[i]\n",
    "        name = etf_name_map.get(symbol, \"Unknown\")\n",
    "        print(f\"  {symbol} ({name}): {w_resampled[i]:.2%}\")\n",
    "else:\n",
    "    w_resampled = None\n",
    "    print(\"\\nCould not generate a resampled portfolio.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29c5c6",
   "metadata": {},
   "source": [
    "Across the bootstrapped optimal portfolios, we actually get a pretty similar results as for the static optimization: we should invest most of our assets into Growth ETFs.  This is more a confirmation of our static, one-shot mean-variance optimization from above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7284c8",
   "metadata": {},
   "source": [
    "#### Rolling estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa4f713",
   "metadata": {},
   "source": [
    "We can check how the efficient frontier estimates change over time.  Ideally, the optimal portfolios are not very sensitive to the estimation window, so we don't have to frequently re-balance. \n",
    "\n",
    "This rolling-window exercise re-estimates the mean–variance optimization for every month using the most recent 60 months of data and selecting for each window the optimal portfolio that matches VOO's volatility.  Plotting the time-series of the top-3 ETFs reveals how sensitive the strategy is to new information: a stable profile indicates robustness and low turnover, while sharp oscillations indicate that the efficient frontier is quite sensitive to which historic data go into the model and for what period it is estimated.  \n",
    "\n",
    "For this model, since we use fewer observations for estimating the covariance matrix, Ledoit-Wolf covariance helps reduce the resulting sampling noise.  I still use the rolling sample mean for optimization as the shrinkage results for the overall sample performed quite poorly, and 60 observations should be enough for a reliable estimate of average returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3232afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This analysis shows how the optimal allocation would have changed over time,\n",
    "# providing insight into the strategy's stability.\n",
    "rolling_dates = returns_monthly.index[ROLLING_WINDOW_MONTHS:]\n",
    "rolling_weights_list = []\n",
    "\n",
    "for date in rolling_dates:\n",
    "    # Create a data window of the last N months\n",
    "    window_data = returns_monthly.loc[:date].iloc[-ROLLING_WINDOW_MONTHS:]\n",
    "    # Estimate parameters on the window\n",
    "    mu_roll = (window_data.mean().values * 12) - expense_vector\n",
    "    cov_roll = LedoitWolf().fit(window_data.values).covariance_ * 12\n",
    "    try:\n",
    "        # Find the optimal (risk-matched) portfolio for this period\n",
    "        ef_roll = efficient_frontier(cov_roll, mu_roll, n_points=30)\n",
    "        _, w_roll = select_portfolio(ef_roll, \"sigma\", voo_sigma_annual)\n",
    "        if w_roll is not None:\n",
    "            rolling_weights_list.append(pd.Series(w_roll, index=etf_symbols, name=date))\n",
    "    except (ValueError, np.linalg.LinAlgError):\n",
    "        continue\n",
    "\n",
    "# Combine results and plot the weight changes for the most important assets\n",
    "if rolling_weights_list:\n",
    "    rolling_weights_df = pd.concat(rolling_weights_list, axis=1).T\n",
    "    # Identify the top 5 ETFs by average weight over time\n",
    "    top_etfs = rolling_weights_df.mean().sort_values(ascending=False).head(5).index\n",
    "    rolling_weights_df[top_etfs].plot(\n",
    "        figsize=(12, 7), title=\"Top 5 ETF Weights Over Time (Rolling Optimization)\", lw=2\n",
    "    )\n",
    "    plt.ylabel(\"Portfolio Weight\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.grid(True, linestyle=\"--\")\n",
    "    plt.legend(title=\"ETFs\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1847453",
   "metadata": {},
   "source": [
    "The results are quite sobering.  The weights move around quite a lot.  However, the broad strokes are similar to the static optimization as the Russell 1000 Growth ETF (VONG), and the Mega Cap Growth ETF (MGK), are frequently the top weighted funds.  The Extended Duration Treasury ETF (EDV), has becomes more popular since mid 2023, which makes sense following the rise in interest rates during that period.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c61494f",
   "metadata": {},
   "source": [
    "#### Black-Litterman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16accced",
   "metadata": {},
   "source": [
    "Black–Litterman is a popular Bayesian portfolio construction framework.  The Black–Litterma model uses market-implied equilibrium returns (the “prior”) and then tilts them toward investor views, producing a posterior return vector that is both economically grounded and data-driven.  \n",
    "\n",
    "In my implementation the prior is a shrunk blend of the sample mean returns and the grand mean, very similar to what we tried above for James-Stein.  It is a light-touch Bayesian shrinkage, and we can allow for even more flexibility by running grid search to find the best shrinkage parameter.  In terms of investor views, I want to impose no specific assumptions, like by how much a specific ETFs outperforms the rest, but rather an economically defensible approach.  I will assume that the ETFs excess returns are approximately equal to VOO’s.  This makes sense if VOO is close to the market portfolio, the the remaining ETFs should eventually perform about as good as the market in the long run.  Uncertainty in the prior (tau_prior) and in the views (omega_scale) are hyper-parameters that we can tune through a grid search, just as with the shrinkage parameter.  The objective also here should make economic sense and I chose again the parameter combination that maximizes the Sharpe ratio. \n",
    "\n",
    "Useful sources: the [documentation of PyPortfolioOpt](https://pyportfolioopt.readthedocs.io/en/latest/BlackLitterman.html) and Thomas Idzorek's [Step-by-Step Guide to the Black-Litterman Model](https://people.duke.edu/~charvey/Teaching/BA453_2006/Idzorek_onBL.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee27b91",
   "metadata": {},
   "source": [
    "Formally, in the Black–Litterman framework we first update the prior (equilibrium) mean returns to the posterior mean, making this model Bayesian-like.\n",
    "\n",
    "$$\n",
    "\\boldsymbol\\mu_{\\text{post}}\n",
    "=\n",
    "\\Bigl[(\\tau\\Sigma)^{-1}+P^{\\!\\top}\\Omega^{-1}P\\Bigr]^{-1}\n",
    "\\Bigl[(\\tau\\Sigma)^{-1}\\boldsymbol\\pi + P^{\\!\\top}\\Omega^{-1}\\mathbf Q\\Bigr]\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $\\boldsymbol\\pi$ – prior (market-implied) mean vector.  In our case this are the sample mean returns shrunk towards the grand mean.  The relevant shrinkage factor which determines how much weight is given to the grand mean is one of the hyperparameters that we need to tune.\n",
    "* $\\Sigma$ – asset-return covariance matrix.  Since we want to be robust here, we are using the Ledoit-Wolf shrunk covariance matrix.\n",
    "* $\\tau$ – scalar reflecting uncertainty in the prior.  We need to tune this hyperparameter.\n",
    "* $P$ – matrix that selects assets involved in each investor view.\n",
    "* $\\mathbf Q$ – vector of return views. This is combined with $P$ to represent the investor's views.\n",
    "* $\\Omega$ – diagonal matrix of view uncertainties (larger ⇒ softer views).  Also this parameter needs to be tuned.\n",
    "\n",
    "\n",
    "We still need to think through how we can incorporate our investor views into the model.  I will assume that in the long run, all return are circa the same as the market portfolio's return, i.e., VOO.  This is a bit restrictive as it imposes return expectations on all assets, but it is in the spirit of Bayesian shrinkage towards the grand mean, and $\\tau$ can be tuned to put less certainty on the prior. \n",
    "\n",
    "We can implement this assumption using the $P$ matrix and the $Q$ vector for each ETF.  Let $k$ index the $k$-th view, and let $j_k$ denote the ETF involved in that view:\n",
    "\n",
    "$$\n",
    "P_{k,j}=1,\\; P_{k,\\text{VOO}}=-1,\n",
    "\\quad\n",
    "Q_k = 0\n",
    "$$\n",
    "meaning *“ETF $j$ will perform about the same as VOO.”*  In other words, the expected excess return of ETF $j_k$ relative to VOO is zero.  Note that the total number of views is equal to the number of ETFs - 1 as we implement one view per asset other than VOO.\n",
    "\n",
    "Using these posterior returns, we choose portfolio weights $\\mathbf w$ by solving the long-only mean–variance optimization program:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{\\mathbf w}\\;&\\tfrac12\\,\\mathbf w^{\\!\\top}\\Sigma\\,\\mathbf w\n",
    "-\\boldsymbol\\mu_{\\text{post}}^{\\!\\top}\\mathbf w \\\\\n",
    "\\text{s.t. }&\\mathbf 1^{\\!\\top}\\mathbf w = 1,\\qquad\n",
    "\\mathbf w \\ge 0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782fad1f",
   "metadata": {},
   "source": [
    "Why do we not have the expected return not as a constraint but as part of the objective function here?  The reason is that we are estimating an equilibrium that maximizes the investors expected utility, based on her views.  We assume here that the investor has a simple **quadratic utility function**:\n",
    "\n",
    "$$\n",
    "U(\\mathbf{w}) = \\boldsymbol{\\mu}^\\top \\mathbf{w} - \\frac{\\gamma}{2} \\, \\mathbf{w}^\\top \\Sigma \\mathbf{w}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbf{w}$ is the vector of portfolio weights.\n",
    "- $\\boldsymbol{\\mu}$ is the vector of expected returns.\n",
    "- $\\Sigma$ is the covariance matrix of asset returns.\n",
    "- $\\gamma > 0$ is the investor's risk aversion coefficient.\n",
    "\n",
    "\n",
    "This utility function reflects a tradeoff between higher returns, $\\boldsymbol{\\mu}^\\top \\mathbf{w}$, and higher portfolio risk, $\\mathbf{w}^\\top \\Sigma \\mathbf{w}$.  The values of $\\gamma$ indicates the investor's risk aversion, the more risk averse the more risky, high-volatility portfolios are penalized, thus acts as the measure of absolute risk aversion.  Quadratic utility functions are frequently used in Finance due to their easy tractability.  We will do so as well and just set $\\gamma$ to 1. \n",
    "\n",
    "The investor’s goal is to maximize $U(\\mathbf{w})$ — that is, to choose portfolio weights $\\mathbf{w}$ that achieve the best balance between return and risk given their risk aversion.  To solve the Black-Litterman model, we multiply the utility function by -1 to get a minimization problem and plug in the posterior return estimates and robust covariance matrix, as well as setting the constraints so that portfolio weights add up to 1 and are non-negative.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5498790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Black-Litterman model starts with market-implied equilibrium returns and\n",
    "# then tilts them based on an investor's specific views, creating a blended,\n",
    "# more intuitive set of expected returns for optimization.\n",
    "def run_black_litterman(\n",
    "    shrink_factor=0.3, tau_prior=0.20, omega_scale=0.02, print_summary=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs Black-Litterman optimization with tunable hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        shrink_factor (float): Weight on the sample mean in the prior;\n",
    "                               1-shrink_factor goes to the grand mean.\n",
    "        tau_prior (float): Prior uncertainty scale. Larger => prior is less certain.\n",
    "        omega_scale (float): Diagonal element for Omega matrix (view uncertainty).\n",
    "                             Larger => views are softer/less binding.\n",
    "        print_summary (bool): If True, prints portfolio weights and top holdings.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (weights_bl, mu_posterior)\n",
    "               - weights_bl (np.ndarray | None): Optimal long-only weights.\n",
    "               - mu_posterior (np.ndarray | None): Posterior expected-return vector.\n",
    "    \"\"\"\n",
    "    n_assets = len(etf_symbols)\n",
    "    # 1. Prior mean (pi) - a blend of sample mean and grand mean\n",
    "    grand_mean = annual_mu_sample.mean()\n",
    "    prior_mean = shrink_factor * annual_mu_sample + (1 - shrink_factor) * grand_mean\n",
    "\n",
    "    try:\n",
    "        # 2. Investor view: each ETF's return is approximately equal to VOO's return\n",
    "        idx_voo = etf_symbols.index(\"VOO\")\n",
    "        other_symbols = [sym for sym in etf_symbols if sym != \"VOO\"]\n",
    "        # P matrix selects assets involved in the view\n",
    "        P = np.zeros((len(other_symbols), n_assets))\n",
    "        for k, sym in enumerate(other_symbols):\n",
    "            P[k, returns_monthly.columns.get_loc(sym)] = 1  # +1 on ETF\n",
    "            P[k, idx_voo] = -1  # -1 on VOO\n",
    "        # Q vector contains the expected outperformance (\"approximately 0\" in this case)\n",
    "        Q = np.zeros(len(other_symbols))\n",
    "\n",
    "        # 3. View-uncertainty (Omega)\n",
    "        Omega = np.diag(np.full(len(Q), omega_scale))\n",
    "\n",
    "        # 4. Posterior mean calculation (core Black-Litterman formula)\n",
    "        inv_cov_prior = np.linalg.inv(tau_prior * annual_cov_shrunk)\n",
    "        inv_Omega = np.linalg.inv(Omega)\n",
    "        middle = np.linalg.inv(inv_cov_prior + P.T @ inv_Omega @ P)\n",
    "        mu_posterior = middle @ (inv_cov_prior @ prior_mean + P.T @ inv_Omega @ Q)\n",
    "\n",
    "        # 5. Optimize portfolio (mean-variance) using the posterior returns\n",
    "        P_qp = opt.matrix(annual_cov_shrunk)\n",
    "        q_qp = opt.matrix(-mu_posterior)\n",
    "        G_qp = opt.matrix(-np.eye(n_assets))  # w >= 0\n",
    "        h_qp = opt.matrix(np.zeros(n_assets))\n",
    "        A_qp = opt.matrix(np.ones((1, n_assets)))  # sum(w) = 1\n",
    "        b_qp = opt.matrix(1.0)\n",
    "\n",
    "        sol = opt.solvers.qp(P_qp, q_qp, G_qp, h_qp, A_qp, b_qp)\n",
    "        if sol[\"status\"] != \"optimal\":\n",
    "            raise RuntimeError(f\"CVXOPT failed: {sol['status']}\")\n",
    "\n",
    "        weights_bl = np.array(sol[\"x\"]).ravel()\n",
    "        weights_bl[weights_bl < 1e-7] = 0\n",
    "        weights_bl /= weights_bl.sum()\n",
    "\n",
    "        if print_summary:\n",
    "            print(\n",
    "                f\"\\n--- Black-Litterman (shrink={shrink_factor}, \"\n",
    "                f\"tau={tau_prior}, omega={omega_scale}) ---\"\n",
    "            )\n",
    "            print(\"Weights > 1%:\")\n",
    "            for i, w in enumerate(weights_bl):\n",
    "                if w > 0.01:\n",
    "                    print(f\"  {etf_symbols[i]}: {w:.1%}\")\n",
    "            top_idx = weights_bl.argsort()[-3:][::-1]\n",
    "            print(\"\\nTop 3 ETFs:\")\n",
    "            for i in top_idx:\n",
    "                sym = etf_symbols[i]\n",
    "                name = etf_name_map.get(sym, \"Unknown\")\n",
    "                print(f\"  {sym} ({name}): {weights_bl[i]:.2%}\")\n",
    "\n",
    "        return weights_bl, mu_posterior\n",
    "\n",
    "    except (ValueError, IndexError, np.linalg.LinAlgError) as e:\n",
    "        if print_summary:\n",
    "            print(f\"Black-Litterman failed: could not locate VOO or matrix error. {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea463e5",
   "metadata": {},
   "source": [
    "There are several hyperparameters that we need to decide upon, specifically the shrinkage factor of expected returns towards the grand mean, the uncertainty about the prior, $\\tau$, and the uncertainty about the views, $\\Omega$.  I will use again grid search to find the hyperparameter setting that maximizes the Sharpe ratio of the optimal portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58f6b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Tuning Black-Litterman Hyperparameters ---\")\n",
    "shrink_grid = [0.2, 0.4, 0.6, 0.8]\n",
    "tau_grid = [0.05, 0.10, 0.20, 0.30, 0.40]\n",
    "omega_grid = [0.005, 0.01, 0.02, 0.05]\n",
    "records = []\n",
    "\n",
    "for shrink in shrink_grid:\n",
    "    for tau in tau_grid:\n",
    "        for omega in omega_grid:\n",
    "            w, mu_post = run_black_litterman(shrink, tau, omega, print_summary=False)\n",
    "            if w is None:\n",
    "                continue\n",
    "            exp_ret = float(w @ mu_post)\n",
    "            vol = float(np.sqrt(w @ annual_cov_shrunk @ w))\n",
    "            sharpe = exp_ret / vol if vol else np.nan\n",
    "            records.append(\n",
    "                {\n",
    "                    \"shrink\": shrink,\n",
    "                    \"tau\": tau,\n",
    "                    \"omega\": omega,\n",
    "                    \"exp_return\": exp_ret,\n",
    "                    \"vol\": vol,\n",
    "                    \"sharpe\": sharpe,\n",
    "                    \"top_weight\": w.max(),\n",
    "                    \"assets_>1pct\": (w > 0.01).sum(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "tune_df = pd.DataFrame(records).sort_values(\"sharpe\", ascending=False)\n",
    "print(\"Tuning results (top 10 by in-sample Sharpe):\")\n",
    "print(tune_df.head(10))\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Rerun with the best parameters found and print a summary\n",
    "print(\"\\n--- Final Black-Litterman Portfolio (using best tuned parameters) ---\")\n",
    "best_row = tune_df.iloc[0]\n",
    "w_bl_opt, mu_bl = run_black_litterman(\n",
    "    best_row[\"shrink\"], best_row[\"tau\"], best_row[\"omega\"], print_summary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d036a26",
   "metadata": {},
   "source": [
    "These results look quite different to what we have seen so far.  While we again have a combination of Treasury ETFs and Growth ETFs, Black-Litterman clearly prefers to most weight into Treasuries.  This makes sense as we shrink the mean returns of the ETFs towards the market return, leading to the posterior returns being quite similar across ETFs.  If expected returns are similar across assets, Black-Litterman only cares about minimizing portfolio volatility, exactly what Treasury ETFs provide.  \n",
    "\n",
    "By the way, enforcing minimum returns in this setting, for example by setting constraints such that the optimal portfolios should deliver at least 80% of VOO's expected return, frequently leads to the algorithm not converging.  Mean shrinkage might make it infeasible to reach higher expected returns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85226116",
   "metadata": {},
   "source": [
    "#### Risk Parity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31696472",
   "metadata": {},
   "source": [
    "The Risk Parity builds a portfolio in which each ETF contributes the same percentage to total portfolio volatility, intentionally ignoring expected returns.  This “equal-risk” stance delivers two practical benefits: first, diversification that is easy to explain to non-quants: no single asset can dominate portfolio risk.  Second, robustness to return-forecast error.  The trade-offs is that in can overweight low-volatility assets and under-deliver on absolute returns.  The code implements the idea by computing marginal risk contributions, minimizing the squared deviation of each asset’s risk share from the equal-split target, and enforcing long-only, fully­-invested weights. \n",
    "\n",
    "Useful source: [Daniel Palomar's presentation](https://palomar.home.ece.ust.hk/ELEC5470_lectures/slides_risk_parity_portfolio.pdf) on pp. 31-32.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1163a36e",
   "metadata": {},
   "source": [
    "Formally, in risk-parity portfolios we ignore expected returns and aim for every asset to shoulder the same share of total portfolio volatility.\n",
    "Given weights $\\mathbf w$ and covariance matrix $\\Sigma$, we can define the following:\n",
    "\n",
    "* **Portfolio volatility**\n",
    "\n",
    "  $$\n",
    "  \\sigma_p=\\sqrt{\\mathbf w^{\\!\\top}\\Sigma\\,\\mathbf w}\n",
    "  $$\n",
    "\n",
    "* **Marginal risk contribution (MRC)** of asset $i$\n",
    "\n",
    "  $$\n",
    "  \\text{MRC}_i = \\frac{\\partial \\sigma_p}{\\partial w_i} = \\frac{(\\Sigma\\,\\mathbf w)_i}{\\sigma_p}\n",
    "  $$\n",
    "\n",
    "* **Total (percentage) risk contribution**\n",
    "\n",
    "  $$\n",
    "  \\text{RC}_i = w_i \\cdot \\text{MRC}_i = \\frac{w_i \\cdot (\\Sigma \\, \\mathbf{w})_i}{\\sigma_p}\n",
    "  $$\n",
    "\n",
    "Risk parity asks for\n",
    "\n",
    "$$\n",
    "\\text{RC}_1=\\text{RC}_2=\\dots=\\text{RC}_n=\\frac{\\sigma_p}{n}.\n",
    "$$\n",
    "\n",
    "A convenient way to reach this is to choose $\\mathbf w$ that solves\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{\\mathbf w}\\;&\\sum_{i=1}^{n}\\bigl(\\text{RC}_i-\\sigma_p/n\\bigr)^2 \\\\[4pt]\n",
    "\\text{s.t. }&\\sum_{i=1}^{n} w_i = 1,\\qquad w_i \\ge 0.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The [`SLSQP routine`](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html) in the code numerically searches for these weights, starting from an equal-weight guess and iteratively shrinking the squared differences between each asset’s risk contribution and the target until all assets contribute the same fraction of total portfolio risk.  SLSQP, or Sequential Least Squares Programming, is quite handy here as it as it works with a non-linear objective function, accepts constraints and boundary conditions, easy to implement, and as added bonus it is deterministic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24a79be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk Parity aims to construct a portfolio where each asset contributes equally\n",
    "# to the total portfolio risk. It ignores expected returns.\n",
    "def portfolio_volatility(weights, cov_matrix):\n",
    "    \"\"\"Calculates the annualized volatility of a portfolio.\"\"\"\n",
    "    return np.sqrt(weights.T @ cov_matrix @ weights)\n",
    "\n",
    "\n",
    "def risk_contributions(weights, cov_matrix):\n",
    "    \"\"\"Calculates each asset's percentage contribution to total portfolio risk.\"\"\"\n",
    "    port_vol = portfolio_volatility(weights, cov_matrix)\n",
    "    if port_vol == 0:\n",
    "        return np.zeros_like(weights)\n",
    "    # Marginal Risk Contribution (MRC) = (Cov * w) / sigma_p\n",
    "    mrc = (cov_matrix @ weights) / port_vol\n",
    "    # Total Risk Contribution = w_i * MRC_i\n",
    "    return weights * mrc\n",
    "\n",
    "\n",
    "def risk_parity_objective(weights, cov_matrix):\n",
    "    \"\"\"\n",
    "    Objective function for the optimizer. It seeks to minimize the\n",
    "    variance of risk contributions across all assets, forcing them to be equal.\n",
    "    \"\"\"\n",
    "    total_risk_contribs = risk_contributions(weights, cov_matrix)\n",
    "    # Target is an equal contribution from each asset\n",
    "    target_contribution = total_risk_contribs.sum() / len(weights)\n",
    "    # Minimize the squared differences from this target\n",
    "    return np.sum((total_risk_contribs - target_contribution) ** 2)\n",
    "\n",
    "\n",
    "# Solve the optimization problem\n",
    "initial_weights = np.ones(n_assets) / n_assets\n",
    "bounds = tuple((0.0, 1.0) for _ in range(n_assets))\n",
    "constraints = {\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1.0}\n",
    "\n",
    "result = minimize(\n",
    "    fun=risk_parity_objective,\n",
    "    x0=initial_weights,\n",
    "    args=(annual_cov_sample,),\n",
    "    method=\"SLSQP\",\n",
    "    bounds=bounds,\n",
    "    constraints=constraints,\n",
    "    options={\"disp\": False},\n",
    ")\n",
    "\n",
    "rp_weights = result.x if result.success else None\n",
    "if rp_weights is not None:\n",
    "    print(\"Optimal Risk Parity Portfolio (weights > 1%):\")\n",
    "    for i, weight in enumerate(rp_weights):\n",
    "        if weight > 0.01:\n",
    "            print(f\"  - {etf_symbols[i]}: {weight:.1%}\")\n",
    "    print(\"\\nTop 3 ETFs for Risk Parity Portfolio:\")\n",
    "    top_indices = np.argsort(rp_weights)[-3:][::-1]\n",
    "    for i in top_indices:\n",
    "        symbol = etf_symbols[i]\n",
    "        name = etf_name_map.get(symbol, \"Unknown\")\n",
    "        print(f\"  {symbol} ({name}): {rp_weights[i]:.2%}\")\n",
    "else:\n",
    "    print(\"Risk Parity optimization failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f6ea8",
   "metadata": {},
   "source": [
    "As risk parity tries to distribute the risk across portfolio assets, we get many small weights across a lot of ETFs.  But it also seems that the highest weights go again to the low-volatility treasury ETFs, not great for expected returns.  We could implement something like L1 regularization also here to penalize small weights, but it likely would not lead to less weight on Treasury ETFs; they are low-volatility and will have large weights as their marginal risk contribution is small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6efd4b3",
   "metadata": {},
   "source": [
    "#### Hierarchical Risk Parity (HRP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9141b4",
   "metadata": {},
   "source": [
    "Hierarchical Risk Parity (HRP) is a machine-learning-inspired take on risk parity portfolio optimization.  It follows a three-step, graph-theoretic workflow: \n",
    "\n",
    "1. **Clustering**: Convert the correlation matrix into a distance matrix and apply hierarchical clustering to group strongly related ETFs based on their co-movements.\n",
    "\n",
    "2. **Quasi-diagonalization**: Reorder the covariance matrix based on the clustering results so that similar assets are placed next to each other. This reveals a block-like structure and reflects asset similarity.\n",
    "\n",
    "3. **Recursive bisection**: The algorithm recursively splits the ordered list of assets in half and allocates capital inversely proportional to each sub-cluster’s variance, proceeding top-down through the tree.\n",
    "\n",
    "The result is a portfolio that automatically diversifies across statistically distinct clusters *without estimating expected returns.*  \n",
    "\n",
    "One big advantage of this method is that is allows for a visual representation of the clusters via a dendrogram.  The disadvantage of the method is that is also will weight heavily on clusters with low volatility.  What we can do, though, is to leverage this to our advantage and improve on the two-asset allocation we found through the static mean-variance optimization.  We can take the portfolio that we found that matches VOO's volatility from the static optimization above (which mostly consists of Growth ETFs) and blend it with the optimal portfolio from HRP, which will have lower volatility and lower expected return, to match VOO's expected return.  Basically, we can find with this method a better low-volatility portfolio than the Treasury ETF we found from the static portfolio optimization.\n",
    "\n",
    "Useful sources: [Hudson and Thames introduction to HRP article](https://hudsonthames.org/an-introduction-to-the-hierarchical-risk-parity-algorithm/) and the original paper by Marcos Lopez de Prado, *Building Diversified Portfolios that Outperform Out-of-Sample*; with the [working paper version on SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2708678)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c397c95",
   "metadata": {},
   "source": [
    "The logic for HRP is based on hierarchical clustering to arrange the assets into clusters based on their correlation and then walk through the tree to assign portfolio weight inversely to each sub-cluster's variance.\n",
    "\n",
    "1. **Convert correlations to “distances”**\n",
    "\n",
    "   For any pair of assets $i,j$ with correlation $\\rho_{ij}$, we define a distance metric as:\n",
    "\n",
    "   $$\n",
    "   d_{ij}= \\sqrt{\\tfrac{1}{2}\\,\\bigl(1-\\rho_{ij}\\bigr)}\n",
    "   $$\n",
    "\n",
    "   A smaller value of $d_{ij}$ means the assets move together more closely.\n",
    "\n",
    "2. **Build the hierarchy**\n",
    "\n",
    "   Apply agglomerative clustering (e.g. *single-linkage*) on the distance matrix $D=(d_{ij})$ to obtain a dendrogram.  What this does is iteratively merge pairs of assets and then clusters based on the minimum distance (in this case, the minimum pair-wise distance for all elements in each cluster).  In the last step, we end up with one cluster containing all assets.  In effect, the algorithm *builds the tree from the leaves to the root*, starting with individual assets and successively merging them into larger clusters.\n",
    "\n",
    "   The final dendrogram gives an ordering (seriation) that groups similar assets next to one another, which we can use to rearrange the covariance matrix.\n",
    "\n",
    "   This is a straightforward approach which we can complete with Python libraries such as [`scipy`](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html) relatively easily.  The original paper provides a step-by-step algorithm to identify clusters and quasi-diagonalize the matrix.  It is not essential that we follow here the same approach as the paper, we only need to find a way to seriate the covariance matrix and remain aware of subtle difference, such as specific tie-breaking rules.\n",
    "\n",
    "3. **Recursive Bisection Allocation**\n",
    "\n",
    "   Move **backward** through the hierarchical tree, starting at the root cluster that contains all assets, and recursively bisect until each individual asset (leaf) is assigned its final weight.\n",
    "\n",
    "   At each step:\n",
    "\n",
    "   1. Split the current cluster into two sub-clusters: **left** $C_L$ and **right** $C_R$.\n",
    "\n",
    "   2. Allocate the cluster's total weight to the two halves *inversely proportional to the sub-clusters' variances* — i.e., assign more weight to the less volatile sub-cluster:\n",
    "\n",
    "   $$\n",
    "   \\alpha = 1 - \\frac{\\sigma_{C_L}^2}{\\sigma_{C_L}^2 + \\sigma_{C_R}^2},\n",
    "   \\qquad\n",
    "   \\begin{cases}\n",
    "   w_{C_L} \\gets w_{C_L} \\cdot \\alpha, \\\\[4pt]\n",
    "   w_{C_R} \\gets w_{C_R} \\cdot (1 - \\alpha)\n",
    "   \\end{cases}\n",
    "   $$\n",
    "\n",
    "   To compute the variance of a cluster $C$ with covariance sub-matrix $\\Sigma_C$ and asset set $k \\in C$:\n",
    "\n",
    "   1. Compute inverse-variance weights:\n",
    "\n",
    "   $$\n",
    "   \\tilde{w}_k = \\frac{1}{\\sigma_k^2},\n",
    "   \\qquad\n",
    "   w_k = \\frac{\\tilde{w}_k}{\\sum_{j \\in C} \\tilde{w}_j}\n",
    "   $$\n",
    "\n",
    "   where $\\sigma_k^2 = \\Sigma_{kk}$ is the variance of asset $k$.\n",
    "\n",
    "   2. Use these weights to calculate the cluster variance:\n",
    "\n",
    "   $$\n",
    "   \\sigma_C^2 = \\mathbf{w}_C^{\\!\\top} \\, \\Sigma_C \\, \\mathbf{w}_C\n",
    "   $$\n",
    "\n",
    "   Repeat the recursive bisection process until every leaf node (individual asset) receives its final weight in the portfolio.\n",
    "\n",
    "The final weight vector $\\mathbf w$ is long-only, sums to one, and tends to equalize risk across clusters without solving a quadratic program.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f435377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HRP uses graph theory and machine learning to build a diversified portfolio.\n",
    "# It clusters assets, reorders the covariance matrix, and allocates weights recursively.\n",
    "def correlation_to_distance(corr_matrix):\n",
    "    \"\"\"Converts a correlation matrix to a distance matrix.\"\"\"\n",
    "    return np.sqrt(0.5 * (1 - corr_matrix))\n",
    "\n",
    "\n",
    "def get_cluster_variance(cov_matrix, cluster_indices):\n",
    "    \"\"\"Calculates the variance of a cluster of assets.\"\"\"\n",
    "    sub_cov = cov_matrix[np.ix_(cluster_indices, cluster_indices)]\n",
    "    # Inverse variance weights within the cluster\n",
    "    inv_var_weights = 1.0 / np.diag(sub_cov)\n",
    "    inv_var_weights /= inv_var_weights.sum()\n",
    "    return inv_var_weights.T @ sub_cov @ inv_var_weights\n",
    "\n",
    "\n",
    "def recursive_bisection(cov_matrix, sorted_indices):\n",
    "    \"\"\"Recursively splits weights between asset clusters.\"\"\"\n",
    "    weights = pd.Series(1.0, index=sorted_indices)\n",
    "    clusters = [sorted_indices]\n",
    "    while clusters:\n",
    "        cluster = clusters.pop(0)\n",
    "        if len(cluster) <= 1:\n",
    "            continue\n",
    "        # Bisect the cluster\n",
    "        split_point = len(cluster) // 2\n",
    "        left_cluster, right_cluster = cluster[:split_point], cluster[split_point:]\n",
    "        # Calculate variance for each sub-cluster\n",
    "        var_left = get_cluster_variance(cov_matrix, left_cluster)\n",
    "        var_right = get_cluster_variance(cov_matrix, right_cluster)\n",
    "        # Allocate weights inversely to cluster variance\n",
    "        alpha = 1.0 - var_left / (var_left + var_right)\n",
    "        weights[left_cluster] *= alpha\n",
    "        weights[right_cluster] *= 1.0 - alpha\n",
    "        # Add the new sub-clusters to the list to be processed\n",
    "        clusters.extend([left_cluster, right_cluster])\n",
    "    return weights.sort_index()\n",
    "\n",
    "\n",
    "# Step 1: Hierarchical Clustering\n",
    "corr_matrix = returns_monthly.corr()\n",
    "dist_matrix = correlation_to_distance(corr_matrix)\n",
    "linkage_matrix = linkage(squareform(dist_matrix), method=\"single\")\n",
    "\n",
    "# Step 2: Quasi-Diagonalization (Seriation)\n",
    "# This reorders assets to place similar assets next to each other.\n",
    "sorted_indices = leaves_list(linkage_matrix)\n",
    "sorted_tickers = [etf_symbols[i] for i in sorted_indices]\n",
    "\n",
    "# Step 3: Recursive Bisection\n",
    "sorted_cov = returns_monthly[sorted_tickers].cov().values * 12\n",
    "hrp_weights_sorted = recursive_bisection(sorted_cov, np.arange(len(sorted_tickers)))\n",
    "\n",
    "# Map weights back to the original order of etf_symbols\n",
    "hrp_weights = np.zeros(n_assets)\n",
    "for i, ticker in enumerate(sorted_tickers):\n",
    "    original_idx = etf_symbols.index(ticker)\n",
    "    hrp_weights[original_idx] = hrp_weights_sorted.iloc[i]\n",
    "\n",
    "print(\"Optimal HRP Portfolio (weights > 1%):\")\n",
    "for i, weight in enumerate(hrp_weights):\n",
    "    if weight > 0.01:\n",
    "        print(f\"  - {etf_symbols[i]}: {weight:.1%}\")\n",
    "\n",
    "if hrp_weights is not None:\n",
    "    print(\"\\nTop 3 ETFs for HRP Portfolio:\")\n",
    "    top_indices = np.argsort(hrp_weights)[-3:][::-1]\n",
    "    for i in top_indices:\n",
    "        symbol = etf_symbols[i]\n",
    "        name = etf_name_map.get(symbol, \"Unknown\")\n",
    "        print(f\"  {symbol} ({name}): {hrp_weights[i]:.2%}\")\n",
    "\n",
    "# Optional: Visualize the asset hierarchy with a dendrogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(linkage_matrix, labels=sorted_tickers, leaf_rotation=90)\n",
    "plt.title(\"HRP Asset Clustering (Dendrogram)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e272d1",
   "metadata": {},
   "source": [
    "\n",
    "The dendrogram shows quite interesting risk clusters.  There are apparently two large clusters, with the right cluster including, for example, all the Mega Cap ETFs (MGC, MGK, and MGV), the Total Stock Market ETF (VTI), but also the Extended Duration Treasury ETF (EDV).  Apparently, the closest ETFs to the Mega Cap Growth ETFs (MGK) in terms of volatility is the Long-Term Corporate Bond ETF (VCLT).  Quite unexpected.  And for the Russell 1000 Growth ETF (VONG) the closest ETF is the Mid-Cap Growth ETF (VOT), which is a bit less unexpected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a64ba6",
   "metadata": {},
   "source": [
    "Hierarchical Risk Parity (HRP) tends to produce a minimum-variance portfolio, which is typically conservative and heavily tilted toward low-volatility assets.  If we aim to achieve higher expected returns, we need to consider blending this portfolio with higher-return alternatives. For instance, the mean-variance optimal portfolio often assigns substantial weight to high-return, high-volatility Growth ETFs and matches VOO's volatility.  We can use the HRP-derived minimum-variance portfolio as a low-risk anchor and then blend it with a Growth ETF portfolio to reach a desired return level—such as matching VOO's expected return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e25cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blend the HRP portfolio with the MVO portfolio (risk-matched to VOO)\n",
    "# to create a new portfolio that meets VOO's expected return.\n",
    "mu_hrp = hrp_weights @ annual_mu_sample\n",
    "mu_mv = w_sigma_raw @ annual_mu_sample\n",
    "# Calculate blending factor 'alpha'\n",
    "alpha = np.clip((voo_mu_annual - mu_hrp) / (mu_mv - mu_hrp + 1e-12), 0, 1)\n",
    "# Create blended weights\n",
    "w_tilt = (1 - alpha) * hrp_weights + alpha * w_sigma_raw\n",
    "w_tilt /= w_tilt.sum()\n",
    "sigma_tilt = np.sqrt(w_tilt @ annual_cov_sample @ w_tilt)\n",
    "\n",
    "print(f\"\\nBlended HRP-MVO portfolio: alpha = {alpha:.3f}\")\n",
    "print(f\"  Expected mu   = {w_tilt @ annual_mu_sample:.2%} (VOO: {voo_mu_annual:.2%})\")\n",
    "print(f\"  Volatility sigma = {sigma_tilt:.2%} (VOO: {voo_sigma_annual:.2%})\")\n",
    "print(\"\\nTop 3 ETFs for Blended HRP-MVO Portfolio:\")\n",
    "top_indices = np.argsort(w_tilt)[-3:][::-1]\n",
    "for i in top_indices:\n",
    "    symbol = etf_symbols[i]\n",
    "    name = etf_name_map.get(symbol, \"Unknown\")\n",
    "    print(f\"  {symbol} ({name}): {w_tilt[i]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3f36d2",
   "metadata": {},
   "source": [
    "As expected, the blended portfolio weighs heavily on the Russell 1000 Growth ETF, but now also includes, for example, the Short-Term Corporate Bond ETFs as low-volatility component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac75a2a4",
   "metadata": {},
   "source": [
    "#### DCC-GARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd46c08e",
   "metadata": {},
   "source": [
    "In this section, we will try to find the optimal portfolios accounting for changing volatility and covariances over time.  Dynamic Conditional Correlation (DCC)-GARCH extends classic GARCH by allowing both volatilities and cross-asset correlations to evolve over time.  This method allows us to model and predict the covariance matrix in a more dynamic fashion than just looking at historic returns.  The estimation procedure is quite parameter heavy, thus we need ideally a lot of observations.  For this, we switch to weekly ETF returns.  We then can fit for each ETF the a univariate GARCH(1,1) and get standardized residuals (residuals divided by the volatility estimate) and conditional variance estimates.  Then, we can estimate a two-parameter DCC(1,1) model via quasi-maximum likelihood.  We can forecast the 1-step-ahead covariance matrix with this and feed it into a monthly mean–variance optimization, allowing for moving portfolio weights and changing correlation relations between assets over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f647a43",
   "metadata": {},
   "source": [
    "Source are the original Engle paper, *Dynamic Conditional Correlation A Simple Class of Multivariate Generalized Autoregressive Conditional Heteroskedasticity Models*, an its [working paper version](https://pages.stern.nyu.edu/~rengle/dccfinal.pdf), Qunxing Pan's [*A Teaching Summary on the Estimation of DCC-type Models*](https://www.atlantis-press.com/article/125960938.pdf), and the website of [NYU's V-Lab](https://vlab.stern.nyu.edu/docs/correlation/GARCH-DCC)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5a574",
   "metadata": {},
   "source": [
    "Before we go into DCC, let us first understand how we can model an individual asset's volatility.  Many time series, such as those of ETFs, often exhibit volatility clustering — periods of high volatility tend to be followed by high volatility, and calm periods tend to follow calm periods.  This is inconsistent with the assumptions that ETFs have a constant variance, but we can use **GARCH models** (Generalized Autoregressive Conditional Heteroskedasticity model) to capture such changes of volatility over time.\n",
    "\n",
    "A GARCH(1,1) models the conditional variance of returns as a function of past squared shocks and the past variance:\n",
    "\n",
    "$$\n",
    "r_{i,t} = \\mu_i + \\varepsilon_{i,t}, \\qquad\n",
    "\\varepsilon_{i,t} = \\sigma_{i,t} z_{i,t}, \\quad\n",
    "z_{i,t} \\sim \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_{i,t}^2 = \\omega_i + a_i\\,\\varepsilon_{i,t-1}^2 + b_i\\,\\sigma_{i,t-1}^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\sigma_{i,t}^2$ is the conditional variance of ETF $i$ at time $t$,\n",
    "- $\\omega_i$, $a_i$, and $b_i$ are model parameters,\n",
    "- $\\varepsilon_{i,t}$ is the return shock (innovation),\n",
    "- $z_{i,t}$ is the standardized innovation, assumed to be i.i.d. standard normal.\n",
    "\n",
    "In estimation, the **standardised residuals** are then computed as: $z_{i,t} = \\varepsilon_{i,t}/\\sigma_{i,t}$.\n",
    "\n",
    "The GARCH(1,1) model thus captures how volatility evolves over time and allows us to forecast next-period volatility.\n",
    "\n",
    "GARCH models are only appropriate if the return series exhibits *autoregressive conditional heteroskedasticity (ARCH effects)* — that is, if the variance of the series changes over time in a predictable way.  Fortunately, we can test for this using *Engle’s ARCH test*, which is implemented in [`statsmodels`](https://www.statsmodels.org/dev/generated/statsmodels.stats.diagnostic.het_arch.html). The test checks whether there is significant autocorrelation in the squared residuals of a return series — a key sign of time-varying volatility.\n",
    "\n",
    "A useful short explanation of the test can also be found in the [MATLAB documentation](https://www.mathworks.com/help/econ/engles-arch-test.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b8cb54",
   "metadata": {},
   "source": [
    "To estimate the Dynamic Conditional Correlation (DCC) between assets, I first estimate univariate GARCH models for each ETF — after verifying that ARCH effects are present in the data.  Then, I fit the DCC model to the standardized residuals to produce a time-varying covariance matrix, allowing for one-step-ahead covariance forecasts.\n",
    "\n",
    "1. **Fit a standard GARCH(1, 1)** to every return series $r_{i,t}$.\n",
    "\n",
    "   This step is comparatively easy, since Python has standard libraries to fit GARCH models, such as [`arch`](https://arch.readthedocs.io/en/latest/univariate/generated/arch.univariate.GARCH.html#).  However, we need to be aware that these models typically need a lot of observation to lead to stable estimates.  Thus, I'm using weekly observations instead of monthly.  Note also that I scale the weekly returns by a factor of 100, since this makes fitting the models easier.\n",
    "\n",
    "2. **Estimate the dynamic correlation structure, DCC(1, 1)**\n",
    "\n",
    "   Collect the vector of standardized residuals at date $t$ as\n",
    "   $\\mathbf z_t = (z_{1,t},\\dots,z_{N,t})^\\top$.\n",
    "   \n",
    "   Let $Q_t$ be the covariance of $\\mathbf z_t$ and $\\bar{Q}$ the unconditional (sample) covariance matrix.  Then, we can fit the following model for $Q_t$ via log-likelihood:\n",
    "\n",
    "   $$\n",
    "   Q_t\n",
    "     \\;=\\;\n",
    "     (1-\\alpha-\\beta)\\,\\bar{Q}\n",
    "     +\\alpha\\,\\mathbf z_{t-1}\\mathbf z_{t-1}^\\top\n",
    "     +\\beta\\,Q_{t-1},\n",
    "     \\qquad 0<\\alpha,\\beta,\\ \\alpha+\\beta<1.\n",
    "   $$\n",
    "\n",
    "   This is a GARCH-like structure applied to the covariance matrix of standardized returns.\n",
    "\n",
    "   Finally, we convert $Q_t$ to the correlation matrix:\n",
    "\n",
    "   $$\n",
    "   R_t = \\bigl(\\operatorname{diag}Q_t\\bigr)^{-1/2} Q_t \\bigl(\\operatorname{diag}Q_t\\bigr)^{-1/2}.\n",
    "   $$\n",
    "\n",
    "   Note that for the quasi-log-likelihood minimization, we take the standardized residuals $\\mathbf z_t$ as observed input from the individual GARCH models, thus we can drop them from the estimation; only $\\alpha$ and $\\beta$ are estimated. \n",
    "\n",
    "3. **One-step-ahead forecast of the full covariance matrix**\n",
    "\n",
    "   To forecast the covariance matrix at time $T+1$, we combine the results from the DCC model and the univariate GARCH models. Two components are required:\n",
    "\n",
    "   *First*, we compute the dynamic correlation matrix $R_{T+1}$ from the DCC model.  \n",
    "   We obtain this by plugging the most recent standardized residual vector $\\mathbf{z}_T$, the last estimated $Q_T$, and the unconditional sample covariance matrix $\\bar{Q}$ into the DCC recursion:\n",
    "\n",
    "   $$\n",
    "   Q_{T+1} = (1 - \\alpha - \\beta)\\,\\bar{Q} + \\alpha\\,\\mathbf{z}_T \\mathbf{z}_T^\\top + \\beta\\,Q_T\n",
    "   $$\n",
    "\n",
    "   Then, convert $Q_{T+1}$ to a correlation matrix:\n",
    "\n",
    "   $$\n",
    "   R_{T+1} = \\left(\\operatorname{diag} Q_{T+1} \\right)^{-1/2} Q_{T+1} \\left(\\operatorname{diag} Q_{T+1} \\right)^{-1/2}\n",
    "   $$\n",
    "\n",
    "   *Second*, we construct the diagonal matrix $D_T$ using the last available conditional standard deviations from the univariate GARCH models:\n",
    "\n",
    "   $$\n",
    "   D_T = \\operatorname{diag}(\\sigma_{1,T}, \\dots, \\sigma_{N,T})\n",
    "   $$\n",
    "\n",
    "   We could forecast one-step-ahead variances $\\sigma_{i,T+1}^2$ from the GARCH models, but since the covariance structure is more important here, I simplify by using the most recent $\\sigma_{i,T}$ values instead.\n",
    "\n",
    "   The final forecast of the full covariance matrix at $T+1$ is then given by:\n",
    "\n",
    "   $$\n",
    "   \\Sigma_{T+1} = D_T\\,R_{T+1}\\,D_T\n",
    "   $$\n",
    "\n",
    "4. **Portfolio optimization with the forecasted covariance**\n",
    "\n",
    "   Feed $\\Sigma_{T+1}$ — and, if desired, a forecast of expected returns — into the mean-variance optimization routine from above to pick weights that target a chosen risk level, i.e., match VOO’s volatility.  Note that we are doing all of this on weekly basis.  To get to monthly weights, I just take the portfolio weight from the last week of the month to define the weight for the next month.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345f7e50",
   "metadata": {},
   "source": [
    "We start implementing the approach by defining some function for the DCC estimation, fitting, and forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e709e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dcc_negloglik(params, std_resids):\n",
    "    \"\"\"Negative log-likelihood of a DCC(1,1) model given standardized residuals.\"\"\"\n",
    "    T, N = std_resids.shape\n",
    "    alpha, beta = params\n",
    "    Qbar = np.cov(std_resids.T)  # Unconditional correlation\n",
    "    Qt = Qbar.copy()  # Initialize Qt_0\n",
    "    loglik = 0.0\n",
    "    for t in range(T):\n",
    "        eps_t = std_resids[t][:, None]  # (N,1)\n",
    "        Qt = (1 - alpha - beta) * Qbar + alpha * (eps_t @ eps_t.T) + beta * Qt\n",
    "        diag_sqrt = np.sqrt(np.diag(Qt))\n",
    "        diag_sqrt[diag_sqrt <= 1e-12] = 1e-12\n",
    "        Rt = Qt / np.outer(diag_sqrt, diag_sqrt)\n",
    "        inv_Rt = np.linalg.inv(Rt)\n",
    "        logdet_Rt = np.log(np.linalg.det(Rt))\n",
    "        loglik += logdet_Rt + eps_t.T @ inv_Rt @ eps_t\n",
    "    return 0.5 * loglik.item()\n",
    "\n",
    "\n",
    "def fit_dcc(std_resids, bounds=((1e-6, 1 - 1e-6),) * 2):\n",
    "    \"\"\"Estimates (alpha, beta) by QML; returns params and last Qt, Rt.\"\"\"\n",
    "    best_val, best_ab = np.inf, None\n",
    "    grid = np.linspace(0.01, 0.15, 5)\n",
    "    for a in grid:\n",
    "        for b in grid:\n",
    "            if a + b < 0.999:\n",
    "                val = _dcc_negloglik((a, b), std_resids)\n",
    "                if val < best_val:\n",
    "                    best_val, best_ab = val, (a, b)\n",
    "    res = minimize(\n",
    "        _dcc_negloglik,\n",
    "        x0=np.array(best_ab),\n",
    "        args=(std_resids,),\n",
    "        bounds=bounds,\n",
    "        constraints={\"type\": \"ineq\", \"fun\": lambda x: 0.999 - (x[0] + x[1])},\n",
    "    )\n",
    "    alpha, beta = max(res.x[0], 0), max(res.x[1], 0)\n",
    "    if alpha + beta >= 0.999:\n",
    "        beta = 0.999 - alpha - 1e-6\n",
    "\n",
    "    Qbar = np.cov(std_resids.T)\n",
    "    Qt = Qbar.copy()\n",
    "    for eps in std_resids:\n",
    "        eps = eps[:, None]\n",
    "        Qt = (1 - alpha - beta) * Qbar + alpha * (eps @ eps.T) + beta * Qt\n",
    "    Rt = Qt / np.outer(np.sqrt(np.diag(Qt)), np.sqrt(np.diag(Qt)))\n",
    "    return alpha, beta, Qt, Rt\n",
    "\n",
    "\n",
    "def forecast_dcc_cov_1step(std_resids, cond_vars):\n",
    "    \"\"\" Returns the 1-step-ahead covariance matrix using a DCC(1,1) model. \"\"\"\n",
    "    # Fit DCC and get last Q_T\n",
    "    alpha, beta, Q_last, _ = fit_dcc(std_resids)\n",
    "\n",
    "    # Unconditional correlation (Q_bar) and latest residual\n",
    "    Q_bar   = np.cov(std_resids.T)\n",
    "    eps_T   = std_resids[-1, :][:, None]        # column vector (N × 1)\n",
    "\n",
    "    # ---- 1-step DCC update --------------------------------------\n",
    "    Q_next = ((1.0 - alpha - beta) * Q_bar\n",
    "              + alpha * (eps_T @ eps_T.T)\n",
    "              + beta  * Q_last)\n",
    "\n",
    "    # Convert to correlation matrix\n",
    "    diag_sqrt = np.sqrt(np.diag(Q_next))\n",
    "    diag_sqrt[diag_sqrt <= 1e-12] = 1e-12       # numerical guard\n",
    "    R_next    = Q_next / np.outer(diag_sqrt, diag_sqrt)\n",
    "\n",
    "    # Scale by last conditional variances to get covariance\n",
    "    var_T     = cond_vars[-1]                   # last row\n",
    "    D_T       = np.diag(np.sqrt(var_T))\n",
    "    Sigma_next = D_T @ R_next @ D_T\n",
    "\n",
    "    return Sigma_next "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c093a09f",
   "metadata": {},
   "source": [
    "Next, collect weekly return data to fit our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587772e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare weekly data for better GARCH model estimation\n",
    "surviving_etf_symbols = returns_monthly.columns.tolist()\n",
    "all_prices_daily = pd.concat(\n",
    "    [get_total_return_series(t) for t in surviving_etf_symbols], axis=1\n",
    ").tz_localize(None)\n",
    "returns_weekly = (\n",
    "    all_prices_daily[surviving_etf_symbols]\n",
    "    .resample(\"W-FRI\")\n",
    "    .last()\n",
    "    .pct_change()\n",
    "    .loc[returns_monthly.index.min() :]\n",
    "    .dropna(how=\"all\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3053d9",
   "metadata": {},
   "source": [
    "Before fitting the DCC-GARCH model, we should check whether the ETF return series exhibit ARCH (Autoregressive Conditional Heteroskedasticity) effects.  To test for ARCH behavior, we can use Engle's ARCH test, which examines whether the squared residuals of a return series are significantly autocorrelated.  If the test is significant, it suggests the presence of time-varying volatility that can be modelled using GARCH.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d31e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Testing for GARCH(1,1) Effects in Weekly Returns ---\")\n",
    "for symbol in surviving_etf_symbols:\n",
    "    series = returns_weekly[symbol].dropna()\n",
    "    if len(series) < 20: continue\n",
    "    lm_test = het_arch(series, nlags=12)\n",
    "    print(f\"{symbol}: ARCH LM Test p-value = {lm_test[1]:.4f}\", end=\"\")\n",
    "    print(\"  -> ARCH effects detected\" if lm_test[1] < 0.05 else \"  -> No significant ARCH effects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4453af5",
   "metadata": {},
   "source": [
    "There are enough ETFs with ARCH effects which we can model using GARCH, so the DCC-GARCH model actually makes sense.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6cb3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Rolling DCC-GARCH Optimization ---\")\n",
    "try:\n",
    "    month_ends = returns_weekly.resample(\"ME\").last().index\n",
    "    voo_sigma_annual_weekly = returns_weekly[\"VOO\"].std() * np.sqrt(52)\n",
    "    rolling_dcc_weights_list = []\n",
    "\n",
    "    for date in month_ends:\n",
    "        data_window = returns_weekly.loc[:date]\n",
    "        if len(data_window) < 104: continue  # Need enough data\n",
    "\n",
    "        try:\n",
    "            print(f\"Optimizing DCC portfolio for month-end {date.date()}...\")\n",
    "            std_resids, cond_vars = [], []\n",
    "            scale = 100.0  # Rescale returns for better model fitting\n",
    "            for s in etf_symbols:\n",
    "                series = scale * data_window[s].dropna()\n",
    "                am = ConstantMean(series)\n",
    "                am.volatility = GARCH(1, 1)\n",
    "                res = am.fit(disp=\"off\")\n",
    "                std_resids.append(res.std_resid)\n",
    "                cond_vars.append((res.conditional_volatility / scale) ** 2)\n",
    "\n",
    "            std_resids = np.column_stack(std_resids)\n",
    "            cond_vars = np.column_stack(cond_vars)\n",
    "\n",
    "            forecasted_cov = forecast_dcc_cov_1step(std_resids, cond_vars) * 52\n",
    "            forecasted_mu = data_window.mean().values * 52\n",
    "\n",
    "            ef_dcc = efficient_frontier(forecasted_cov, forecasted_mu, n_points=30)\n",
    "            _, w_dcc = select_portfolio(ef_dcc, \"sigma\", voo_sigma_annual_weekly)\n",
    "            if w_dcc is not None:\n",
    "                rolling_dcc_weights_list.append(pd.Series(w_dcc, index=etf_symbols, name=date))\n",
    "        except Exception as e:\n",
    "            print(f\"  Skipped {date.date()} due to error: {e}\")\n",
    "  \n",
    "except Exception as e:\n",
    "    print(f\"Failed to run DCC-GARCH analysis. Error: {e}\")\n",
    "    dynamic_returns_series_dcc = pd.Series(dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a0fa1",
   "metadata": {},
   "source": [
    "We can now visualize the time series of optimal portfolio weights that match VOO's volatility.  To evaluate performance, we also calculate monthly portfolio returns, assuming that the portfolio weights estimated in the last week of each month are held constant throughout the following month.  This simulates a realistic strategy where portfolios are rebalanced on monthly basis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff66fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rolling_dcc_weights_list:\n",
    "    dcc_weights_df = pd.concat(rolling_dcc_weights_list, axis=1).T\n",
    "    top_dcc_etfs = dcc_weights_df.mean().sort_values(ascending=False).head(3).index\n",
    "    dcc_weights_df[top_dcc_etfs].plot(\n",
    "        figsize=(12, 7),\n",
    "        title=\"Top 3 ETF Weights Over Time (DCC-GARCH Optimization)\",\n",
    "        lw=2,\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # Create monthly return series for the DCC strategy\n",
    "    dcc_shifted_weights = dcc_weights_df.shift(1).reindex(returns_monthly.index).ffill()\n",
    "    common_idx = dcc_shifted_weights.dropna().index.intersection(returns_monthly.index)\n",
    "    aligned_weights = dcc_shifted_weights.loc[common_idx]\n",
    "    aligned_returns = returns_monthly.loc[common_idx]\n",
    "    dynamic_returns_series_dcc = pd.Series(\n",
    "        np.sum(aligned_weights.values * aligned_returns[etf_symbols].values, axis=1),\n",
    "        index=common_idx,\n",
    "    )\n",
    "else:\n",
    "    dynamic_returns_series_dcc = pd.Series(dtype=float)\n",
    "    print(\"No successful DCC-GARCH optimizations were completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109235ab",
   "metadata": {},
   "source": [
    "The optimal portfolio weights vary considerably over time. Interestingly, they tend to oscillate primarily between the Russell 1000 Growth ETF (VONG), the Mega Cap Growth ETF (MGK), and the Extended Duration Treasury ETF (EDV).  This makes economic sense: since overall market risk is governed by the conditional covariance structure, the optimizer shifts weight toward higher-return assets (i.e., growth-oriented ETFs) when the estimated market risk is low.  Conversely, when the conditional covariance is elevated — signaling higher market risk — the portfolio reallocate toward lower-volatility assets, such as long-duration Treasuries.  \n",
    "\n",
    "What is concerning, however, are the frequent extreme allocations, where the entire portfolio is concentrated in a single ETF.  Theses allocations can change sharply from one month to the next, increasing portfolio turnover and thus transaction costs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9042fd84",
   "metadata": {},
   "source": [
    "### 5. Regime‑Switching Model\n",
    "\n",
    "In this section, we will implement a more dynamic model that changes optimal portfolios based on the current state of the economy.  Specifically, we will estimate a two‑state Markov regime-switching model using macro and market factors to predict the next state of the economy.  Intuitively, we can think of this model splitting the time series of returns into, for example, low market risk and high market risk periods.  It would make sense to invest into more aggressive, growth heavy ETFs if we know the market is relatively calm, while during more uncertain periods it might be better to invest into more bond heavy ETFs that promise more stable return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652aa6d6",
   "metadata": {},
   "source": [
    "Sources are Andrew Filardo's paper, [*Choosing information variables for transition probabilities in a time-varying transition probability Markov switching model*](https://www.kansascityfed.org/documents/7002/choosing-information-variables-for-transition-probabilities-filardo-pdf-rwp98-091.pdf), the book chapter by Francis Diebold et al.], [*Regime Switching with Time-Varying Transition Probabilities*](https://www.sas.upenn.edu/~fdiebold/papers/paper69/pa.dlw.pdf), the [statsmodel documentation for Markov switching dynamic regression models](https://www.statsmodels.org/dev/examples/notebooks/generated/markov_regression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57f1ee3",
   "metadata": {},
   "source": [
    "The Markov regime-switching model is relatively straightforward.  We model monthly returns of VOO, our proxy for the overall market return, as being generated by one of *K* unobserved regimes.  Each regime is characterized by its own mean and variance. The regime dynamics follow a *first-order Markov chain*, where the transition probabilities are modeled as a logit function of macroeconomic predictors, such as the previous month’s VIX.\n",
    "\n",
    "To determine the appropriate number of regimes *K* and the most relevant predictors, we perform a grid search and select the specification with the lowest Bayesian Information Criterion (BIC).  Additionally, we require that each regime includes a sufficient number of observations to allow for reliable estimation of regime-specific optimal portfolios.\n",
    "\n",
    "1. **Return equation (conditional on the latent regime)**\n",
    "\n",
    "   I assume that the market returns follow a Markov process.  I approximate the market returns with VOO:\n",
    "\n",
    "   $$\n",
    "   r_t \\;=\\; \\mu_{s_t} \\;+\\; \\varepsilon_t,\n",
    "   \\qquad\n",
    "   \\varepsilon_t \\sim \\mathcal N\\!\\bigl(0,\\;\\sigma_{s_t}^2\\bigr),\n",
    "   $$\n",
    "\n",
    "   where\n",
    "\n",
    "   * $r_t$ = monthly VOO return,\n",
    "   * $s_t \\in \\{1,\\dots,K\\}$ = unobserved regime at time $t$,\n",
    "   * each regime $j$ has its own mean $\\mu_j$ and variance $\\sigma_j^2$.\n",
    "\n",
    "2. **Regime dynamics: first-order Markov chain with time-varying transition probabilities (TVTP)**\n",
    "\n",
    "   To fit the Markov model, we assume that the transition probabilities between regimes are driven by macroeconomic conditions.  Conceptually, we can model these probabilities using a logit specification, where lagged economic indicators serve as predictors for the likelihood of moving from one regime to another.\n",
    "\n",
    "   To capture relevant economic conditions, we could consider a broad set of macroeconomic variables — such as unemployment rates, industrial production, yield spreads, inflation expectations, and others.  In practice, we perform a grid search over various combinations of predictors to identify the best specification.  We find that the most important predictor is the [**Volatility Index (VIX)**](https://www.cboe.com/tradable_products/vix/), which reflects the market's expected volatility over the next 30 days, based on S&P 500 options.  Since VIX provides a forward-looking measure of market uncertainty, it is a natural candidate for distinguishing between high- and low-risk regimes.\n",
    "\n",
    "   The transition probability from regime $i$ to regime $j$, conditional on lagged predictors $x_{t-1}$, is modeled as:\n",
    "\n",
    "   $$\n",
    "   \\Pr\\!\\bigl[s_t = j \\mid s_{t-1} = i,\\;x_{t-1}\\bigr]\n",
    "   \\;=\\;\n",
    "   \\pi_{ij}\\!\\left(x_{t-1}\\right),\n",
    "   $$\n",
    "\n",
    "   where $x_{t-1}% is a vector of lagged macroeconomic predictors:\n",
    "\n",
    "   $$\n",
    "   x_{t-1} =\n",
    "   \\begin{bmatrix}\n",
    "   1\\\\[2pt] \\text{VIX}_{\\,t-1}\\\\[2pt] \\vdots\n",
    "   \\end{bmatrix},\n",
    "   $$\n",
    "\n",
    "   and the logit link function defines the transition probabilities as:\n",
    "\n",
    "   $$\n",
    "   \\pi_{ij}(x) \\;=\\;\n",
    "   \\frac{\\exp\\!\\bigl(\\gamma_{ij}^\\top x\\bigr)}\n",
    "      {\\displaystyle\\sum_{\\ell=1}^{K}\\exp\\!\\bigl(\\gamma_{i\\ell}^\\top x\\bigr)},\n",
    "   \\quad \\text{for } j = 1, \\dots, K.\n",
    "   $$\n",
    "\n",
    "\n",
    "\n",
    "3. **Model selection and optimal regime portfolio estimation**\n",
    "\n",
    "   To complete the model, we must determine the number of regimes $K$ to include.  This is done through hyperparameter tuning, selecting the number of regimes that minimizes the BIC.  To ensure robustness—and to guarantee that we can estimate an optimal portfolio for each regime—we impose as constraint that each regime must contain a minimum number of observations.  This prevents the model from having regimes that occur only once or twice, which would reduce interpretability and out-of-sample stability.\n",
    "\n",
    "   Once the model is estimated, we obtain two key outputs:\n",
    "\n",
    "   - A time series of smoothed regime probabilities, indicating the likelihood of each regime at every point in time.\n",
    "   - A state sequence, representing the most likely regime at each time.\n",
    "\n",
    "   We then estimate the optimal portfolio for *each* regime, using only the observations assigned to that regime. \n",
    "\n",
    "   Finally, to construct forward-looking portfolios, we can predict the optimal allocation for the next month by computing a weighted average of the regime-specific portfolios.  The weights are given by the predicted regime probabilities, which depend on the most recent value of the exogenous predictors (e.g., VIX). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff22b1d",
   "metadata": {},
   "source": [
    "As first step, we load marco-economic variables from [FRED](https://fred.stlouisfed.org/) which we may use exogenous predictors for the transition probabilities in the Markov model.  We then align the indices of these predictors with the ETF return data to ensure consistency in the modeling timeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fccef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the seed before for the Markov regime fitting.  \n",
    "np.random.seed(42)  \n",
    "\n",
    "def get_fred_data(start, end):\n",
    "    \"\"\"\n",
    "    Fetch macro-financial predictors from FRED:\n",
    "    - VIX (market volatility)\n",
    "    - 3M and 10Y Treasury yields\n",
    "    - Fed Funds Rate\n",
    "    - 5-Year Inflation Expectations\n",
    "    - Unemployment Rate\n",
    "    - Consumer Sentiment\n",
    "\n",
    "    Also computes yield spreads:\n",
    "    - 10Y - 3M\n",
    "    - 10Y - Fed Funds\n",
    "    \"\"\"\n",
    "    print(\"Fetching FRED macro predictors...\")\n",
    "\n",
    "    symbols = {\n",
    "        \"3M\": \"DGS3MO\",               # 3-Month Treasury Yield (%)\n",
    "        \"10Y\": \"DGS10\",               # 10-Year Treasury Yield (%)\n",
    "        \"FedFunds\": \"FEDFUNDS\",      # Effective Federal Funds Rate (%)\n",
    "        \"VIX\": \"VIXCLS\",             # Volatility Index\n",
    "        \"T5YIE\": \"T5YIE\",            # 5-Year Inflation Expectations (%)\n",
    "        \"UNRATE\": \"UNRATE\",          # Unemployment Rate (%)\n",
    "        \"UMCSENT\": \"UMCSENT\",        # Consumer Sentiment Index\n",
    "        \"BAA10Y\": \"BAA10Y\",          # Moody’s Baa Corporate Bond Yield - 10Y Treasury\n",
    "        \"USSLIND\": \"USSLIND\",        # Leading Index for the United States\n",
    "        \"CPI\": \"CPIAUCSL\",           # Consumer Price Index (not seasonally adjusted)\n",
    "        \"INDPRO\": \"INDPRO\"           # Industrial Production Index\n",
    "    }\n",
    "\n",
    "\n",
    "    try:\n",
    "        df = DataReader(list(symbols.values()), \"fred\", start, end)\n",
    "        df.columns = list(symbols.keys())\n",
    "\n",
    "        # Convert interest rates from % to decimals where appropriate\n",
    "        for col in [\"3M\", \"10Y\", \"FedFunds\", \"T5YIE\", \"BAA10Y\"]:\n",
    "            df[col] = df[col] / 100.0\n",
    "\n",
    "        # Construct yield spread\n",
    "        df[\"Spread_10Y_3M\"] = df[\"10Y\"] - df[\"3M\"]\n",
    "\n",
    "        # Construct corporate bond spread (already spread vs 10Y)\n",
    "        df[\"CreditSpread\"] = df[\"BAA10Y\"]  # already a spread\n",
    "\n",
    "        # Transform CPI and INDPRO to YoY % change (inflation and output growth)\n",
    "        df[\"CPI\"]    = df[\"CPI\"].ffill()\n",
    "        df[\"INDPRO\"] = df[\"INDPRO\"].ffill()\n",
    "        df[\"CPI_YoY\"]     = df[\"CPI\"].pct_change(12, fill_method=None)\n",
    "        df[\"INDPRO_YoY\"]  = df[\"INDPRO\"].pct_change(12, fill_method=None)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Could not fetch FRED data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "exog_df = get_fred_data(returns_monthly.index.min(), returns_monthly.index.max())\n",
    "\n",
    "# ---  Align and Prepare Data for Modeling ---\n",
    "# Align all data to our monthly return frequency\n",
    "exog_monthly = exog_df.resample(\"ME\").last().ffill()\n",
    "common_index = returns_monthly.index.intersection(exog_monthly.index)\n",
    "returns_aligned = returns_monthly.loc[common_index]\n",
    "exog_aligned = exog_monthly.loc[common_index]\n",
    "\n",
    "# Use LAGGED economic data to predict the NEXT month's regime to prevent lookahead bias\n",
    "exog_lagged = exog_aligned.shift(1).dropna()\n",
    "final_index = returns_aligned.index.intersection(exog_lagged.index)\n",
    "returns_final = returns_aligned.loc[final_index]\n",
    "exog_final_lagged = exog_lagged.loc[final_index, ['3M', '10Y', 'FedFunds', 'VIX', 'T5YIE', 'UNRATE', 'UMCSENT', 'BAA10Y',\n",
    "       'USSLIND', 'CPI', 'INDPRO', 'Spread_10Y_3M', 'CreditSpread', 'CPI_YoY',\n",
    "       'INDPRO_YoY']]\n",
    "endog_voo = returns_final[\"VOO\"]  # Market returns (VOO) to identify regimes\n",
    "\n",
    "print(f\"Final dataset for regime modeling has {len(endog_voo)} monthly observations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298cb692",
   "metadata": {},
   "source": [
    "We now have all relevant data and can fit the Markov regime-switching model.  To do so, we must specify two key hyperparameters:\n",
    "\n",
    "1. The number of regimes $K$,\n",
    "2. The macroeconomic predictors* used as exogenous variables for the transition probabilities.\n",
    "\n",
    "For both hyperparameter tuning tasks, we aim to minimize BIC to identify the best model specification.  Additionally, we reject any models in which one or more regimes contain too few observations.\n",
    "\n",
    "To select the number of regimes, we perform a simple grid search starting with $K = 2 $ as the minimum.\n",
    "\n",
    "To choose which macroeconomic variables to include as predictors, we use a *sequential (greedy) approach*: starting with an empty predictor set, we iteratively add variables one at a time.  A variable is retained only if it improves model fit.  This approach is not guaranteed to find the globally optimal set of predictors, as it assumes that adding a useful variable will improve model fit regardless of the other variables already included.  However, it is a computationally efficient and interpretable alternative to evaluating all possible predictor combinations.  To improve effectiveness, I order candidate predictors based on economic intuition, prioritizing variables that are most likely to capture regime dynamics (e.g., VIX, yield spreads, Fed Funds rate).\n",
    "\n",
    "Once the best model is selected and fitted, we estimate the *regime-specific volatility and expected return* for VOO.  This allows us to evaluate whether the estimated regimes make economic sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499ec4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_exog_vars = [\n",
    "    \"VIX\",             # Market volatility\n",
    "    \"Spread_10Y_3M\",   # Yield curve\n",
    "    \"FedFunds\",        # Short rate policy stance\n",
    "    \"T5YIE\",           # Inflation expectations\n",
    "    \"BAA10Y\",          # Corporate bond spread\n",
    "    \"USSLIND\",         # Leading index\n",
    "    \"UNRATE\",          # Unemployment\n",
    "    \"CPI_YoY\",         # Actual inflation\n",
    "    \"INDPRO_YoY\",      # Output momentum\n",
    "    \"UMCSENT\"          # Consumer sentiment\n",
    "]\n",
    "\n",
    "models = {}\n",
    "for k in range(2, MAX_REGIMES_TO_TEST + 1):\n",
    "    print(f\"\\n=== Fitting model with {k} regimes ===\")\n",
    "\n",
    "    best_bic = np.inf\n",
    "    best_model = None\n",
    "    best_vars = []\n",
    "    remaining = [var for var in ordered_exog_vars if var in exog_final_lagged.columns]\n",
    "    selected = []\n",
    "\n",
    "    while remaining:\n",
    "        bic_candidates = []\n",
    "        for var in remaining:\n",
    "            current_vars = selected + [var]\n",
    "            exog_subset = sm.add_constant(exog_final_lagged[current_vars])\n",
    "\n",
    "            try:\n",
    "                mod = MarkovRegression(\n",
    "                    endog=endog_voo,\n",
    "                    k_regimes=k,\n",
    "                    trend=\"c\",\n",
    "                    switching_variance=True,\n",
    "                    exog_tvtp=exog_subset,\n",
    "                )\n",
    "                res = mod.fit(search_reps=30, maxiter=300, disp=False)\n",
    "\n",
    "                assigned_regimes = res.smoothed_marginal_probabilities.idxmax(axis=1)\n",
    "                if (assigned_regimes.value_counts() < MIN_OBS_PER_REGIME).any():\n",
    "                    continue\n",
    "\n",
    "                bic_candidates.append((var, res.bic, res))\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        if not bic_candidates:\n",
    "            break\n",
    "\n",
    "        best_var, var_bic, var_model = min(bic_candidates, key=lambda x: x[1])\n",
    "\n",
    "        if var_bic < best_bic:\n",
    "            selected.append(best_var)\n",
    "            remaining.remove(best_var)\n",
    "            best_bic = var_bic\n",
    "            best_model = var_model\n",
    "            best_vars = selected.copy()\n",
    "            print(f\"  Added {best_var}, BIC improved to {best_bic:.2f}\")\n",
    "        else:\n",
    "            print(f\"  No BIC improvement for {k} regimes with more variables.\")\n",
    "            break\n",
    "\n",
    "    if best_model:\n",
    "        models[k] = (best_model, best_vars)\n",
    "        print(f\"  > Final model for {k} regimes: {best_vars}, BIC = {best_bic:.2f}\")\n",
    "    else:\n",
    "        print(f\"  > No valid model found for {k} regimes.\")\n",
    "\n",
    "\n",
    "if not models:\n",
    "    raise RuntimeError(\"No suitable regime-switching models could be fitted.\")\n",
    "\n",
    "# Select the model with the lowest BIC\n",
    "best_k = min(models, key=lambda k: models[k][0].bic)\n",
    "best_model_results, best_exog_vars = models[best_k]\n",
    "\n",
    "print(f\"\\nBest model selected: {best_k} regimes with predictors {best_exog_vars}\")\n",
    "print(f\"Lowest BIC = {best_model_results.bic:.2f}\")\n",
    "\n",
    "# --- Interpret and Label the Regimes ---\n",
    "# To make regimes interpretable, we sort them by their volatility.\n",
    "regime_vols = best_model_results.params.filter(like=\"sigma2\").sort_values()\n",
    "regime_order = regime_vols.index.str.extract(r\"\\[(\\d+)\\]\")[0].astype(int)\n",
    "regime_map = {old_idx: new_idx for new_idx, old_idx in enumerate(regime_order)}\n",
    "\n",
    "# Display characteristics of each sorted regime\n",
    "sorted_params = pd.DataFrame()\n",
    "for i in range(best_k):\n",
    "    original_idx = regime_order.iloc[i]\n",
    "    # Annualize parameters for interpretability\n",
    "    mean_ann = best_model_results.params[f\"const[{original_idx}]\"] * 12 * 100\n",
    "    vol_ann = np.sqrt(best_model_results.params[f\"sigma2[{original_idx}]\"]) * np.sqrt(12) * 100\n",
    "    sorted_params[f\"Regime {i}\"] = [f\"{mean_ann:.1f}%\", f\"{vol_ann:.1f}%\"]\n",
    "sorted_params.index = [\"Annualized Mean (VOO)\", \"Annualized Volatility (VOO)\"]\n",
    "print(\"\\nCharacteristics of Identified Market Regimes (Sorted by Volatility):\")\n",
    "print(sorted_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321ef8ed",
   "metadata": {},
   "source": [
    "The best-performing specification is a Markov regime-switching model with 2 regimes and VIX as the sole exogenous predictor for the transition probabilities.\n",
    "\n",
    "We can also see that Regime 0 is the low risk - high expected return regime, while Regime 1 is the high risk - low expected return regime.  This structure makes economic sense: the model captures alternating states of the economy — periods of stability and growth with low volatility versus periods of market stress or crisis with high volatility.  A forward-looking measure of market uncertainty like VIX is a natural predictor to distinguish between these regimes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fb872f",
   "metadata": {},
   "source": [
    "To verify that the model aligns with our economic intuition, we can plot the marginal (smoothed) probabilities for the two regimes and compare them to the return variation of VOO over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2658eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the final, sorted series of regime probabilities\n",
    "smoothed_probs = best_model_results.smoothed_marginal_probabilities.rename(\n",
    "    columns=regime_map\n",
    ").sort_index(axis=1)\n",
    "regime_series = smoothed_probs.idxmax(axis=1).rename(\"regime\")\n",
    "\n",
    "\n",
    "# --- Visualize Regime Probabilities vs. Market Returns ---\n",
    "fig, axes = plt.subplots(\n",
    "    2, 1, figsize=(14, 8), sharex=True, gridspec_kw={\"height_ratios\": [3, 2]}\n",
    ")\n",
    "smoothed_probs.plot(ax=axes[0], kind=\"area\", stacked=True, colormap=\"viridis\", alpha=0.8)\n",
    "axes[0].set_title(\"Smoothed Probabilities of Each Market Regime Over Time\", fontsize=14)\n",
    "axes[0].set_ylabel(\"Probability\")\n",
    "axes[0].legend(title=\"Regime\", loc=\"upper left\")\n",
    "returns_final[\"VOO\"].plot(ax=axes[1], color=\"black\", label=\"VOO Monthly Return\", alpha=0.7)\n",
    "axes[1].set_title(\"VOO Monthly Returns\", fontsize=14)\n",
    "axes[1].set_ylabel(\"Return\")\n",
    "axes[1].axhline(0, color=\"grey\", lw=1, linestyle=\"--\")\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301ba3f6",
   "metadata": {},
   "source": [
    "We observe that during periods of high volatility—i.e., large return fluctuations in VOO, such as in 2020 and again in 2022—the model assigns high probability to Regime 1 (yellow). In contrast, during calmer periods with steady returns, such as in 2017, the model identifies Regime 0 (purple).\n",
    "\n",
    "It seems the Markov regime-switching model successfully separates the time series into economically meaningful regimes.  For each regime, we can now estimate an optimal portfolio.  Using the predicted regime probabilities for the next month, we can blend the regime-specific portfolios into a forward-looking investment strategy that dynamically adjusts to changing market conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846dd6fc",
   "metadata": {},
   "source": [
    "### 6. Regime‑Aware Dynamic Strategy\n",
    "\n",
    "From the previous section, we have a Markov regime-switching model that estimates the probability of being in a low- or high-risk state of the world at any given time. In t his section, we estimate the optimal portfolio for each regime and use these to construct a*one-month forward-looking trading strategy.\n",
    "\n",
    "To do so, we split the sample based on the regime that is more likely in each period, using the smoothed probabilities from the fitted model.  For each regime-specific subsample, we estimate an optimal portfolio that matches VOO’s global volatility *within that regime*.\n",
    "\n",
    "This is particularly useful because the Markov model provides predicted probabilities for each regime in the next period—based on current macroeconomic conditions captured by the VIX.  We can use these probabilities to blend the regime-specific portfolios into a forward-looking portfolio strategy that dynamically adjusts to the expected state of the market.\n",
    "\n",
    "Note that this constitutes an in-sample test, as the probabilities are inferred directly from the fitted model.  In section 8, we will evaluate how this strategy performs out-of-sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e50fb4",
   "metadata": {},
   "source": [
    "Now that we have a fitted Markov model with multiple states, we can estimate for *each* regime the efficient frontier and the optimal portfolio that matches VOO's volatility.  We can wrap this within-regime optimization in a function, so we can re-use the approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb3df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute a separate optimal portfolio for each identified regime.\n",
    "def compute_regime_optimal_weights(\n",
    "    returns,\n",
    "    regime_labels,\n",
    "    etf_symbols,\n",
    "    expense_vector,\n",
    "    target_volatility,\n",
    "    n_regimes,\n",
    "    frontier_points=50,\n",
    "    min_obs_per_regime=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each hidden-state regime, estimate expected returns / covariance,\n",
    "    build the efficient frontier, and select the portfolio whose annualised\n",
    "    volatility matches `target_volatility` (e.g. VOO).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Monthly returns of the ETFs (index = Date, columns = symbols).\n",
    "    regime_labels : Series\n",
    "        Integer regime label for each date (same index as `returns`).\n",
    "    etf_symbols : list[str]\n",
    "        Ordered list of ETF tickers (column order used downstream).\n",
    "    expense_vector : ndarray\n",
    "        Annualised expense ratio for each ETF, in the same order as `etf_symbols`.\n",
    "    target_volatility : float\n",
    "        Benchmark volatility to match when picking the optimal point\n",
    "        on each regime’s frontier (e.g. VOO’s σᵇ).\n",
    "    n_regimes : int\n",
    "        Total number of regimes in the hidden-state model.\n",
    "    frontier_points : int, default 50\n",
    "        Number of points generated on each frontier.\n",
    "    min_obs_per_regime : int or None\n",
    "        Minimum observations required to estimate a regime frontier.\n",
    "        Defaults to `max(24, len(etf_symbols))`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[int, ndarray]     regime → optimal weight vector\n",
    "    dict[int, DataFrame]   regime → frontier with columns ['volatility', 'expected_return']\n",
    "    \"\"\"\n",
    "    if min_obs_per_regime is None:\n",
    "        min_obs_per_regime = max(24, len(etf_symbols))\n",
    "\n",
    "    regime_optimal_weights = {}\n",
    "    regime_frontiers       = {}\n",
    "    exp_ret_by_regime      = {}\n",
    "    cov_by_regime          = {}\n",
    "\n",
    "\n",
    "    for r in range(n_regimes):\n",
    "        in_regime = regime_labels == r\n",
    "        if in_regime.sum() < min_obs_per_regime:\n",
    "            # Fallback: 100 % VOO if data too sparse\n",
    "            w_fallback = np.array([1.0 if s == \"VOO\" else 0.0 for s in etf_symbols])\n",
    "            regime_optimal_weights[r] = w_fallback\n",
    "            continue\n",
    "\n",
    "        # Use only returns from months assigned to regime r\n",
    "        returns_r = returns.loc[in_regime, etf_symbols]\n",
    "\n",
    "        # Annualised expected returns and covariance (Ledoit–Wolf shrinkage)\n",
    "        exp_ret_r = returns_r.mean().values * 12 - expense_vector\n",
    "        cov_r = LedoitWolf().fit(returns_r.values).covariance_ * 12\n",
    "\n",
    "        # keep for downstream plotting / diagnostics\n",
    "        exp_ret_by_regime[r] = exp_ret_r\n",
    "        cov_by_regime[r]     = cov_r\n",
    "\n",
    "        # Build the frontier and prune dominated points\n",
    "        frontier_r = efficient_frontier(cov_r, exp_ret_r, n_points=frontier_points)\n",
    "        frontier_r = prune_frontier(frontier_r)              \n",
    "\n",
    "        # Pick portfolio whose volatility is closest to target_volatility\n",
    "        _, w_opt = select_portfolio(frontier_r, \"sigma\", target_volatility)\n",
    "\n",
    "        if w_opt is not None:\n",
    "            print(f\"  > Top 3 ETFs for Regime {r} Portfolio (matching VOO vol):\")\n",
    "            top_indices = np.argsort(w_opt)[-3:][::-1]\n",
    "            for idx in top_indices:\n",
    "                if w_opt[idx] > 0.01:\n",
    "                    symbol = etf_symbols[idx]\n",
    "                    name = etf_name_map.get(symbol, \"Unknown\")\n",
    "                    print(f\"    {symbol} ({name}): {w_opt[idx]:.2%}\")\n",
    "\n",
    "            regime_optimal_weights[r] = w_opt\n",
    "            regime_frontiers[r] = frontier_r\n",
    "        else:\n",
    "            w_fallback = np.array([1.0 if s == \"VOO\" else 0.0 for s in etf_symbols])\n",
    "            regime_optimal_weights[r] = w_fallback\n",
    "\n",
    "    return (\n",
    "        regime_optimal_weights,\n",
    "        regime_frontiers,\n",
    "        exp_ret_by_regime,\n",
    "        cov_by_regime,\n",
    "    )\n",
    "\n",
    "# --- Call the weight-selection helper ---\n",
    "regime_opt_weights, regime_frontiers, exp_ret_by_regime, cov_by_regime = compute_regime_optimal_weights(\n",
    "    returns           = returns_final, \n",
    "    regime_labels     = regime_series,\n",
    "    etf_symbols       = etf_symbols,\n",
    "    expense_vector    = expense_vector,\n",
    "    target_volatility = voo_sigma_annual,  \n",
    "    n_regimes         = best_k,\n",
    "    frontier_points   = FRONTIER_POINTS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b9a43",
   "metadata": {},
   "source": [
    "We can also visualize the efficient frontiers for each regime, as well as the optimal portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e4eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regime_frontiers(\n",
    "    regime_frontiers,\n",
    "    regime_optimal_weights,\n",
    "    cov_by_regime,\n",
    "    exp_ret_by_regime,\n",
    "    etf_symbols,\n",
    "    benchmark_vol,\n",
    "    benchmark_ret,\n",
    "    title=\"Efficient Frontiers by Regime\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Draw each regime’s efficient frontier plus the selected optimal portfolio,\n",
    "    and overlay the benchmark (e.g. VOO).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    regime_frontiers : dict[int, DataFrame]\n",
    "        Output from `compute_regime_optimal_weights`.\n",
    "    regime_optimal_weights : dict[int, ndarray]\n",
    "        Output from `compute_regime_optimal_weights`.\n",
    "    cov_by_regime : dict[int, ndarray]\n",
    "        Covariance matrices used when selecting weights (optional: for stars).\n",
    "    exp_ret_by_regime : dict[int, ndarray]\n",
    "        Expected-return vectors used when selecting weights.\n",
    "    etf_symbols, benchmark_vol, benchmark_ret : see above\n",
    "    \"\"\"\n",
    "    num_regimes = len(regime_frontiers)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, num_regimes))\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for r, frontier in regime_frontiers.items():\n",
    "        plt.plot(\n",
    "            frontier[\"sigma\"],\n",
    "            frontier[\"mu\"],\n",
    "            label=f\"Regime {r} Frontier\",\n",
    "            lw=2,\n",
    "            color=colors[r],\n",
    "        )\n",
    "\n",
    "        # ──────────────────────────────────────────────────────────────────\n",
    "        # optional star marker (needs cov & mu); skip if not provided\n",
    "        # ──────────────────────────────────────────────────────────────────\n",
    "        if cov_by_regime and exp_ret_by_regime:\n",
    "            w_opt = regime_optimal_weights[r]\n",
    "            opt_vol = np.sqrt(w_opt.T @ cov_by_regime[r] @ w_opt)\n",
    "            opt_ret = w_opt.T @ exp_ret_by_regime[r]\n",
    "            plt.scatter(\n",
    "                opt_vol,\n",
    "                opt_ret,\n",
    "                marker=\"*\",\n",
    "                s=250,\n",
    "                color=colors[r],\n",
    "                edgecolors=\"black\",\n",
    "                zorder=5,\n",
    "            )\n",
    "\n",
    "    # benchmark marker\n",
    "    if benchmark_vol is not None and benchmark_ret is not None:\n",
    "        plt.scatter(\n",
    "            [benchmark_vol],\n",
    "            [benchmark_ret],\n",
    "            color=\"black\",\n",
    "            marker=\"X\",\n",
    "            s=200,\n",
    "            label=\"Benchmark (VOO)\",\n",
    "            zorder=6,\n",
    "        )\n",
    "\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(\"Annualised Volatility\", fontsize=12)\n",
    "    plt.ylabel(\"Annualised Return\", fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Plot the efficient frontiers for each regime ---\n",
    "plot_regime_frontiers(\n",
    "    regime_frontiers       = regime_frontiers,     # from compute_regime_optimal_weights\n",
    "    regime_optimal_weights = regime_opt_weights,   # from compute_regime_optimal_weights\n",
    "    cov_by_regime          = cov_by_regime,                   # omit → no star markers\n",
    "    exp_ret_by_regime      = exp_ret_by_regime,                   # omit → no star markers\n",
    "    etf_symbols            = etf_symbols,\n",
    "    benchmark_vol          = voo_sigma_annual,\n",
    "    benchmark_ret          = voo_mu_annual,\n",
    "    title                  = \"Efficient Frontiers by Regime (Training Sample)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40531a7",
   "metadata": {},
   "source": [
    "The plot shows that the low-volatility regime, Regime 0, has a steep efficient frontier positioned *northwest* of VOO’s global risk-return profile.  We know this is the low-volatility, high-expected-return regime, so it makes sense that this \"good\" state of the world would feature a steep frontier, where small increases in risk yield disproportionately high returns.  Because conditional risk is limited in this regime, we can afford to overweight historically risky ETFs to maximize expected returns.  This leads the optimizer to fully invest in the Mega Cap Growth ETF (MGK).  Since we are restricted to long-only portfolios, we cannot construct combinations that reach higher volatility levels, even though the portfolio remains below VOO’s global volatility.\n",
    "\n",
    "In contrast, Regime 1—the \"crisis\" regime—lies entirely below VOO's global expected return and exhibits a relatively flat efficient frontier, with a wide range of volatility but limited expected return.  In this high-risk state of the world, the optimal portfolio assigns more weight to the Short-Term Inflation-Protected Securities ETF (VTIP).  Since we still target VOO’s volatility level, the optimizer blends VTIP with the Dividend Appreciation ETF (VIG), and additionally includes the S\\&P Mid-Cap 400 Value ETF (IVOV).\n",
    "\n",
    "The inclusion of IVOV and VIG is new, but it makes intuitive sense.  During periods of crisis, investors tend to seek assets with strong intrinsic value based on fundamentals, as well as diversification relative to large-cap growth.  The Mid-Cap Value ETF offers both characteristics.\n",
    "\n",
    "The allocation to VIG is also reasonable. During the rising interest rate environment in the later part of the sample period, strong dividend growth can help offset some of the negative impact on stock prices. The Gordon Growth Model provides a simple relationship between a stock’s price, expected dividends, discount rate (or required return), and dividend growth:\n",
    "\n",
    "$$\n",
    "P = \\frac{D_1}{r - g},\n",
    "$$\n",
    "\n",
    "and its total derivative is:\n",
    "\n",
    "$$\n",
    "dP = -\\frac{P}{r - g} \\, dr + \\frac{P}{r - g} \\, dg.\n",
    "$$\n",
    "\n",
    "This implies that when the discount rate $r$ increases—as it does in the later part of our sample—an increase in expected dividend growth $g$ can offset the downward pressure on prices.  Since VIG, by design, tracks firms with a consistent history of dividend increases, it is plausible that its underlying dividend growth rate rises during the test period, helping to cushion the impact of higher interest rates on its price.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cca905",
   "metadata": {},
   "source": [
    "For back-testing, we use the marginal regime probabilities to blend the optimal portfolios for each regime.  This allows us to estimate the monthly in-sample returns of the strategy, which we will use for performance comparison in the sections below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973fecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At each month, the portfolio is a blend of the regime-optimal portfolios,\n",
    "# weighted by the smoothed probability of being in each regime at that time.\n",
    "dynamic_weights_list = []\n",
    "for t in range(len(returns_final)):\n",
    "    probs_t = smoothed_probs.iloc[t]\n",
    "    blended_w = np.zeros(n_assets)\n",
    "    # Create blended portfolio by weighting each regime's portfolio by its probability\n",
    "    for i in range(best_k):\n",
    "        if i in regime_opt_weights:\n",
    "            blended_w += probs_t[i] * regime_opt_weights[i]\n",
    "    dynamic_weights_list.append(blended_w / blended_w.sum())\n",
    "\n",
    "# Calculate the monthly returns of this dynamic portfolio\n",
    "dynamic_port_returns = np.sum(\n",
    "    np.array(dynamic_weights_list) * returns_final[etf_symbols].values, axis=1\n",
    ")\n",
    "dynamic_returns_series = pd.Series(dynamic_port_returns, index=returns_final.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe1d4e",
   "metadata": {},
   "source": [
    "### 7. Final Performance Comparison\n",
    "\n",
    "In this section, we compare the performance of all portfolio strategies we considered so far.  We define key performance metrics, plot the in-sample cumulative returns of each strategy, and run Monte Carlo return simulations for models that do not rely on dynamic weight adjustments.\n",
    "\n",
    "There are several key takeaways:\n",
    "\n",
    "- VOO is a strong benchmark.  Only about half of the models achieve a higher Sharpe ratio, i.e., expected return per unit of risk, and even then, the improvement is not much more than 20–30 percent.\n",
    "- The static mean-variance optimization and the Markov regime-switching model perform best in the in-sample test.\n",
    "- Risk Parity and Hierarchical Risk Parity are particularly effective at identifying low-volatility portfolios but tend to produce lower expected returns.\n",
    "- In our setting, the Black-Litterman model also results in a portfolio allocation that minimizes volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a08540",
   "metadata": {},
   "source": [
    "We first calculate in-sample performance measures for the different strategies—specifically total return, annual returns, volatility, and Sharpe ratio.  Since the time series of returns becomes more complete over time, I limit the evaluation to the last seven years and drop the first three years to make sure that all models are fitted on complete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1090c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define a Performance Metrics Calculator ---\n",
    "def calculate_performance_metrics(returns_series):\n",
    "    \"\"\"\n",
    "    Calculates key performance metrics for a series of returns.\n",
    "\n",
    "    Args:\n",
    "        returns_series (pd.Series): A series of periodic (e.g., monthly) returns.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of performance metrics.\n",
    "    \"\"\"\n",
    "    if returns_series.empty or returns_series.isnull().all():\n",
    "        return {\n",
    "            \"Total Return (%)\": np.nan, \"Annualized Return (%)\": np.nan,\n",
    "            \"Annualized Volatility (%)\": np.nan, \"Sharpe Ratio\": np.nan,\n",
    "        }\n",
    "    n_periods_per_year = 12  # For monthly returns\n",
    "    ann_return = ((1 + returns_series.mean()) ** n_periods_per_year - 1) * 100\n",
    "    ann_vol = returns_series.std() * np.sqrt(n_periods_per_year) * 100\n",
    "    sharpe = sharpe_ratio(ann_return / 100, ann_vol / 100)\n",
    "    \n",
    "    cumulative_returns = (1 + returns_series).cumprod()\n",
    "    total_return = (cumulative_returns.iloc[-1] - 1) * 100\n",
    "    \n",
    "    return {\n",
    "        \"Total Return (%)\": total_return,\n",
    "        \"Annualized Return (%)\": ann_return,\n",
    "        \"Annualized Volatility (%)\": ann_vol,\n",
    "        \"Sharpe Ratio\": sharpe,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Prepare All Strategy Returns for Comparison ---\n",
    "# Use a common lookback window for a fair comparison\n",
    "lookback_years = 7\n",
    "end_date_lookback = max(returns_final.index)\n",
    "start_date_lookback = end_date_lookback - pd.DateOffset(years=lookback_years)\n",
    "returns_win = returns_final.loc[start_date_lookback:end_date_lookback].dropna(how=\"all\")\n",
    "\n",
    "# Helper function to calculate strategy returns safely\n",
    "def get_strategy_returns(weights):\n",
    "    if weights is not None:\n",
    "        return returns_win[etf_symbols] @ weights\n",
    "    return pd.Series(dtype=float, index=returns_win.index)\n",
    "\n",
    "strategies = {\n",
    "    \"VOO Benchmark\": returns_win[\"VOO\"],\n",
    "    \"Static Raw (Risk-Match)\": get_strategy_returns(w_sigma_raw),\n",
    "    \"Static L1 Regularized (Risk-Match)\": get_strategy_returns(w_sigma_reg_l1),\n",
    "    \"Static Shrunk (Risk-Match)\": get_strategy_returns(w_sigma_shrunk),\n",
    "    \"Static Resampled\": get_strategy_returns(w_resampled),\n",
    "    \"Black-Litterman\": get_strategy_returns(w_bl_opt),\n",
    "    \"Risk Parity\": get_strategy_returns(rp_weights),\n",
    "    \"Hierarchical Risk Parity\": get_strategy_returns(hrp_weights),\n",
    "    \"HRP-MVO Blended\": get_strategy_returns(w_tilt),\n",
    "    \"DCC-GARCH Dynamic\": dynamic_returns_series_dcc.loc[start_date_lookback:end_date_lookback],\n",
    "    \"Regime-Aware Dynamic\": dynamic_returns_series.loc[start_date_lookback:end_date_lookback],\n",
    "}\n",
    "\n",
    "# --- Display Performance Metrics Table ---\n",
    "all_perf_metrics = {name: calculate_performance_metrics(ret.dropna()) for name, ret in strategies.items()}\n",
    "all_perf_df = pd.DataFrame(all_perf_metrics).T\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"      COMPREHENSIVE STRATEGY PERFORMANCE METRICS\")\n",
    "print(\"=\" * 70)\n",
    "print(all_perf_df.sort_values(by=\"Sharpe Ratio\", ascending=False))\n",
    "print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de41b0b5",
   "metadata": {},
   "source": [
    "We can also plot the cumulative returns for the strategies over the test period and compare to VOO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d685a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "for name, rtn in strategies.items():\n",
    "    if rtn.dropna().empty: continue\n",
    "    growth = (1 + rtn).cumprod()\n",
    "    if name.lower().startswith(\"voo\"):\n",
    "        growth.plot(ax=ax, label=name, lw=3, linestyle=\"--\", color=\"black\")\n",
    "    else:\n",
    "        growth.plot(ax=ax, label=name, lw=2)\n",
    "ax.set_title(f\"Cumulative Performance – Last {lookback_years} Years\", fontsize=16)\n",
    "ax.set_xlabel(\"Date\", fontsize=12)\n",
    "ax.set_ylabel(\"Growth of $1 (log scale)\", fontsize=12)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend(loc=\"upper left\", fontsize=10)\n",
    "ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47277e95",
   "metadata": {},
   "source": [
    "Looking at the cumulative return chart and the performance metrics, we see that VOO performs quite well—but there are models that outperform it.  The static mean-variance optimization methods, as well as the Markov regime-switching model, perform the best.  The HRP blend with the mean-variance optimal portfolio also performs well, but if we are already identifying the mean-variance efficient portfolio, it's unclear why we would blend it again with an HRP portfolio.\n",
    "\n",
    "On the other hand, the DCC-GARCH model performs slightly worse than VOO, making it less attractive—especially given the high frequency with which the weights oscillate, resulting in frequent rebalancing of the portfolio.  We could potentially improve performance by using weekly or even daily portfolio updates, avoiding the approximation of using the predicted optimal portfolio from the last week of the previous month for the entire following month.  However, this would increase portfolio turnover, and the average investor certainly doesn’t have time for that (I certainly don’t).\n",
    "\n",
    "Black-Litterman, Risk Parity, and HRP show the weakest performance.  We already know that the mean shrinkage in Black-Litterman causes the algorithm to effectively collapse into a volatility-minimization strategy—similar to the objective followed by Risk Parity and HRP.  These approaches are useful if we are seeking low volatility, but if the goal is to outperform the market, we need to rely on different models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c8592",
   "metadata": {},
   "source": [
    "We can also compare performance based on simulated returns.  I do this only for the models that provide stable portfolio weights—not for DCC-GARCH and the Markov regime-switching model.  For those dynamic models, we would need to build a more complex simulation environment.  For example, for the Markov regime-switching model, we would essentially need to define a full Markov model along with the exogenous predictors, simulate data based on that model, and then apply our approach to check its performance—quite complex, especially if we can instead perform out-of-sample testing in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6928d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate future returns to see how our portfolios might perform under a wide\n",
    "# range of possible outcomes, based on the historical return distribution.\n",
    "print(f\"Running Monte Carlo simulation with {MC_SIM_SCENARIOS} scenarios...\")\n",
    "# Use monthly parameters from the full sample for the simulation\n",
    "monthly_mu_sample = annual_mu_sample / 12\n",
    "rng = np.random.default_rng(seed=42)  # For reproducibility\n",
    "\n",
    "# Generate all simulated paths at once for efficiency\n",
    "simulated_returns_monthly = rng.multivariate_normal(\n",
    "    mean=monthly_mu_sample,\n",
    "    cov=sample_cov,  # Use the original sample covariance\n",
    "    size=(MC_SIM_SCENARIOS, MC_SIM_HORIZON_MONTHS),\n",
    ")\n",
    "\n",
    "\n",
    "def simulate_portfolio_performance(weights):\n",
    "    \"\"\"Calculates performance metrics from simulated return paths.\"\"\"\n",
    "    if weights is None or np.isnan(weights).any():\n",
    "        return {\n",
    "            \"Mean Ann. Return (%)\": np.nan, \"Ann. Volatility (%)\": np.nan,\n",
    "            \"Sharpe Ratio\": np.nan, \"VaR 5% (Ann.) (%)\": np.nan,\n",
    "        }\n",
    "    portfolio_sim_returns = simulated_returns_monthly @ weights\n",
    "    # Calculate annualized metrics from the simulation results\n",
    "    mean_monthly_return = np.mean(portfolio_sim_returns)\n",
    "    std_monthly_return = np.std(portfolio_sim_returns)\n",
    "    \n",
    "    annual_mean_return = mean_monthly_return * 12 * 100\n",
    "    annual_volatility = std_monthly_return * np.sqrt(12) * 100\n",
    "    sharpe = sharpe_ratio(annual_mean_return/100, annual_volatility/100)\n",
    "    \n",
    "    # Value-at-Risk (VaR): The worst expected annualized loss at a 5% confidence level.\n",
    "    var_5_percent = np.percentile(portfolio_sim_returns, 5) * 12 * 100\n",
    "    \n",
    "    return {\n",
    "        \"Mean Ann. Return (%)\": annual_mean_return,\n",
    "        \"Ann. Volatility (%)\": annual_volatility,\n",
    "        \"Sharpe Ratio\": sharpe,\n",
    "        \"VaR 5% (Ann.) (%)\": var_5_percent,\n",
    "    }\n",
    "\n",
    "# Define weights for all strategies to be simulated\n",
    "voo_weights = np.array([1.0 if s == \"VOO\" else 0.0 for s in etf_symbols])\n",
    "simulation_portfolios = {\n",
    "    \"VOO Benchmark\": voo_weights,\n",
    "    \"Static Raw (Risk-Match)\": w_sigma_raw,\n",
    "    \"Static L1 Regularized (Risk-Match)\": w_sigma_reg_l1,\n",
    "    \"Static Shrunk (Risk-Match)\": w_sigma_shrunk,\n",
    "    \"Static Resampled\": w_resampled,\n",
    "    \"Black-Litterman\": w_bl_opt,\n",
    "    \"Risk Parity\": rp_weights,\n",
    "    \"Hierarchical Risk Parity\": hrp_weights,\n",
    "    \"HRP-MVO Blended\": w_tilt,\n",
    "}\n",
    "\n",
    "# Run simulation for each portfolio\n",
    "sim_results = {name: simulate_portfolio_performance(w) for name, w in simulation_portfolios.items()}\n",
    "sim_results_df = pd.DataFrame(sim_results).T\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"      MONTE CARLO SIMULATION SUMMARY ({MC_SIM_HORIZON_MONTHS // 12}-YEAR HORIZON)\")\n",
    "print(\"=\" * 70)\n",
    "print(sim_results_df.sort_values(by=\"Sharpe Ratio\", ascending=False))\n",
    "print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d348108d",
   "metadata": {},
   "source": [
    "The Monte Carlo simulations confirm what we observed in the in-sample backtesting: the static mean-variance optimization approach outperforms the market portfolio.  To test the robustness of this result, we perform out-of-sample backtesting in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f5078",
   "metadata": {},
   "source": [
    "### Section 8. Back-Testing Framework: Last 3-Year Out-of-Sample\n",
    "\n",
    "This section evaluates how well our static mean-variance optimization and the Markov regime-switching model perform on unseen data.  These two models showed the best performance in the in-sample period. Specifically, we fit model parameters using the first seven years of data, then let the models run over the following three years.\n",
    "\n",
    "For the mean-variance optimization, no further action is required after identifying the optimal portfolio—we simply observe how it performs during the test period.  For the Markov model, by contrast, we adjust portfolio weights dynamically based on the predicted regime for each upcoming period during the test window.\n",
    "\n",
    "Finally, we benchmark both strategies against VOO to determine which model truly performs better out-of-sample.  Focusing on the 2022–2025 period is particularly interesting for the Markov regime-switching model, as this was a time of sharply rising interest rates.  If the Markov model, trained on data from a relatively low interest rate environment, could still outperform during a high-rate regime, we would have a strategy that is robust across changing market conditions. Unfortunately, we will see that this is not the case.\n",
    "\n",
    "In the end, we conclude that the simple mean-variance optimization approach outperforms both in-sample and out-of-sample.  Quite comforting—simple financial theory still has real-world value!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93f522f",
   "metadata": {},
   "source": [
    "Before splitting our sample into training and test periods, it makes sense to consider how the market environment has evolved over time. Specifically, we can visualize the interest rate environment using the already collected data for the Fed Funds Rate, the 10-Year Treasury yield, and the 3-Month T-Bill rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2cba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_aligned[['FedFunds', '10Y', '3M']].plot(figsize=(12, 6), linewidth=2)\n",
    "\n",
    "plt.title('Interest Rate Environment Over Time', fontsize=16)\n",
    "plt.ylabel('Rate (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.legend([\"Fed Funds Rate\", \"10-Year Treasury\", \"3-Month T-Bill\"])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de787f17",
   "metadata": {},
   "source": [
    "We can clearly see that the economic environment has changed dramatically over our sample period. Starting from a relatively low interest rate environment with modest increases, rates dropped sharply during the pandemic before rising steeply in 2022. We should therefore be mindful that the training period—covering the first 7 years—captures a low-interest rate regime, while the test period falls within a high-interest rate regime. If our models perform well despite this stark shift, we can be more confident in the robustness of our approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f449c9a8",
   "metadata": {},
   "source": [
    "We can now split our sample into training period and test period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5302a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_windows(returns, fit_years=7, test_years=3):\n",
    "    \"\"\"Return training and testing DataFrames given total monthly returns.\"\"\"\n",
    "    end_test   = returns.index.max()\n",
    "    start_test = end_test - relativedelta(years=test_years) + relativedelta(months=1)\n",
    "    start_fit  = start_test - relativedelta(years=fit_years)\n",
    "    train = returns.loc[start_fit : start_test - relativedelta(months=1)]\n",
    "    test  = returns.loc[start_test : end_test]\n",
    "    return train, test\n",
    "\n",
    "#  split sample  (7-year fit  |  3-year test)\n",
    "train_returns, test_returns = split_windows(returns_final, fit_years=7, test_years=3)\n",
    "train_exog  = exog_final_lagged[best_exog_vars].loc[train_returns.index] # Select the best exogenous predictor\n",
    "test_exog   = exog_final_lagged[best_exog_vars].loc[test_returns.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287a8e75",
   "metadata": {},
   "source": [
    "Next, we test how the static mean-variance optimization performs out-of-sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edc41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annualiszd mean and covariance from the training sample\n",
    "exp_ret_train = train_returns.mean().values * 12 - expense_vector\n",
    "cov_train     = train_returns.cov().values  * 12\n",
    "\n",
    "# Build the efficient frontier and pick the portfolio\n",
    "# whose volatility matches the VOO benchmark\n",
    "ef_train = prune_frontier(\n",
    "    efficient_frontier(cov_train, exp_ret_train, FRONTIER_POINTS)\n",
    ")\n",
    "_, w_raw = select_portfolio(ef_train, \"sigma\", voo_sigma_annual)\n",
    "\n",
    "# fallback if optimization failed\n",
    "if w_raw is None:\n",
    "    w_raw = np.array([1.0 if s == \"VOO\" else 0.0 for s in etf_symbols])\n",
    "\n",
    "# Apply the fixed weights over the entire test period\n",
    "oos_ret_raw = (test_returns[etf_symbols] @ w_raw).rename(\"RAW OOS\")\n",
    "\n",
    "# Report Compound Annual Growth Rate (\"CAGR\"), a an intital check.\n",
    "cagr = (1 + oos_ret_raw).prod() ** (12 / len(oos_ret_raw)) - 1\n",
    "print(f\"\\nRaw-frontier out-of-sample CAGR: {cagr:.2%}\")\n",
    "\n",
    "# Inspect top holdings\n",
    "top_idx = np.argsort(w_raw)[-3:][::-1]\n",
    "print(\"\\nTop 3 ETF weights (fixed allocation):\")\n",
    "for idx in top_idx:\n",
    "    if w_raw[idx] > 0.01:\n",
    "        sym = etf_symbols[idx]\n",
    "        name = etf_name_map.get(sym, \"Unknown\")\n",
    "        print(f\"  {sym} ({name}): {w_raw[idx]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a65c9c",
   "metadata": {},
   "source": [
    "The optimal portfolio allocation is quite similar to the full in-sample fit, except for the addition of the Dividend Appreciation ETF (VIG), and a shift toward a different type of securities ETF: the Short-Term Inflation-Protected Securities ETF (VTIP).\n",
    "\n",
    "VIG delivers performance reasonably close to VOO.  Given the relatively low interest rates during the training period, the Gordon Growth Model suggests that a strong dividend growth rate $g$ can support performance.  As a refresher, the Gordon Growth Model provides a simple relationship between stock price, expected dividends, discount rate (or market risk), and dividend growth:\n",
    "\n",
    "$$\n",
    "P = \\frac{D_1}{r - g},\n",
    "$$\n",
    "\n",
    "VIG, by design, tracks firms with a consistent history of dividend increases.  Thus, a strong dividend growth rate $g$ can help support prices even when the discount rate $r$ remains low or is only gradually rising, as was the case during the training period.  As discussed above, when the discount rate $r$ rises more sharply, a contemporaneous increase in $g$ can offset some of the negative impact on stock prices—an effect particularly relevant during the test period.\n",
    "\n",
    "Overall, though, the static mean-variance optimization remains quite consistent in recommending a combination of a high-growth equity ETF and a Treasury ETF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84fb0fb",
   "metadata": {},
   "source": [
    "Next, we fit the Markov regime-switching model on the training period and evaluate its performance during the test period.\n",
    "\n",
    "The only noteworthy detail here is how we predict transition probabilities out-of-sample for the Markov model.  Once we have a fitted model from the training data, we create a second “test” model that includes the exogenous regressor used to predict the next state—VIX in our case.  We then apply the Hamilton filter using the estimated parameters from the training model to this test model.\n",
    "\n",
    "The Hamilton filter recursively calculates the probabilities of being in each latent state at time $t$, using all available data up to time $t - 1$.  In essence, it performs Bayesian inference over the latent regime path based on the observed data and the transition structure.  A more detailed explanation can be found in this [R blog post](https://www.r-bloggers.com/2022/02/understanding-hamilton-regime-switching-model-using-r-package/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d52c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- fit Markov-Regression on the training window -----------------------\n",
    "k_regimes = 2                    # or `best_k` if cross-validated earlier\n",
    "mr_train = MarkovRegression(\n",
    "    endog = train_returns[\"VOO\"],\n",
    "    k_regimes = k_regimes,\n",
    "    trend = \"c\",\n",
    "    switching_variance = True,\n",
    "    exog_tvtp = sm.add_constant(train_exog)\n",
    ")\n",
    "\n",
    "res_train = mr_train.fit(search_reps=30, maxiter=300, disp=False)\n",
    "\n",
    "# --- filter / forecast regimes on the test window --------------------------\n",
    "mr_test = MarkovRegression(\n",
    "    endog              = test_returns[\"VOO\"],\n",
    "    k_regimes          = k_regimes,\n",
    "    trend              = \"c\",\n",
    "    switching_variance = True,\n",
    "    exog_tvtp          = sm.add_constant(test_exog)\n",
    ")\n",
    "\n",
    "# Apply Hamilton Filter with parameters from model fitted on training data\n",
    "# to predict one-step-head transition probabilities.  Importantly, we\n",
    "# fix the parameters that are used for the probability model to the \n",
    "# results from the model fitted on the training data.\n",
    "res_test = mr_test.filter(res_train.params) \n",
    "\n",
    "# One-step-ahead probabilities:  P(state_t | information_set_{t-1})\n",
    "pred_probs = res_test.predicted_marginal_probabilities.copy()\n",
    "pred_probs.index = test_returns.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cb6ff8",
   "metadata": {},
   "source": [
    "We now have the out-of-sample predicted marginal probabilities for each regime. We can visualize how the in-sample regime probabilities compare to VOO’s performance and to the predictor, VIX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_probs = res_train.smoothed_marginal_probabilities     # DataFrame k_regimes cols\n",
    "train_idx   = train_probs.index                             # convenience\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    3, 1, figsize=(14, 12), sharex=True,\n",
    "    gridspec_kw={\"height_ratios\": [3, 2, 2]}\n",
    ")\n",
    "\n",
    "# --- Regime probabilities ---------------------------------------------------\n",
    "train_probs.plot(\n",
    "    ax=axes[0],\n",
    "    kind=\"area\",\n",
    "    stacked=True,\n",
    "    colormap=\"viridis\",\n",
    "    alpha=0.8,\n",
    "    linewidth=0\n",
    ")\n",
    "axes[0].set_title(\"Smoothed Regime Probabilities (In-Sample)\", fontsize=14)\n",
    "axes[0].set_ylabel(\"Probability\")\n",
    "axes[0].legend(title=\"Regime\", loc=\"upper left\")\n",
    "\n",
    "# --- VOO returns ------------------------------------------------------------\n",
    "returns_final.loc[train_idx, \"VOO\"].plot(\n",
    "    ax=axes[1],\n",
    "    color=\"black\",\n",
    "    label=\"VOO Monthly Return\",\n",
    "    alpha=0.7\n",
    ")\n",
    "axes[1].set_title(\"VOO Monthly Returns (In-Sample)\", fontsize=14)\n",
    "axes[1].set_ylabel(\"Return\")\n",
    "axes[1].axhline(0, color=\"grey\", lw=1, linestyle=\"--\")\n",
    "\n",
    "# --- Exogenous predictor: VIX ----------------------------------------------\n",
    "train_exog[\"VIX\"].plot(\n",
    "    ax=axes[2],\n",
    "    lw=1.5,\n",
    "    label=\"VIX\"\n",
    ")\n",
    "axes[2].set_title(\"VIX (In-Sample)\", fontsize=14)\n",
    "axes[2].set_ylabel(\"Level\")\n",
    "axes[2].legend(loc=\"upper left\")\n",
    "\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1e7fa",
   "metadata": {},
   "source": [
    "It is unfortunate that the regime probabilities are essentially inverted compared to those from the Markov regime-switching model fitted on the entire dataset.  This highlights how sensitive the model is to the inclusion of additional data from the test period.  However, the good news is that VIX still appears to perform well as a predictor—higher VIX levels increase the likelihood of being in Regime 1, the \"crisis\" state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8d6cd8",
   "metadata": {},
   "source": [
    "We can estimate the optimal portfolio for each regime in the training period and evaluate the out-of-sample performance of the model by blending the optimal portfolios using the out-of-sample regime probabilities at each point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efe92b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training-sample regime probabilities into hard labels\n",
    "regime_labels_train = (\n",
    "    res_train.smoothed_marginal_probabilities          # DataFrame: cols 0..k-1\n",
    "             .idxmax(axis=1)                           # Series of ints\n",
    ")\n",
    "\n",
    "# Compute optimal portfolio for each regime\n",
    "regime_opt_weights, regime_frontiers, exp_ret_by_regime, cov_by_regime = (\n",
    "    compute_regime_optimal_weights(\n",
    "        returns           = returns_final.loc[train_returns.index],\n",
    "        regime_labels     = regime_labels_train,\n",
    "        etf_symbols       = etf_symbols,\n",
    "        expense_vector    = expense_vector,\n",
    "        target_volatility = voo_sigma_annual,          # match VOO's volatility\n",
    "        n_regimes         = k_regimes,\n",
    "        frontier_points   = FRONTIER_POINTS,\n",
    "        min_obs_per_regime=24 \n",
    "    )\n",
    ")\n",
    "\n",
    "# (optional) quick sanity-check plot\n",
    "plot_regime_frontiers(regime_frontiers, regime_opt_weights,\n",
    "                       cov_by_regime, exp_ret_by_regime,\n",
    "                       etf_symbols, voo_sigma_annual, voo_mu_annual)\n",
    "\n",
    "# --- build the dynamic out-of-sample portfolio ------------------------------\n",
    "oos_weights = (\n",
    "    pred_probs\n",
    "    .apply(\n",
    "        # blend the regime-specific weights by one-step-ahead probabilities\n",
    "        lambda p: sum(p[j] * regime_opt_weights[j] for j in range(k_regimes)),\n",
    "        axis=1, result_type=\"expand\"\n",
    "    )\n",
    "    .pipe(lambda w: w.div(w.sum(axis=1), axis=0))      # normalise so weight sum to one\n",
    ")\n",
    "oos_weights.columns = etf_symbols\n",
    "\n",
    "# realised return in month t uses the weights decided *before* that month\n",
    "oos_port_ret = (\n",
    "    (oos_weights.shift(1) * test_returns[etf_symbols])\n",
    "    .sum(axis=1)\n",
    "    .dropna()\n",
    ")\n",
    "oos_port_ret.name = \"Regime-Switching OOS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3396e32a",
   "metadata": {},
   "source": [
    "The optimal portfolios are quite interesting, but also familiar from the in-sample fitting.  Regime 0 invests mostly in the S&P Small-Cap 600 Value ETF (VIOV) and the Short-Term Inflation-Protected Securities ETF (VTIP).  In Regime 1, on the other hand, the portfolio is again fully invested in the Mega Cap Growth ETF (MGK).  However, the regimes are completely swapped—Regime 0 is now the \"good\" state of the world, yet we invest in a defensive portfolio, while in the \"crisis\" state we invest in the aggressive Growth ETF.  This inversion is a direct consequence of the marginal regime probabilities being essentially flipped compared to the model fitted on the full dataset.\n",
    "\n",
    "Now we can plot the regime characteristics from the training data to check whether the regimes make economic sense. As a final check, we can estimate the compound annual growth rate, which provides a direct measure of average performance during the out-of-sample period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b4456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull per-regime monthly parameters from the Markov-regression result\n",
    "stats = []\n",
    "for j in range(k_regimes):\n",
    "    mean_ann = res_train.params[f\"const[{j}]\"] * 12 * 100            # %\n",
    "    vol_ann  = (res_train.params[f\"sigma2[{j}]\"] ** 0.5) * (12 ** 0.5) * 100  # %\n",
    "    stats.append({\"regime\": j, \"mean\": mean_ann, \"vol\": vol_ann})\n",
    "\n",
    "stats_df = pd.DataFrame(stats).set_index(\"regime\")\n",
    "\n",
    "# Order regimes by volatility\n",
    "order = stats_df.sort_values(\"vol\").index        # e.g. [1, 0] for two regimes\n",
    "\n",
    "# Print mean return and volatility\n",
    "table = pd.DataFrame()\n",
    "for i, r in enumerate(order):\n",
    "    table[f\"Regime {i}\"] = [f\"{stats_df.loc[r, 'mean']:.1f}%\",\n",
    "                            f\"{stats_df.loc[r, 'vol']:.1f}%\"]\n",
    "\n",
    "table.index = [\"Annualised Mean (VOO)\", \"Annualised Volatility (VOO)\"]\n",
    "\n",
    "print(\"\\nCharacteristics of Identified Market Regimes (sorted by volatility):\")\n",
    "print(table)\n",
    "\n",
    "# CAGR for sanity check\n",
    "cagr = (1 + oos_port_ret).prod() ** (12 / len(oos_port_ret)) - 1\n",
    "print(f\"\\nOut-of-sample CAGR: {cagr:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1a563e",
   "metadata": {},
   "source": [
    "The performance is quite sobering, and even VOO’s behavior appears unusual.  Regime 0, which invests in defensive Treasury and value stocks, is associated with low market volatility, while Regime 1—characterized by high market volatility—allocates fully to Mega Cap Growth.  Based on our earlier findings, we would have expected the allocations to be exactly the other way around.  The high-interest-rate environment during the test period seems to have fundamentally shifted the structure of the regimes.\n",
    "\n",
    "We can also visualize the predicted out-of-sample regime probabilities and plot them against the VIX.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea837aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_idx = pred_probs.index                                        # convenience\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    3, 1, figsize=(14, 12), sharex=True,          \n",
    "    gridspec_kw={\"height_ratios\": [3, 2, 2]}\n",
    ")\n",
    "\n",
    "# --- Regime probabilities ---\n",
    "pred_probs.plot(\n",
    "    ax=axes[0],\n",
    "    kind=\"area\",\n",
    "    stacked=True,\n",
    "    colormap=\"viridis\",\n",
    "    alpha=0.8,\n",
    "    linewidth=0\n",
    ")\n",
    "axes[0].set_title(\"Predicted Regime Probabilities (Out-of-Sample)\", fontsize=14)\n",
    "axes[0].set_ylabel(\"Probability\")\n",
    "axes[0].legend(title=\"Regime\", loc=\"upper left\")\n",
    "\n",
    "# --- VOO returns ---\n",
    "returns_final.loc[oos_idx, \"VOO\"].plot(\n",
    "    ax=axes[1],\n",
    "    color=\"black\",\n",
    "    label=\"VOO Monthly Return\",\n",
    "    alpha=0.7\n",
    ")\n",
    "axes[1].set_title(\"VOO Monthly Returns (Out-of-Sample)\", fontsize=14)\n",
    "axes[1].set_ylabel(\"Return\")\n",
    "axes[1].axhline(0, color=\"grey\", lw=1, linestyle=\"--\")\n",
    "\n",
    "# --- Exogenous predictor: VIX ---\n",
    "test_exog.loc[oos_idx, \"VIX\"].plot(\n",
    "    ax=axes[2],\n",
    "    lw=1.5,\n",
    "    label=\"VIX\"\n",
    ")\n",
    "axes[2].set_title(\"VIX (Out-of-Sample)\", fontsize=14)\n",
    "axes[2].set_ylabel(\"Level\")\n",
    "axes[2].legend(loc=\"upper left\")\n",
    "\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb54fb",
   "metadata": {},
   "source": [
    "The Markov model does seem to capture economic states reasonably well using the exogenous predictors: when the VIX goes up, we are more likely to be in Regime 0, and vice versa.  However, this is the opposite of what we found in the training period—there, Regime 0 was the \"good\" state of the world, while Regime 1 represented the \"crisis\" state with higher market volatility.\n",
    "\n",
    "VOO also doesn’t cooperate: VOO shows reduced volatility with a high expected return even during periods classified as the \"crisis\" Regime 1.  This highlights a broader issue with the Markov regime-switching model in the out-of-sample test: we have relatively few observations overall, and the last few years appear to fall outside the typical pattern on which the model was trained.  Ultimately, this helps explain the relatively low out-of-sample return of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6605bc0",
   "metadata": {},
   "source": [
    "We can compare the out-of-sample performance of our two methods and plot the cumulative returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed62a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gather the two OOS return streams + VOO for reference --------------------\n",
    "oos_strategies = {\n",
    "    \"Regime-Aware Dynamic\": oos_port_ret,    # from the Markov model loop\n",
    "    \"Static Raw (Risk-Match)\": oos_ret_raw,  # one-shot raw-frontier weights\n",
    "    \"VOO Benchmark\": test_returns[\"VOO\"].loc[oos_port_ret.index],\n",
    "}\n",
    "\n",
    "# --- Cumulative-performance plot during the test window -----------------------\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "for name, ser in oos_strategies.items():\n",
    "    if ser.dropna().empty:\n",
    "        continue\n",
    "    growth = (1 + ser).cumprod()\n",
    "    style  = {\"lw\": 3, \"linestyle\": \"--\", \"color\": \"black\"} if name.startswith(\"VOO\") else {\"lw\": 2}\n",
    "    growth.plot(ax=ax, label=name, **style)\n",
    "\n",
    "ax.set_title(\"Cumulative Performance – OOS Test Window\", fontsize=14)\n",
    "ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Growth of $1 (log scale)\"); ax.set_yscale(\"log\")\n",
    "ax.legend(loc=\"upper left\", fontsize=9); ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# --- Performance-metrics table ------------------------------------------------\n",
    "perf = {name: calculate_performance_metrics(ser.dropna()) for name, ser in oos_strategies.items()}\n",
    "perf_df = pd.DataFrame(perf).T\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"      OUT-OF-SAMPLE PERFORMANCE METRICS  \")\n",
    "print(\"=\"*60)\n",
    "print(perf_df.sort_values(\"Sharpe Ratio\", ascending=False).round(2))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066dc262",
   "metadata": {},
   "source": [
    "The performance of the Markov regime-switching model is quite disappointing, given how well it performed in-sample.  What could explain this?  On the one hand, the recent changes in interest rate policy over the past couple of years are difficult for the model to capture, as it was trained during a relatively low interest rate environment.  There is also the issue that the further we forecast regimes away from the original training period, the more unreliable the model becomes.  In fact, during the first couple of months after fitting the Markov model, we actually *do* outperform VOO!\n",
    "\n",
    "Lastly, there is also a question of realism: would we really estimate our model once and then keep it unchanged for years, ignoring any recent information?\n",
    "\n",
    "We can certainly do better.  It would make more economic sense to regularly re-fit the model, incorporating the most recent observations.  Below, I implement this idea: starting with the original training window, I keep the model fixed for several months, then re-estimate it using an updated training set that includes the most recent data.  To determine the best re-fitting frequency, I also conduct hyperparameter tuning—specifically, testing whether we should re-fit every 3, 6, or 12 months.\n",
    "\n",
    "It is also worth noting that the Markov regime-switching model is quite sensitive to the starting conditions of the optimization.  In other words, results can vary significantly depending on the random seed used.  This is not ideal, but expected given the relatively small number of observations available to estimate the model.  To improve the likelihood of reaching the global optimum, we can increase `search_reps` to explore better starting values and raise `maxiter` to allow for more iterations during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38cf9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_grid        = [3, 6, 12]         # months between re-estimations to try\n",
    "results_dyn       = {}                 # CAGR for Regime-Aware strategy\n",
    "results_raw       = {}                 # CAGR for Static RAW frontier\n",
    "best_series_dyn   = None               # will hold return series for best chunk\n",
    "best_series_raw   = None\n",
    "\n",
    "for CHUNK_SIZE in chunk_grid:\n",
    "    print(\"\\n\" + \"#\"*85)\n",
    "    print(f\" BACK-TEST: refit every {CHUNK_SIZE:>2} months \")\n",
    "    print(\"#\"*85)\n",
    "\n",
    "    # fresh copies so each grid value starts from the same baseline\n",
    "    tr_ret   = train_returns.copy()\n",
    "    tr_exog  = train_exog.copy()\n",
    "\n",
    "    oos_dyn_list, oos_raw_list = [], []\n",
    "\n",
    "    test_months = test_returns.index\n",
    "    for start in range(0, len(test_months), CHUNK_SIZE):\n",
    "        idx_slice  = test_months[start : start + CHUNK_SIZE]\n",
    "        if idx_slice.empty: break\n",
    "        slice_ret  = test_returns.loc[idx_slice]\n",
    "        slice_exog = test_exog.loc[idx_slice]\n",
    "\n",
    "        print(f\"\\n{'='*70}\\n  Refit #{start//CHUNK_SIZE+1} | \"\n",
    "              f\"chunk = {CHUNK_SIZE} mo | data through {idx_slice[-1]:%Y-%m}\\n{'='*70}\")\n",
    "\n",
    "        # ── Static RAW frontier ------------------------------------------------\n",
    "        exp_raw = tr_ret.mean().values * 12 - expense_vector\n",
    "        cov_raw = tr_ret.cov().values  * 12\n",
    "        ef_raw  = prune_frontier(efficient_frontier(cov_raw, exp_raw, FRONTIER_POINTS))\n",
    "        _, w_raw = select_portfolio(ef_raw, \"sigma\", voo_sigma_annual)\n",
    "        if w_raw is None:\n",
    "            w_raw = np.array([1.0 if s == \"VOO\" else 0.0 for s in etf_symbols])\n",
    "\n",
    "        # print top 3 holdings for transparency\n",
    "        print(\"  RAW-frontier top holdings:\")\n",
    "        for idx in np.argsort(w_raw)[-3:][::-1]:\n",
    "            if w_raw[idx] > 0.01:\n",
    "                sym   = etf_symbols[idx]\n",
    "                name  = etf_name_map.get(sym, \"Unknown\")\n",
    "                print(f\"    {sym:<5} ({name}): {w_raw[idx]:.2%}\")\n",
    "\n",
    "        w_raw_df   = pd.DataFrame([w_raw] * len(idx_slice), index=idx_slice, columns=etf_symbols)\n",
    "        rtn_raw_sl = (w_raw_df.shift(1) * slice_ret[etf_symbols]).sum(axis=1).dropna()\n",
    "        oos_raw_list.append(rtn_raw_sl)\n",
    "\n",
    "        # ── Markov model -------------------------------------------------------\n",
    "        mr_tr  = MarkovRegression(tr_ret[\"VOO\"], k_regimes=2, trend=\"c\",\n",
    "                                  switching_variance=True, exog_tvtp=sm.add_constant(tr_exog))\n",
    "\n",
    "        res_tr = mr_tr.fit(search_reps=30, maxiter=300, disp=False)\n",
    "  \n",
    "        labels = res_tr.smoothed_marginal_probabilities.idxmax(axis=1)\n",
    "        # give it the *matching* date index — use the last len(labels) dates\n",
    "        #labels.index = tr_ret.index[-len(labels):]\n",
    "\n",
    "        # use only the rows that the model actually used\n",
    "        #aligned_ret  = tr_ret.loc[labels.index]\n",
    "        aligned_ret  = tr_ret\n",
    "\n",
    "        counts = labels.value_counts()\n",
    "        print(\"Obs per regime in current training window:\")\n",
    "        for r in range(k_regimes):\n",
    "            print(f\"  Regime {r}: {counts.get(r, 0)} months\")\n",
    "        \n",
    "        reg_w, *_ = compute_regime_optimal_weights(\n",
    "            returns=aligned_ret,\n",
    "            regime_labels=labels,\n",
    "            etf_symbols=etf_symbols,\n",
    "            expense_vector=expense_vector,\n",
    "            target_volatility=voo_sigma_annual,\n",
    "            n_regimes=2,\n",
    "            frontier_points=FRONTIER_POINTS,\n",
    "            min_obs_per_regime=24,\n",
    "        )\n",
    "\n",
    "        mr_ts   = MarkovRegression(slice_ret[\"VOO\"], k_regimes=2, trend=\"c\",\n",
    "                                   switching_variance=True, exog_tvtp=sm.add_constant(slice_exog))\n",
    "        res_ts  = mr_ts.filter(res_tr.params)\n",
    "        probs   = res_ts.predicted_marginal_probabilities\n",
    "        probs.index = idx_slice\n",
    "\n",
    "        w_dyn   = (probs.apply(lambda p: sum(p[j]*reg_w[j] for j in range(2)),\n",
    "                               axis=1, result_type=\"expand\")\n",
    "                          .pipe(lambda w: w.div(w.sum(axis=1), axis=0)))\n",
    "        w_dyn.columns = etf_symbols\n",
    "        rtn_dyn_sl = (w_dyn.shift(1) * slice_ret[etf_symbols]).sum(axis=1).dropna()\n",
    "        oos_dyn_list.append(rtn_dyn_sl)\n",
    "\n",
    "        # expand training window\n",
    "        tr_ret  = pd.concat([tr_ret,  slice_ret])\n",
    "        tr_exog = pd.concat([tr_exog, slice_exog])\n",
    "\n",
    "    # full OOS series for this chunk size\n",
    "    series_dyn = pd.concat(oos_dyn_list).rename(f\"dyn_{CHUNK_SIZE}\")\n",
    "    series_raw = pd.concat(oos_raw_list).rename(f\"raw_{CHUNK_SIZE}\")\n",
    "\n",
    "    cagr_dyn   = (1+series_dyn).prod()**(12/len(series_dyn)) - 1\n",
    "    cagr_raw   = (1+series_raw).prod()**(12/len(series_raw)) - 1\n",
    "    results_dyn[CHUNK_SIZE] = cagr_dyn\n",
    "    results_raw[CHUNK_SIZE] = cagr_raw\n",
    "\n",
    "    if best_series_dyn is None or cagr_dyn > results_dyn.get(best_series_dyn.name.split('_')[-1], -9):\n",
    "        best_series_dyn = series_dyn\n",
    "    if best_series_raw is None or cagr_raw > results_raw.get(best_series_raw.name.split('_')[-1], -9):\n",
    "        best_series_raw = series_raw\n",
    "\n",
    "# ── pick optimal chunk sizes --------------------------------------------------\n",
    "best_chunk_dyn  = max(results_dyn,  key=results_dyn.get)\n",
    "best_chunk_raw  = max(results_raw,  key=results_raw.get)\n",
    "\n",
    "print(\"\\n\" + \"#\"*85)\n",
    "print(\"Hyper-parameter tuning complete\")\n",
    "for c in chunk_grid:\n",
    "    print(f\"  {c:>2}-month | Regime-Aware CAGR {results_dyn[c]:.2%} \"\n",
    "          f\"| RAW CAGR {results_raw[c]:.2%}\")\n",
    "print(f\"\\n⇒ Best chunk: Regime-Aware = {best_chunk_dyn} mo,  RAW = {best_chunk_raw} mo\")\n",
    "print(\"#\"*85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9103bd",
   "metadata": {},
   "source": [
    "It turns out that semi-annual re-fitting is optimal for both the static mean-variance optimization and the Markov regime-switching model.\n",
    "\n",
    "There are a few interesting observations from the re-fitting procedure:\n",
    "\n",
    "* **Static Mean-Variance Optimization:**\n",
    "  Adding new data over time has little impact on the optimal portfolio. Across most re-fitting periods, the optimizer continues to allocate roughly 80% to the Russell 1000 Growth ETF and the remainder to the Short-Term Treasury ETF or the Dividend Appreciation ETF.\n",
    "\n",
    "* **Markov Regime-Switching Model:**\n",
    "  For the re-fitted model up to 2023, Regime 0 reflects a defensive portfolio—comprising Value ETFs and Treasury ETFs—while Regime 1 is fully invested in the Mega Cap Growth ETF.  However, by *Refit #3* in August 2023, this pattern reverses. The rising interest rate environment appears to shift market dynamics: Regime 0 now favors full investment in Mega Cap Growth, while Regime 1 adopts a more defensive stance.\n",
    "\n",
    "* **Later Re-fits and Emerging Trends:**\n",
    "  After 2023, the regimes alternate again in terms of which one favors the Mega Cap Growth ETF. More interestingly, we begin to see a rising allocation to the Dividend Appreciation ETF (VIG). As discussed above, during periods of rising interest rates, strong dividend growth can help support equity prices by partially offsetting the negative impact of higher discount rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eada54a",
   "metadata": {},
   "source": [
    "As with the in-sample backtesting above, we can now calculate the out-of-sample performance metrics for both the one-shot fitted and the re-fitted models, as well as plot the cumulative returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather ALL OOS return streams (align on common index)\n",
    "oos_strategies = {\n",
    "    \"Dynamic Regime (refit)\"      : best_series_dyn,  # refit Markov\n",
    "    \"Dynamic Regime (single fit)\" : oos_port_ret,     # one-shot Markov\n",
    "    \"RAW Frontier (refit)\"        : best_series_raw,  # refit raw σ-match\n",
    "    \"Static RAW Frontier\"         : oos_ret_raw,      # one-shot raw σ-match\n",
    "    \"VOO Benchmark\"               : test_returns[\"VOO\"].loc[oos_port_ret.index],\n",
    "}\n",
    "\n",
    "# make sure all series share the same date index for plotting / metrics\n",
    "common_idx = sorted(set().union(*(s.index for s in oos_strategies.values())))\n",
    "oos_strategies = {k: v.reindex(common_idx).dropna() for k, v in oos_strategies.items()}\n",
    "\n",
    "# Performance-metrics table\n",
    "perf = {name: calculate_performance_metrics(ser) for name, ser in oos_strategies.items()}\n",
    "perf_df = pd.DataFrame(perf).T\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"           OUT-OF-SAMPLE PERFORMANCE METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(perf_df.sort_values(\"Sharpe Ratio\", ascending=False).round(2))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc52619",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for name, ser in oos_strategies.items():\n",
    "    growth = (1 + ser).cumprod()\n",
    "    style  = {\"lw\": 3, \"linestyle\": \"--\", \"color\": \"black\"} if name.startswith(\"VOO\") else {\"lw\": 2}\n",
    "    growth.plot(ax=ax, label=name, **style)\n",
    "\n",
    "ax.set_title(\"Cumulative Performance – Out-of-Sample Test Window\", fontsize=14)\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Growth of $1 (log scale)\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend(loc=\"upper left\", fontsize=9)\n",
    "ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8166249d",
   "metadata": {},
   "source": [
    "Under the more realistic assumption that we re-estimate the model regularly as new data become available, both the Markov regime-switching model and the mean-variance optimization method perform better.  This highlights the importance of using recent time series data when estimating models—especially given the substantial changes in the interest rate environment, which older model fits may not capture.  We also see how risky it is to extrapolate too far using an outdated model: although the Markov model initially outperforms VOO in the first few months after fitting, this advantage disappears after about half a year.\n",
    "\n",
    "Overall, simple mean-variance optimization delivers the best results—particularly when we incorporate more recent data and re-fit the model approximately every six months."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4c109e",
   "metadata": {},
   "source": [
    "### 9. Tool to select optimal portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ea4f27",
   "metadata": {},
   "source": [
    "We now know that the best performance is delivered by the optimal portfolio defined by the simple static mean-variance optimization, using the most recent set of data.  We can set up a simple tool that lets users select their preferences based on expected return or volatility, and then provides the corresponding optimal portfolio on the mean-variance efficient frontier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58461cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Frontier arrays – ensure they exist ---\n",
    "\n",
    "sigmas = np.asarray(ef_raw[\"sigma\"])\n",
    "mus = np.asarray(ef_raw[\"mu\"])\n",
    "weights = np.asarray(ef_raw[\"weights\"])\n",
    "\n",
    "symbols = globals().get(\"symbols\", etf_symbols)\n",
    "name_map = globals().get(\"name_map\", etf_name_map)\n",
    "\n",
    "# --- Build the figure ---\n",
    "\n",
    "fig = go.FigureWidget()\n",
    "fig.layout.hovermode = \"closest\"\n",
    "fig.layout.clickmode = \"event+select\"\n",
    "fig.layout.plot_bgcolor = \"white\"\n",
    "fig.layout.paper_bgcolor = \"white\"\n",
    "\n",
    "fig.add_scatter(x=sigmas, y=mus, mode=\"lines\", line=dict(color=\"lightgray\"),\n",
    "                hoverinfo=\"skip\", showlegend=False)\n",
    "\n",
    "frontier_pts = go.Scatter(\n",
    "    x=sigmas, y=mus, mode=\"markers\",\n",
    "    marker=dict(size=7, color=\"gray\", opacity=0.5),\n",
    "    name=\"Raw Frontier\",\n",
    "    hoverlabel=dict(bgcolor=\"white\"),\n",
    "    hovertemplate=\"σ: %{x:.2%}<br>µ: %{y:.2%}<br>(click to see top weights)<extra></extra>\"\n",
    ")\n",
    "fig.add_trace(frontier_pts)\n",
    "\n",
    "fig.add_scatter(x=[voo_sigma_annual], y=[voo_mu_annual], mode=\"markers\",\n",
    "                marker=dict(symbol=\"diamond\", size=14, color=\"royalblue\", line=dict(width=2, color=\"black\")),\n",
    "                name=\"VOO (ref)\",\n",
    "                hoverlabel=dict(bgcolor=\"white\"),\n",
    "                hovertemplate=\"VOO\\nσ: %{x:.2%}<br>µ: %{y:.2%}<extra></extra>\")\n",
    "\n",
    "mid = len(sigmas) // 2\n",
    "fig.add_scatter(x=[sigmas[mid]], y=[mus[mid]], mode=\"markers\",\n",
    "                marker=dict(size=14, color=\"red\", line=dict(width=2, color=\"black\")),\n",
    "                name=\"Selected\",\n",
    "                hoverinfo=\"skip\")\n",
    "sel_idx = len(fig.data) - 1\n",
    "\n",
    "fig.update_layout(title=\"Efficient Frontier (Raw Estimates)\",\n",
    "                  xaxis_title=\"Volatility (σ)\", yaxis_title=\"Expected Return (µ)\",\n",
    "                  height=480)\n",
    "\n",
    "# --- Top 3 Portfolio Components ---\n",
    "\n",
    "def format_html(idx: int) -> str:\n",
    "    w = weights[idx]\n",
    "    top_idx = np.argsort(w)[-3:][::-1]\n",
    "    rows = [\n",
    "        f\"<li><b>{symbols[i]}</b> <span style='color:#666'>({name_map.get(symbols[i],'Unknown')})</span>\"\n",
    "        f\"<span style='float:right'>{w[i]:.2%}</span></li>\"\n",
    "        for i in top_idx if w[i] > 0.001\n",
    "    ]\n",
    "    return (\n",
    "        \"<div style='font-family:Arial, sans-serif; font-size:14px; line-height:1.4; max-width:420px;'>\"\n",
    "        f\"<h4 style='margin:4px 0 8px 0; font-size:15px;'>Top 3 ETFs – Frontier Point {idx+1}</h4>\"\n",
    "        \"<ul style='list-style:none; padding-left:0; margin:0;'>\" + \"\\n\".join(rows) + \"</ul></div>\"\n",
    "    )\n",
    "\n",
    "out = HTML()\n",
    "\n",
    "# --- Sliders ---\n",
    "\n",
    "mu_slider = FloatSlider(value=mus[mid], min=mus.min(), max=mus.max(), step=0.0005,\n",
    "                        description=\"Target µ\", readout_format=\".2%\", continuous_update=False,\n",
    "                        layout=dict(width=\"450px\"))\n",
    "\n",
    "sigma_slider = FloatSlider(value=sigmas[mid], min=sigmas.min(), max=sigmas.max(), step=0.0005,\n",
    "                           description=\"Target σ\", readout_format=\".2%\", continuous_update=False,\n",
    "                           layout=dict(width=\"450px\"))\n",
    "\n",
    "# --- Sync logic ---\n",
    "\n",
    "def move_to_index(idx: int):\n",
    "    with fig.batch_update():\n",
    "        fig.data[sel_idx].x = [sigmas[idx]]\n",
    "        fig.data[sel_idx].y = [mus[idx]]\n",
    "    mu_slider.value = mus[idx]\n",
    "    sigma_slider.value = sigmas[idx]\n",
    "    out.value = format_html(idx)\n",
    "\n",
    "move_to_index(mid)\n",
    "\n",
    "def on_mu_change(change):\n",
    "    if change[\"name\"] == \"value\":\n",
    "        idx = int(np.argmin(np.abs(mus - change[\"new\"])))\n",
    "        move_to_index(idx)\n",
    "\n",
    "def on_sigma_change(change):\n",
    "    if change[\"name\"] == \"value\":\n",
    "        idx = int(np.argmin(np.abs(sigmas - change[\"new\"])))\n",
    "        move_to_index(idx)\n",
    "\n",
    "mu_slider.observe(on_mu_change, names=\"value\")\n",
    "sigma_slider.observe(on_sigma_change, names=\"value\")\n",
    "\n",
    "fig.data[1].on_click(lambda trace, points, selector: move_to_index(points.point_inds[0]) if points.point_inds else None)\n",
    "\n",
    "# --- Snap-to button for VOO mu and sigma ---\n",
    "\n",
    "snap_button = Button(description=\"Snap to VOO\", button_style=\"info\")\n",
    "snap_criterion = Dropdown(\n",
    "    options=[(\"Match µ (return)\", \"mu\"), (\"Match σ (volatility)\", \"sigma\")],\n",
    "    value=\"mu\",\n",
    "    layout=dict(width=\"180px\")\n",
    ")\n",
    "\n",
    "def on_snap(_):\n",
    "    if snap_criterion.value == \"mu\":\n",
    "        idx = int(np.argmin(np.abs(mus - voo_mu_annual)))\n",
    "    else:\n",
    "        idx = int(np.argmin(np.abs(sigmas - voo_sigma_annual)))\n",
    "    move_to_index(idx)\n",
    "\n",
    "snap_button.on_click(on_snap)\n",
    "\n",
    "# --- Display ------------------------------------------------------\n",
    "\n",
    "display(VBox([\n",
    "    fig,\n",
    "    mu_slider,\n",
    "    sigma_slider,\n",
    "    HBox([snap_button, snap_criterion]),\n",
    "    out\n",
    "]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## =============================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n#               PORTFOLIO OPTIMIZATION & DYNAMIC ASSET ALLOCATION\n#\n# ==============================================================================\n#\n# OVERVIEW:\n#\n# This script performs a comprehensive portfolio analysis using various optimization\n# techniques. It starts with classic Mean-Variance Optimization and then explores\n# more advanced methods to create robust and dynamic asset allocation strategies.\n#\n# The script is divided into the following main parts:\n#\n#   1.  SETUP & CONFIGURATION:\n#       -   Loads necessary libraries and defines global settings.\n#\n#   2.  DATA LOADING & PREPARATION:\n#       -   Fetches a list of Vanguard ETFs and their metadata.\n#       -   Downloads historical price data for these ETFs from Yahoo Finance.\n#       -   Processes the data to calculate monthly returns over a 10-year window.\n#\n#   3.  STATIC PORTFOLIO ANALYSIS (MEAN-VARIANCE OPTIMIZATION):\n#       -   Calculates the expected returns (mu) and covariance matrix of the ETFs.\n#       -   Introduces \"shrinkage\" techniques (Ledoit-Wolf) to create more\n#           stable estimates of these parameters.\n#       -   Defines a function to compute the \"Efficient Frontier,\" which represents\n#           the set of optimal portfolios.\n#       -   Constructs several static portfolios and compares them to a benchmark (VOO).\n#\n#   4.  ADVANCED STATIC MODELS & ROBUSTNESS CHECKS:\n#       -   Performs a Monte Carlo simulation to forecast portfolio performance.\n#       -   Uses a \"Resampled Efficient Frontier\" (bootstrapping) to build a\n#           portfolio that is less sensitive to estimation errors.\n#       -   Implements a \"Rolling Window\" analysis to see how the optimal\n#           portfolio weights would have changed over time.\n#       -   Implement Black-Litterman, Risk Parity, and Hierarchical Risk Parity (HRP) Optimization\n#           analysis additional advanced methods.\n#\n#   5.  REGIME-SWITCHING MODEL:\n#       -   Loads external economic data (VIX, US Treasury yields) that may\n#           influence market behavior.\n#       -   Fits a Markov Regime-Switching model to identify distinct market\n#           \"regimes\" (e.g., low-volatility growth, high-volatility decline).\n#\n#   6.  REGIME-AWARE DYNAMIC STRATEGY:\n#       -   Calculates an optimal portfolio for each identified market regime.\n#       -   Backtests a dynamic strategy that adjusts its portfolio allocation\n#           based on the real-time probability of being in each regime.\n#\n#   7.  FINAL PERFORMANCE COMPARISON:\n#       -   Plots the cumulative returns of all tested strategies.\n#       -   Calculates and displays a table of key performance metrics (Return,\n#           Volatility, Sharpe Ratio, Max Drawdown) for a comprehensive comparison.\n#\n# ==============================================================================\n\n# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SECTION 1: SETUP & CONFIGURATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n\n# --- Standard Library Imports ---\nimport os\nimport re\nimport pandas as pd\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By\nimport time\n\nimport numpy as np\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nimport cvxopt as opt\nimport statsmodels.api as sm\n\nfrom scipy.cluster.hierarchy import linkage, dendrogram, leaves_list\nfrom scipy.spatial.distance import squareform\nfrom scipy.optimize import minimize\nimport cvxpy as cp\n\nfrom sklearn.covariance import LedoitWolf\nfrom pandas_datareader.data import DataReader\nfrom statsmodels.tsa.regime_switching.markov_regression import MarkovRegression\n\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\n\n# --- Global Settings & Constants ---\n\n# NOTE: You may need to change this to the directory where your script and data file are located.\nDIRECTORY = '.'\n# The file 'vanguard_etf_list.xlsx' is expected to be in the above directory.\n# It should contain ETF metadata like Symbol, Fund name, and Expense ratio.\n\n# Analysis Period\nANALYSIS_YEARS = 15\n\n# Optimization & Simulation Parameters\nFRONTIER_POINTS = 50          # Number of points to calculate on the efficient frontier.\nMC_SIM_SCENARIOS = 10000      # Number of scenarios for Monte Carlo simulation.\nMC_SIM_HORIZON_MONTHS = 120   # 10-year horizon for simulation.\nRESAMPLE_ITERATIONS = 100     # Number of bootstrap iterations for resampled frontier.\nROLLING_WINDOW_MONTHS = 60    # 5-year rolling window for dynamic weight analysis.\n\n# Regime Modeling Parameters\nMIN_OBS_PER_REGIME = 10       # Minimum data points required to consider a regime valid.\nMAX_REGIMES_TO_TEST = 4       # Test models with 2 up to this number of regimes.\n\n# --- Initial Setup ---\nos.chdir(DIRECTORY)\n\n# Configure pandas to display floating-point numbers with 3 decimal places.\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n# Configure the CVXOPT solver to not display progress messages during optimization.\nopt.solvers.options['show_progress'] = False\n\n\n# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SECTION 2: DATA LOADING & PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\nprint(\"--- Section 2: Loading ETF Data and Calculating Returns ---\")\n\n# --- 2a. Load ETF Metadata Directly from Vanguard Website ---\n# Vanguard\u2019s advisor site displays all available ETFs, including their tickers, names, and expense ratios.\n# Since the data is rendered with JavaScript, we use Selenium to simulate a browser and extract the table contents.\n# The site uses pagination to split the full list into two pages (~97 funds total), which we handle below.\n\n# --- Setup Selenium Chrome driver in headless mode (no visible browser window) ---\noptions = Options()\noptions.add_argument(\"--headless=new\")       # Use latest headless mode\noptions.add_argument(\"--disable-gpu\")        # Optional: improves headless stability\n\n# Point to local chromedriver binary (must match your installed Chrome version)\nservice = Service(executable_path=\"./chromedriver\")\ndriver = webdriver.Chrome(service=service, options=options)\n\n# --- Helper function to extract ETF data from the current table view ---\ndef extract_table_data():\n    \"\"\"Extract ETF data from the current table view as a list of dictionaries.\"\"\"\n    table = driver.find_element(By.CSS_SELECTOR, \"table tbody\")     # Locate table body\n    rows = table.find_elements(By.TAG_NAME, \"tr\")                   # Each row is one ETF\n    data = []\n\n    for row in rows:\n        try:\n            # Extract ticker symbol (1st column)\n            symbol = row.find_elements(By.TAG_NAME, \"td\")[0].text.strip()\n\n            # Extract and clean fund name (from embedded <div> tag)\n            raw_name = row.find_elements(By.TAG_NAME, \"div\")[1].text.strip()\n            fund_name = re.sub(r\"\\s*(NEW FUND)?\\s*$\", \"\", raw_name.replace(\"\\n\", \" \")).strip()\n\n            # Extract and convert expense ratio (8th column), handle missing or malformed data\n            expense_ratio = row.find_elements(By.TAG_NAME, \"td\")[7].text.strip().replace('%', '').strip()\n            try:\n                expense_ratio = float(expense_ratio) / 100\n            except ValueError:\n                expense_ratio = None\n\n            data.append({\n                \"Symbol\": symbol,\n                \"Fund name\": fund_name,\n                \"Expense ratio\": expense_ratio\n            })\n\n        except Exception:\n            # Skip row if parsing fails (e.g. structure is malformed)\n            continue\n\n    return data\n\n# --- Scrape data across paginated ETF table ---\ntry:\n    # Step 1: Load Vanguard ETF page and allow full render\n    driver.get(\"https://advisors.vanguard.com/investments/etfs\")\n    time.sleep(6)\n\n    # Step 2: Extract ETF data from first page (first 50 ETFs)\n    extracted_data = extract_table_data()\n\n    # Step 3: Click \"Next page\" button to reveal remaining ETFs\n    try:\n        next_button = driver.find_element(By.XPATH, '//button[@aria-label[contains(., \"Forward one page\")]]')\n        next_button.click()\n        time.sleep(5)  # Wait for second page to load\n\n        # Step 4: Extract ETF data from second page (remaining ~47 ETFs)\n        extracted_data += extract_table_data()\n\n    except Exception as e:\n        print(\"Pagination failed or second page not available.\")\n        print(f\"Reason: {e}\")\n\n    # Step 5: Load ETF metadata into DataFrame and create lookup dictionaries\n    df = pd.DataFrame(extracted_data)\n    etf_name_map = dict(zip(df['Symbol'], df['Fund name']))\n    etf_expense_map = dict(zip(df['Symbol'], df['Expense ratio']))\n    etf_symbols = list(etf_name_map.keys())\n\n    print(f\"Successfully extracted {len(etf_symbols)} ETFs across pages.\")\n    print(df.head())\n\n# --- Handle errors by falling back to a static list of core ETFs ---\nexcept Exception as e:\n    print(\"Could not complete ETF extraction.\")\n    print(f\"Reason: {e}\")\n\n    etf_symbols = ['VOO', 'VTI', 'VEA', 'VWO', 'BND', 'BNDX', 'VGIT', 'VGLT', 'VTIP', 'MUB']\n    etf_name_map = {s: s for s in etf_symbols}\n    etf_expense_map = {s: 0.0003 for s in etf_symbols}  # Fallback default expense ratio\n\n# --- Always close the Selenium browser session, even on failure ---\nfinally:\n    driver.quit()\n\n    \n# --- 2b. Filter the ETF Universe ---\n# To create a diversified portfolio of broad asset classes, we remove specialized\n# sector-specific ETFs and redundant funds.\nindustry_keywords = [\n    'Energy', 'Health Care', 'Consumer', 'Materials', 'Financials',\n    'Utilities', 'Real Estate', 'Industrials', 'Communication', 'Information Technology'\n]\n# List of specific ETFs to remove, often because they are sector-focused or\n# overlap significantly with broader ETFs like VTI.\nremove_symbols = ['VGT', 'VHT', 'VPU', 'VDC', 'VAW', 'VIS', 'VFH', 'VNQ', 'VOX', 'VDE', 'VCR']\n\ndef is_industry_or_duplicate(symbol):\n    \"\"\"Checks if an ETF is a sector-specific or redundant fund based on its name or symbol.\"\"\"\n    name = etf_name_map.get(symbol, '')\n    is_industry = any(keyword in name for keyword in industry_keywords)\n    is_duplicate = symbol in remove_symbols\n    return is_industry or is_duplicate\n\n# Apply the filter and remove any duplicate symbols that might exist in the original list.\netf_symbols = [sym for sym in etf_symbols if not is_industry_or_duplicate(sym)]\netf_symbols = list(dict.fromkeys(etf_symbols))\nprint(f\"Filtered down to {len(etf_symbols)} ETFs for analysis.\")\n\n\n# --- 2c. Fetch Historical Price Data ---\ndef get_total_return_series(ticker):\n    \"\"\"\n    Fetches the maximum available historical price data for a given ticker\n    from Yahoo Finance.\n\n    Args:\n        ticker (str): The stock or ETF symbol.\n\n    Returns:\n        pd.DataFrame: A DataFrame with a single column of historical closing prices,\n                      adjusted for dividends and splits. Returns an empty DataFrame\n                      if the ticker cannot be fetched.\n    \"\"\"\n    print(f\"Processing {ticker}...\")\n    try:\n        stock = yf.Ticker(ticker)\n        # 'back_adjust=True' provides a total return series by adjusting prices\n        # for both dividends and stock splits. 'auto_adjust=False' is needed\n        # for back_adjust to work.\n        df = stock.history(\n            period=\"max\",\n            auto_adjust=False,\n            back_adjust=True\n        )[['Close']].rename(columns={'Close': ticker})\n        return df\n    except Exception as e:\n        print(f\"Could not fetch data for {ticker}: {e}\")\n        return pd.DataFrame()\n\n# Loop through the filtered list of ETFs and combine their price series into one DataFrame.\nall_prices = pd.DataFrame()\nfor ticker in etf_symbols:\n    price_df = get_total_return_series(ticker)\n    if not price_df.empty:\n        all_prices = pd.concat([all_prices, price_df], axis=1)\n\n# --- 2d. Process and Clean Return Data ---\n# Standardize the index to datetime objects without timezone information.\nall_prices.index = pd.to_datetime(all_prices.index).tz_localize(None)\n\n# Resample daily prices to month-end prices, then calculate monthly percentage returns.\nreturns_monthly = all_prices.resample('M').last().pct_change()\n\n# Limit the data to the last N years for a more relevant analysis window.\ncutoff_date = returns_monthly.index.max() - pd.DateOffset(years=ANALYSIS_YEARS)\nreturns_monthly = returns_monthly[returns_monthly.index >= cutoff_date]\n\n# Data Cleaning:\n# 1. Drop any ETF (column) that doesn't have at least 50% of the data points in our window.\nmin_observations = int(len(returns_monthly) * 0.50)\nreturns_monthly = returns_monthly.dropna(axis=1, thresh=min_observations)\n\n# 2. Drop any month (row) that still has missing values after filtering columns.\nreturns_monthly = returns_monthly.dropna(axis=0)\n\n# Update the final list of ETF symbols and related data based on the cleaned DataFrame.\netf_symbols = returns_monthly.columns.tolist()\n\n# The S&P 500 ETF (VOO) is used as our primary benchmark. The analysis cannot\n# proceed without it.\nif 'VOO' not in etf_symbols:\n    raise ValueError(\"VOO data is missing or was dropped. It is required for the benchmark comparison.\")\n\n# Create a NumPy array of expense ratios in the same order as our final ETF symbols.\n# This will be used to calculate net returns.\nexpense_vector = np.array([etf_expense_map.get(sym, 0.0) for sym in etf_symbols])\nprint(f\"Final analysis will use {len(etf_symbols)} ETFs over {len(returns_monthly)} months.\")\n\n\n# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SECTION 3: STATIC PORTFOLIO ANALYSIS (MEAN-VARIANCE OPTIMIZATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\nprint(\"\\n--- Section 3: Performing Static Mean-Variance Optimization ---\")\n\n# --- 3a. Estimate Expected Returns and Covariance ---\n# Portfolio optimization requires two key inputs:\n# 1. Expected Returns (mu): The anticipated return for each asset.\n# 2. Covariance Matrix: A measure of how asset returns move together.\n\n# Calculate historical annualized mean returns, net of expense ratios.\n# We multiply by 12 to annualize the monthly mean returns.\nannual_mu_sample = returns_monthly.mean().values * 12 - expense_vector\n\n# The sample covariance matrix is calculated from the historical returns.\nsample_cov = returns_monthly.cov().values\nannual_cov_sample = sample_cov * 12\n\n# --- 3b. Apply Shrinkage to Improve Estimates ---\n# Historical sample estimates can be \"noisy\" and may not be good predictors of the future.\n# Shrinkage techniques adjust these estimates to be more stable and robust.\n\n# Ledoit-Wolf Shrinkage for the Covariance Matrix:\n# This method computes an optimal blend of the sample covariance matrix and a more\n# structured, less noisy target matrix. It's a standard way to get a more reliable\n# covariance estimate, especially when the number of assets is large relative to\n# the number of observations.\nlw = LedoitWolf().fit(returns_monthly.values)\nannual_cov_shrunk = lw.covariance_ * 12\n\n# Note on James-Stein Shrinkage for Mean Returns:\n# The original script attempted James-Stein shrinkage for the mean returns.\n# However, this method can be overly aggressive, often shrinking all expected\n# returns towards the grand mean, which eliminates valuable differences between assets.\n# We will proceed using the simple historical sample means, as they are more common\n# in practice for this type of analysis.\n#\n# def james_stein_shrinkage(mu):\n#     mu_bar = mu.mean()\n#     n = len(mu)\n#     # Formula corrected for clarity\n#     shrinkage_factor = 1 - ((n - 3) * mu.var()) / ((n - 1) * np.sum((mu - mu_bar) ** 2))\n#     shrinkage_factor = max(0, min(shrinkage_factor, 1))\n#     return shrinkage_factor * mu_bar + (1 - shrinkage_factor) * mu\n# annual_mu_shrunk = james_stein_shrinkage(annual_mu_sample)\n#\n# Based on the original script's conclusion (\"very aggressive shrinkage... not useful\"),\n# we will use the un-shrunk `annual_mu_sample` for our primary analysis.\nannual_mu = annual_mu_sample\n\nprint(\"Annualized Expected Returns (Sample):\")\nprint(pd.Series(annual_mu, index=etf_symbols).round(3))\n\n\n# --- 3c. Define the Efficient Frontier Optimizer ---\ndef efficient_frontier(cov_mat, mu_vec, n_points=50):\n    \"\"\"\n    Calculates the efficient frontier using the Markowitz mean-variance optimization model.\n\n    The \"Efficient Frontier\" is the set of portfolios that provide the highest\n    expected return for a given level of risk (volatility).\n\n    This function uses the CVXOPT quadratic programming solver to find the portfolio\n    weights that minimize portfolio variance for a range of target expected returns.\n\n    Args:\n        cov_mat (np.array): The annualized covariance matrix of asset returns.\n        mu_vec (np.array): The annualized vector of asset expected returns.\n        n_points (int): The number of points to calculate along the frontier.\n\n    Returns:\n        dict: A dictionary containing the returns ('mu'), volatilities ('sigma'),\n              and portfolio weights ('weights') for each point on the frontier.\n    \"\"\"\n    n = len(mu_vec)  # Number of assets\n\n    # The optimization problem is to minimize: (1/2) * w' * P * w\n    # subject to constraints. Here, P is the covariance matrix.\n    P = opt.matrix(cov_mat)\n    # The 'q' term is for a linear part of the objective, which is zero here.\n    q = opt.matrix(np.zeros((n, 1)))\n\n    # Constraint 1: Weights must be non-negative (G*w <= h).\n    # -w_i <= 0  (or w_i >= 0)\n    # Constraint 2: Weights must be less than 1 (w_i <= 1).\n    G = opt.matrix(np.vstack([-np.eye(n), np.eye(n)]))\n    h = opt.matrix(np.vstack([np.zeros((n, 1)), np.ones((n, 1))]))\n\n    # Constraint 3: Sum of weights must equal 1, and portfolio return must equal target. (A*w = b)\n    # The solver will iterate through different target returns (`mu_target`).\n    A = opt.matrix(np.vstack([mu_vec, np.ones((1, n))]))\n\n    # Iterate through a range of target returns to trace the frontier.\n    target_mus = np.linspace(mu_vec.min(), mu_vec.max(), n_points)\n    frontier = {'mu': [], 'sigma': [], 'weights': []}\n\n    for mu_target in target_mus:\n        b = opt.matrix([mu_target, 1.0])  # Target return and sum of weights = 1\n        try:\n            solution = opt.solvers.qp(P, q, G, h, A, b)\n            if solution['status'] == 'optimal':\n                weights = np.array(solution['x']).flatten()\n                # Calculate the resulting portfolio volatility (sigma).\n                sigma = np.sqrt(weights.T @ cov_mat @ weights)\n                frontier['mu'].append(mu_target)\n                frontier['sigma'].append(sigma)\n                frontier['weights'].append(weights)\n        except ValueError:\n            # The solver may fail for some target returns if no solution exists.\n            pass\n\n    return frontier\n\n# --- 3d. Define Benchmark and Helper Functions ---\nvoo_returns_monthly = returns_monthly['VOO']\nvoo_mu_annual = voo_returns_monthly.mean() * 12 - etf_expense_map.get('VOO', 0.0)\nvoo_sigma_annual = voo_returns_monthly.std() * np.sqrt(12)\n\ndef select_portfolio(frontier, target_metric, target_value):\n    \"\"\"\n    Selects a portfolio from the efficient frontier that is closest to a target value.\n\n    Args:\n        frontier (dict): The efficient frontier dictionary.\n        target_metric (str): The metric to match ('mu' or 'sigma').\n        target_value (float): The target return or volatility.\n\n    Returns:\n        tuple: The index and weights of the selected portfolio, or (None, None).\n    \"\"\"\n    if not frontier[target_metric]:\n        return None, None\n    diffs = np.abs(np.array(frontier[target_metric]) - target_value)\n    idx = diffs.argmin()\n    return idx, frontier['weights'][idx]\n\n# --- 3e. Generate Frontiers and Select Portfolios ---\nprint(\"Generating efficient frontiers...\")\n# Frontier using simple sample estimates\nef_raw = efficient_frontier(annual_cov_sample, annual_mu, n_points=FRONTIER_POINTS)\n# Frontier using shrinkage-adjusted covariance\nef_shrunk = efficient_frontier(annual_cov_shrunk, annual_mu, n_points=FRONTIER_POINTS)\n\n# Find portfolios on each frontier that match the VOO benchmark's risk or return\n_, w_mu_raw = select_portfolio(ef_raw, 'mu', voo_mu_annual)\n_, w_sigma_raw = select_portfolio(ef_raw, 'sigma', voo_sigma_annual)\n_, w_mu_shrunk = select_portfolio(ef_shrunk, 'mu', voo_mu_annual)\n_, w_sigma_shrunk = select_portfolio(ef_shrunk, 'sigma', voo_sigma_annual)\n\n# Display the composition of the selected portfolios\nportfolios_to_display = {\n    'Raw (Mu-matched)': w_mu_raw,\n    'Raw (Sigma-matched)': w_sigma_raw,\n    'Shrunk (Mu-matched)': w_mu_shrunk,\n    'Shrunk (Sigma-matched)': w_sigma_shrunk\n}\n\nfor label, weights in portfolios_to_display.items():\n    if weights is not None:\n        top_indices = np.argsort(weights)[-3:][::-1]\n        print(f\"\\nTop 3 ETFs for {label} Portfolio:\")\n        for i in top_indices:\n            if weights[i] > 0.001: # Only show assets with meaningful weight\n                symbol = etf_symbols[i]\n                name = etf_name_map.get(symbol, 'Unknown')\n                print(f\"  {symbol} ({name}): {weights[i]:.2%}\")\n\n# --- 3f. Plot Static Efficient Frontiers ---\nplt.figure(figsize=(10, 7))\nplt.plot(ef_raw['sigma'], ef_raw['mu'], 'o-', label='Raw Estimate Frontier', alpha=0.7)\nplt.plot(ef_shrunk['sigma'], ef_shrunk['mu'], 'o-', label='Shrunk Covariance Frontier', lw=2)\nplt.scatter([voo_sigma_annual], [voo_mu_annual], color='red', marker='X', s=200, label='VOO Benchmark', zorder=5)\n\nplt.title('Efficient Frontiers: Raw vs. Shrunk Covariance')\nplt.xlabel('Annualized Volatility (Risk)')\nplt.ylabel('Annualized Expected Return')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SECTION 4: ADVANCED STATIC MODELS & ROBUSTNESS CHECKS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\nprint(\"\\n--- Section 4: Performing Robustness Checks ---\")\n\n# --- 4a. Monte Carlo Simulation ---\n# We simulate future returns to see how our portfolios might perform under a wide\n# range of possible outcomes, based on the historical return distribution.\nprint(f\"Running Monte Carlo simulation with {MC_SIM_SCENARIOS} scenarios...\")\n\n# Use monthly parameters for the simulation\nmonthly_mu_sample = annual_mu / 12\n\nrng = np.random.default_rng(seed=42)\nsimulated_returns_monthly = rng.multivariate_normal(\n    mean=monthly_mu_sample,\n    cov=sample_cov,\n    size=(MC_SIM_SCENARIOS, MC_SIM_HORIZON_MONTHS)\n)\n\ndef simulate_performance(weights):\n    \"\"\"Calculates performance metrics from simulated returns.\"\"\"\n    if weights is None:\n        return {'mean': np.nan, 'vol': np.nan, 'VaR_5': np.nan}\n    # Calculate portfolio returns for each scenario and time step.\n    portfolio_sim_returns = simulated_returns_monthly @ weights\n    # Annualize the results\n    annual_mean_return = np.mean(portfolio_sim_returns) * 12\n    annual_volatility = np.std(portfolio_sim_returns) * np.sqrt(12)\n    # Value-at-Risk (VaR): The worst expected loss at a 5% confidence level.\n    var_5_percent = np.percentile(portfolio_sim_returns, 5) * 12\n    return {\n        'mean': annual_mean_return,\n        'vol': annual_volatility,\n        'VaR_5': var_5_percent\n    }\n\n# Compare simulated performance of Sigma-matched portfolios vs. the benchmark\nperf_sigma = {\n    'Raw': simulate_performance(w_sigma_raw),\n    'Shrunk': simulate_performance(w_sigma_shrunk),\n    'VOO': {'mean': voo_mu_annual, 'vol': voo_sigma_annual, 'VaR_5': np.nan} # VOO is the baseline\n}\nprint(\"\\nSimulated Performance Summary (Sigma-matched to VOO):\")\nprint(pd.DataFrame(perf_sigma).T)\n\n\n# --- 4b. Resampled Efficient Frontier (Bootstrapping) ---\n# This technique addresses \"estimation error\" by creating many new return datasets\n# via bootstrapping (sampling with replacement). An optimal portfolio is found for\n# each bootstrapped sample, and the final portfolio is the average of all these\n# optimal portfolios. This leads to a more diversified and stable allocation.\nprint(f\"\\nRunning Resampled Frontier simulation with {RESAMPLE_ITERATIONS} iterations...\")\n\nn_obs = returns_monthly.shape[0]\nresampled_weights_list = []\n\nfor i in range(RESAMPLE_ITERATIONS):\n    if (i + 1) % 25 == 0:\n        print(f\"  Resample iteration {i+1}/{RESAMPLE_ITERATIONS}...\")\n\n    # Create a bootstrap sample of the monthly returns.\n    boot_indices = np.random.choice(n_obs, size=n_obs, replace=True)\n    returns_boot = returns_monthly.iloc[boot_indices]\n\n    # Recalculate parameters for the bootstrap sample.\n    mu_boot = returns_boot.mean().values * 12 - expense_vector\n    try:\n        # Use shrunk covariance for better stability in each sample.\n        lw_boot = LedoitWolf().fit(returns_boot.values)\n        cov_boot = lw_boot.covariance_ * 12\n\n        # Generate frontier and select the sigma-matched portfolio.\n        ef_boot = efficient_frontier(cov_boot, mu_boot, n_points=30)\n        _, w_boot = select_portfolio(ef_boot, 'sigma', voo_sigma_annual)\n\n        if w_boot is not None:\n            resampled_weights_list.append(w_boot)\n    except (ValueError, np.linalg.LinAlgError):\n        # Skip iteration if solver or covariance estimation fails.\n        continue\n\n# The final resampled portfolio is the average of the weights from all iterations.\nif resampled_weights_list:\n    w_resampled = np.mean(resampled_weights_list, axis=0)\n    top_indices = np.argsort(w_resampled)[-5:][::-1]\n    print(\"\\nTop 5 ETFs for Resampled Portfolio:\")\n    for i in top_indices:\n        symbol = etf_symbols[i]\n        name = etf_name_map.get(symbol, 'Unknown')\n        print(f\"  {symbol} ({name}): {w_resampled[i]:.2%}\")\nelse:\n    w_resampled = None\n    print(\"\\nCould not generate a resampled portfolio.\")\n\n\n# --- 4c. Rolling Window Estimation ---\n# This analysis shows how the optimal portfolio allocation would have changed over time\n# as new data became available, providing insight into the strategy's stability.\nprint(f\"\\nPerforming Rolling Window analysis with a {ROLLING_WINDOW_MONTHS}-month window...\")\n\nrolling_dates = returns_monthly.index[ROLLING_WINDOW_MONTHS:]\nrolling_weights_list = []\n\nfor date in rolling_dates:\n    # Create a data window of the last N months.\n    window_data = returns_monthly.loc[:date].iloc[-ROLLING_WINDOW_MONTHS:]\n\n    # Estimate parameters on the window.\n    mu_roll = window_data.mean().values * 12 - expense_vector\n    \n    # Use shrunk covariance for better performance with smaller sample.\n    lw_roll = LedoitWolf().fit(window_data.values)\n    cov_roll = lw_roll.covariance_ * 12\n\n    try:\n        # Find the optimal (sigma-matched) portfolio for this period.\n        ef_roll = efficient_frontier(cov_roll, mu_roll, n_points=30)\n        _, w_roll = select_portfolio(ef_roll, 'sigma', voo_sigma_annual)\n        if w_roll is not None:\n            rolling_weights_list.append(pd.Series(w_roll, index=etf_symbols, name=date))\n    except (ValueError, np.linalg.LinAlgError):\n        continue\n\n# Combine results and plot the weight changes for the most important assets.\nif rolling_weights_list:\n    rolling_weights_df = pd.concat(rolling_weights_list, axis=1).T\n    top_etfs = rolling_weights_df.mean().sort_values(ascending=False).head(5).index\n\n    rolling_weights_df[top_etfs].plot(\n        figsize=(12, 7),\n        title='Top 5 ETF Weights Over Time (Rolling Optimization)'\n    )\n    plt.ylabel(\"Portfolio Weight\")\n    plt.xlabel(\"Date\")\n    plt.grid(True, linestyle='--')\n    plt.tight_layout()\n    plt.show()\n\n\n# --- 4d. Black-Litterman  ---\n# This implementation uses a data-driven prior: a shrinkage estimator that blends\n# the sample mean and an equal-mean neutral vector. The investor expresses a single\n# view that VEA will outperform VWO by 1% annualized, which is encoded via matrix P and vector Q.\n# The posterior expected returns m_bl are then used in a constrained mean-variance optimizer.\nprint(\"\\n--- Black-Litterman Portfolio Optimization ---\")\n\n# 1. Estimate a data-driven prior (shrunk sample mean)\nneutral_mean = np.full_like(annual_mu_sample, annual_mu_sample.mean())\nlambda_ = 0.2  # Shrinkage intensity\npi = lambda_ * annual_mu_sample + (1 - lambda_) * neutral_mean\n\n\n# 2. Specify investor views (e.g., tilt toward international or small-cap)\n# For illustration, assume view: international (VEA, VWO) will outperform\nP = np.array([[0, 0, 0, 0, 1, -1]])  # VEA - VWO\nQ = np.array([0.01])  # View: VEA outperform VWO by 1% annualized\n\n# View uncertainty\nomega = np.diag(np.full(len(Q), 0.0025))  # Moderate confidence\nbl_tau = 0.05\n\n# Black-Litterman posterior mean\nM_inverse = np.linalg.inv(np.linalg.inv(bl_tau * annual_cov_shrunk) + P.T @ np.linalg.inv(omega) @ P)\nm_bl = M_inverse @ (np.linalg.inv(bl_tau * annual_cov_shrunk) @ pi + P.T @ np.linalg.inv(omega) @ Q)\n\n# Optimize using mean-variance\nw_bl = cp.Variable(len(etf_symbols))\nobjective = cp.Maximize(m_bl @ w_bl - cp.quad_form(w_bl, annual_cov_shrunk) * 0.5)\nconstraints = [cp.sum(w_bl) == 1, w_bl >= 0]\nprob = cp.Problem(objective, constraints)\nprob.solve()\nw_bl_opt = w_bl.value\n\nprint(\"Optimal Black-Litterman Portfolio Weights:\")\nfor i, wt in enumerate(w_bl_opt):\n    if wt > 0.01:\n        print(f\"  - {etf_symbols[i]}: {wt:.1%}\")\n\n# --- 4e. Risk Parity (HRP) ---\nprint(\"\\n--- Risk Parity Portfolio Optimization ---\")\n\n\n# --- Define functions for risk and risk contributions ---\n\n# Portfolio standard deviation\ndef portfolio_vol(weights, cov):\n    return np.sqrt(weights.T @ cov @ weights)\n\n# Marginal contribution to risk\ndef marginal_risk_contribution(weights, cov):\n    port_vol = portfolio_vol(weights, cov)\n    return (cov @ weights) / port_vol\n\n# Total contribution to risk per asset\ndef risk_contributions(weights, cov):\n    port_vol = portfolio_vol(weights, cov)\n    mrc = marginal_risk_contribution(weights, cov)\n    return weights * mrc / port_vol\n\n# Objective: minimize squared differences in risk contributions\ndef risk_parity_objective(weights, cov):\n    rc = risk_contributions(weights, cov)\n    avg_rc = np.mean(rc)\n    return np.sum((rc - avg_rc) ** 2)\n\n# ---  Solve optimization problem ---\n\nn = annual_cov_sample.shape[0]\nx0 = np.ones(n) / n  # initial weights (equal weight)\nbounds = [(0.0, 1.0) for _ in range(n)]  # long-only\nconstraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0})\n\nresult = minimize(\n    fun=risk_parity_objective,\n    x0=x0,\n    args=(annual_cov_sample,),\n    method='SLSQP',\n    bounds=bounds,\n    constraints=constraints,\n    options={'disp': False}\n)\n\nrp_weights = result.x\n\n# --- Display results ---\nprint(\"Optimal Risk Parity Portfolio Weights:\")\nfor i, wt in enumerate(rp_weights):\n    if wt > 0.01:\n        print(f\"  - {etf_symbols[i]}: {wt:.1%}\")\n\n\n# --- 4f. Hierarchical Risk Parity (HRP) ---\nprint(\"\\n--- Hierarchical Risk Parity Portfolio Optimization ---\")\n\n# --- Helper: Correlation to distance ---\ndef correl_dist(corr):\n    return np.sqrt(0.5 * (1 - corr))\n\n# --- Helper: Get cluster variance ---\ndef get_cluster_var(cov, cluster_indices):\n    sub_cov = cov[np.ix_(cluster_indices, cluster_indices)]\n    inv_var_weights = 1.0 / np.diag(sub_cov)\n    inv_var_weights /= inv_var_weights.sum()\n    cluster_var = inv_var_weights @ sub_cov @ inv_var_weights\n    return cluster_var\n\n# --- Recursive bisection for HRP weights ---\ndef recursive_bisect(cov, sort_ix):\n    weights = pd.Series(1.0, index=sort_ix)\n    clusters = [sort_ix]\n\n    while clusters:\n        cluster = clusters.pop(0)\n        if len(cluster) <= 1:\n            continue\n        split = int(len(cluster) / 2)\n        left = cluster[:split]\n        right = cluster[split:]\n\n        var_left = get_cluster_var(cov, left)\n        var_right = get_cluster_var(cov, right)\n        alpha = 1.0 - var_left / (var_left + var_right)\n\n        weights[left] *= alpha\n        weights[right] *= (1.0 - alpha)\n        clusters += [left, right]\n\n    return weights.sort_index()\n\n# --- Step 1: Correlation, distance, linkage ---\ncorr = returns_monthly.corr()\ndist = correl_dist(corr)\nlink = linkage(squareform(dist), method='single')\n\n# --- Step 2: Seriation (quasi-diagonalization) ---\nsort_ix = leaves_list(link)\nsorted_tickers = [etf_symbols[i] for i in sort_ix]\n\n# --- Step 3: Covariance matrix ---\ncov = returns_monthly[sorted_tickers].cov().values * 12  # annualize\n\n# --- Step 4: Compute HRP weights ---\nhrp_weights_series = recursive_bisect(cov, np.arange(len(sorted_tickers)))\n\n# Map weights to original ETF symbol order\nhrp_weights = np.zeros(len(etf_symbols))\nfor i, ticker in enumerate(sorted_tickers):\n    original_idx = etf_symbols.index(ticker)\n    hrp_weights[original_idx] = hrp_weights_series[i]\n\n# --- Display ---\nprint(\"Optimal HRP Portfolio Weights:\")\nfor i, wt in enumerate(hrp_weights):\n    if wt > 0.01:\n        print(f\"  - {etf_symbols[i]}: {wt:.1%}\")\n\n# --- Optional: Dendrogram visualization ---\ndef plot_dendrogram(link, labels):\n    plt.figure(figsize=(10, 4))\n    dendrogram(link, labels=labels, leaf_rotation=90)\n    plt.title(\"HRP Asset Clustering (Dendrogram)\")\n    plt.tight_layout()\n    plt.show()\n\nplot_dendrogram(link, [etf_symbols[i] for i in sort_ix])\n\n# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SECTION 5: REGIME-SWITCHING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\nprint(\"\\n--- Section 5: Building a Market Regime-Switching Model ---\")\n\n# The market does not behave uniformly; it switches between different states or \"regimes\".\n# We will build a model to identify these regimes based on the market's own behavior\n# (using VOO returns) and external economic indicators.\n\n# --- 5a. Load Exogenous Economic Data ---\n# We use VIX (volatility index) and US Treasury yields as indicators of the\n# broader economic environment.\nstart_date = returns_monthly.index.min()\nend_date = returns_monthly.index.max()\n\ndef get_yield_curve(start, end):\n    \"\"\"Fetches US Treasury yield data from FRED.\"\"\"\n    print(\"Fetching yield curve data...\")\n    symbols = {'3M': \"DGS3MO\", '10Y': \"DGS10\"}\n    df = pd.DataFrame()\n    for label, fred_code in symbols.items():\n        try:\n            data = DataReader(fred_code, 'fred', start, end)\n            df[label] = data[fred_code]\n        except Exception:\n            df[label] = np.nan\n    df = df / 100.0  # Convert from percent to decimal\n    df['Spread_10Y_3M'] = df['10Y'] - df['3M']\n    return df\n\n# Fetch VIX and Yield Curve data.\nyield_curve_df = get_yield_curve(start_date, end_date)\nvix_df = yf.Ticker('^VIX').history(start=start_date, end=end_date)[['Close']]\nvix_df.rename(columns={'Close': 'VIX'}, inplace=True)\n\n# --- 5b. Align and Prepare Data for Modeling ---\n# Combine all external data and align it to our monthly return frequency.\nexog_df = pd.concat([yield_curve_df, vix_df], axis=1)\nexog_df = exog_df.resample('M').last().ffill().dropna()\n\n# Find the common date range between our ETF returns and the economic data.\ncommon_index = returns_monthly.index.intersection(exog_df.index)\nreturns_aligned = returns_monthly.loc[common_index]\nexog_aligned = exog_df.loc[common_index]\n\n# We use *lagged* economic data to predict the *next* month's regime.\n# This ensures our model is not using future information.\nexog_lagged = exog_aligned.shift(1).dropna()\n\n# Final alignment after lagging.\nfinal_index = returns_aligned.index.intersection(exog_lagged.index)\nreturns_final = returns_aligned.loc[final_index]\nexog_final_lagged = exog_lagged.loc[final_index]\n\n# The model will identify regimes based on the returns of the broad market (VOO).\nendog_voo = returns_final['VOO']\n\nprint(f\"Final dataset for regime modeling has {len(endog_voo)} monthly observations.\")\n\n# --- 5c. Fit the Markov Regime-Switching Model ---\n# We will test models with different numbers of regimes (e.g., 2, 3, 4) and\n# select the best one based on the Bayesian Information Criterion (BIC), which\n# balances model fit with model complexity.\nmodels = {}\nfor k in range(2, MAX_REGIMES_TO_TEST + 1):\n    print(f\"Fitting model with {k} regimes...\")\n    try:\n        # This model allows both the mean return and the volatility to be different in each regime.\n        # The 'exog_tvtp' allows the economic data to influence the probability of switching regimes.\n        mod = MarkovRegression(\n            endog=endog_voo,\n            k_regimes=k,\n            trend='c',  # Constant mean term\n            switching_variance=True,\n            exog_tvtp=sm.add_constant(exog_final_lagged)\n        )\n        res = mod.fit(search_reps=20) # Search for the best starting parameters\n\n        # --- Validation Check ---\n        # Ensure that each identified regime has a sufficient number of observations.\n        assigned_regimes = res.smoothed_marginal_probabilities.idxmax(axis=1)\n        counts = assigned_regimes.value_counts()\n        if (counts < MIN_OBS_PER_REGIME).any():\n            print(f\"  > Model with {k} regimes rejected: A regime had insufficient observations.\")\n            continue\n\n        models[k] = res\n        print(f\"  > Model with {k} regimes is valid. BIC: {res.bic:.2f}\")\n    except Exception as e:\n        print(f\"  > Failed to fit model with {k} regimes: {e}\")\n\nif not models:\n    raise RuntimeError(\"No suitable regime-switching models could be fitted.\")\n\n# Select the model with the lowest BIC.\nbest_k = min(models, key=lambda k: models[k].bic)\nbest_model_results = models[best_k]\nprint(f\"\\nBest model selected: {best_k} regimes (Lowest BIC = {best_model_results.bic:.2f})\")\n\n\n# --- 5d. Interpret and Label the Regimes ---\n# To make the regimes interpretable, we sort them by their volatility (sigma).\n# This gives us consistent labels, e.g., \"Regime 0\" is always the lowest volatility state.\nregime_vols = best_model_results.params.filter(like='sigma2').sort_values()\nregime_order = regime_vols.index.str.extract(r'\\[(\\d+)\\]')[0].astype(int)\nregime_map = {old_idx: new_idx for new_idx, old_idx in enumerate(regime_order)}\n\n# Display the characteristics (mean return and volatility) of each sorted regime.\nsorted_params = pd.DataFrame()\nfor i in range(best_k):\n    original_idx = regime_order.iloc[i]\n    mean = best_model_results.params[f'const[{original_idx}]'] * 12 * 100 # Annualized %\n    vol = np.sqrt(best_model_results.params[f'sigma2[{original_idx}]']) * np.sqrt(12) * 100 # Annualized %\n    sorted_params[f'Regime {i}'] = [f'{mean:.1f}%', f'{vol:.1f}%']\n\nsorted_params.index = ['Annualized Mean (VOO)', 'Annualized Volatility (VOO)']\nprint(\"\\nCharacteristics of Identified Market Regimes (Sorted by Volatility):\")\nprint(sorted_params)\n\n# Get the final, sorted series of regime probabilities and assignments.\nsmoothed_probs = best_model_results.smoothed_marginal_probabilities.rename(columns=regime_map).sort_index(axis=1)\nregime_series = smoothed_probs.idxmax(axis=1).rename('regime')\n\n# ==============================================================================\n# TVTP Analysis: Extract and Visualize Time-Varying Transition Probabilities\n# ==============================================================================\n\n# --- Transition Probability Tensor ---\n# This tensor stores the full set of time-varying transition probabilities estimated by the model.\n# Dimensions: (T, k_regimes, k_regimes)\n# For each time step t, transition_probs[t, i, j] gives the probability of moving FROM regime i TO regime j.\ntransition_probs = best_model_results.transition_matrices\n\n# --- Extract Coefficients of the Transition Models ---\n# The Markov model uses logistic regressions (logit) to model how predictors affect the probability of switching regimes.\n# These logistic regressions are estimated for each transition pair (origin \u2192 target), e.g., 0\u21921, 1\u21922, etc.\n\n# Select only the parameters that relate to the exogenous variables affecting transition probs\ntvtp_params = best_model_results.params.filter(like='exog_tvtp').copy()\n\n# Convert the parameter index (e.g., 'exog_tvtp[const] transition 0->1') into a DataFrame for easier parsing\ntvtp_coeffs = tvtp_params.reset_index()\n\n# Extract numerical identifiers for the target and origin regimes from the string index\n# Example: From 'transition 0->1' extract origin=0 and target=1\ntvtp_coeffs[['target', 'origin']] = tvtp_coeffs['index'].str.extract(r'(\\d+)->(\\d+)').astype(int)\n\n# Extract the name of the explanatory variable (e.g., 'const', 'vix', 'slope') used in the transition model\ntvtp_coeffs['variable'] = tvtp_coeffs['index'].str.extract(r'exog_tvtp\\[(.*?)\\]')\n\n# Pivot the long table into a matrix format:\n# Rows: (origin, target), Columns: predictor variables\n# This allows you to see the logistic regression coefficients for each transition\ntvtp_coeffs = tvtp_coeffs.pivot_table(index=['origin', 'target'], columns='variable', values=0)\n\n# --- Display ---\n# This prints the logistic regression coefficients that govern the transition dynamics.\n# Positive values mean the variable increases the probability of that regime transition.\nprint(\"\\nTransition Coefficients (per transition origin\u2192target):\")\nprint(tvtp_coeffs.round(3))\n\n# ==============================================================================\n# Visualization of Time-Varying Transition Probabilities into Each Regime\n# ==============================================================================\n\n# Create one subplot per regime, showing the probability of entering that regime over time\nfig, axes = plt.subplots(best_k, 1, figsize=(12, 3.5 * best_k), sharex=True)\n\n# Loop over each regime (j), plotting P(\u2192 j) \u2014 the probability of transitioning *into* regime j\nfor j in range(best_k):\n    # At each time t, take the average probability of transitioning into regime j from any other regime i\n    # transition_probs[t, i, j] is the probability of i\u2192j\n    trans_probs_to_j = pd.Series(\n        transition_probs[:, :, j].mean(axis=1),  # Average across all i for fixed j\n        index=returns_final.index               # Time index\n    )\n\n    # Plot the series for this regime\n    axes[j].plot(trans_probs_to_j, label=f'Prob(\u2192 Regime {j})', color='crimson')\n    axes[j].set_title(f'Time-Varying Transition Probability into Regime {j}')\n    axes[j].set_ylabel('Probability')\n    axes[j].legend()\n    axes[j].grid(True)\n\n# Label the x-axis once (shared across subplots)\nplt.xlabel(\"Time\")\nplt.tight_layout()\nplt.show()\n\n\n# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SECTION 6: REGIME-AWARE DYNAMIC STRATEGY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\nprint(\"\\n--- Section 6: Building and Backtesting the Dynamic Strategy ---\")\n\n# --- 6a. Calculate Regime-Specific Optimal Portfolios ---\n# Now, we compute a separate efficient frontier and optimal portfolio for each regime.\n# The idea is to hold the best possible portfolio for the current market environment.\nregime_frontiers = {}\nregime_optimal_weights = {}\n\nplt.figure(figsize=(12, 8))\ncolors = plt.cm.viridis(np.linspace(0, 1, best_k))\n\nfor i in range(best_k):\n    print(f\"\\nAnalyzing Regime {i}...\")\n    in_regime_periods = (regime_series == i)\n\n    # We need enough data points in a regime to get reliable estimates.\n    if in_regime_periods.sum() < max(12, len(etf_symbols)):\n        print(f\"  > Skipping Regime {i}, not enough data points ({in_regime_periods.sum()}).\")\n        # Fallback to a 100% VOO portfolio for this regime if we can't optimize.\n        regime_optimal_weights[i] = np.array([1.0 if s == 'VOO' else 0.0 for s in etf_symbols])\n        continue\n\n    # Estimate parameters using only the data from this regime.\n    returns_regime = returns_final[in_regime_periods]\n    mu_regime = returns_regime.mean().values * 12 - expense_vector\n    cov_regime = LedoitWolf().fit(returns_regime.values).covariance_ * 12\n\n    # Generate the efficient frontier for this regime.\n    ef_regime = efficient_frontier(cov_regime, mu_regime, n_points=FRONTIER_POINTS)\n    regime_frontiers[i] = ef_regime\n\n    # Find the optimal portfolio by matching the overall VOO benchmark's volatility.\n    _, w_opt = select_portfolio(ef_regime, 'sigma', voo_sigma_annual)\n\n    if w_opt is not None:\n        regime_optimal_weights[i] = w_opt\n        print(f\"  > Optimal Portfolio for Regime {i} (matching VOO vol):\")\n        top_indices = np.argsort(w_opt)[-3:][::-1]\n        for idx in top_indices:\n            if w_opt[idx] > 0.01:\n                print(f\"    - {etf_symbols[idx]}: {w_opt[idx]:.1%}\")\n\n        # Plot the frontier and the selected optimal point.\n        plt.plot(ef_regime['sigma'], ef_regime['mu'], label=f'Regime {i} Frontier', color=colors[i], lw=2)\n        opt_sigma = np.sqrt(w_opt @ cov_regime @ w_opt)\n        opt_mu = w_opt @ mu_regime\n        plt.scatter(opt_sigma, opt_mu, marker='*', s=250, color=colors[i], zorder=5, edgecolors='black')\n    else:\n        print(\"  > Could not find an optimal portfolio. Using VOO as fallback.\")\n        regime_optimal_weights[i] = np.array([1.0 if s == 'VOO' else 0.0 for s in etf_symbols])\n\n# Finalize and show the plot of all regime frontiers.\nplt.scatter([voo_sigma_annual], [voo_mu_annual], color='black', marker='X', s=200, label='VOO (Overall)', zorder=5)\nplt.title('Efficient Frontiers for Each Market Regime')\nplt.xlabel('Annualized Volatility (Sigma)')\nplt.ylabel('Annualized Return (Mu)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# --- 6b. Backtest the Dynamic Strategy ---\n# At each month, our portfolio is a blend of the regime-optimal portfolios,\n# weighted by the probability of being in each regime at that time.\ndynamic_weights = []\nfor t in range(len(returns_final)):\n    # Get the smoothed probabilities for this time step.\n    probs_t = smoothed_probs.iloc[t]\n    blended_w = np.zeros(len(etf_symbols))\n    # Create the blended portfolio.\n    for i in range(best_k):\n        if i in regime_optimal_weights:\n            blended_w += probs_t[i] * regime_optimal_weights[i]\n    # Normalize weights to ensure they sum to 1.\n    blended_w /= blended_w.sum()\n    dynamic_weights.append(blended_w)\n\n# Calculate the monthly returns of this dynamic portfolio.\ndynamic_port_returns = np.sum(np.array(dynamic_weights) * returns_final[etf_symbols].values, axis=1)\ndynamic_returns_series = pd.Series(dynamic_port_returns, index=returns_final.index)\n\n\n# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SECTION 7: FINAL PERFORMANCE COMPARISON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\nprint(\"\\n--- Section 7: Comparing All Strategies ---\")\n\n# --- 7a. Define a Performance Metrics Calculator ---\ndef calculate_metrics(returns_series):\n    \"\"\"\n    Calculates key performance metrics for a series of returns.\n    \"\"\"\n    # Total Cumulative Return\n    cumulative_returns = (1 + returns_series).cumprod()\n    total_return = (cumulative_returns.iloc[-1] - 1) * 100\n\n    # Annualized Return (Geometric)\n    ann_return = ((1 + returns_series.mean()) ** 12 - 1) * 100\n\n    # Annualized Volatility\n    ann_vol = returns_series.std() * np.sqrt(12) * 100\n\n    # Sharpe Ratio (assumes risk-free rate is 0)\n    # Measures risk-adjusted return. Higher is better.\n    sharpe = (ann_return / ann_vol) if ann_vol > 0 else np.nan\n\n    # Maximum Drawdown\n    # The largest peak-to-trough drop in portfolio value. A measure of downside risk.\n    peak = cumulative_returns.expanding(min_periods=1).max()\n    drawdown = (cumulative_returns / peak - 1) * 100\n    max_drawdown = drawdown.min()\n\n    return {\n        \"Total Return (%)\": total_return,\n        \"Annualized Return (%)\": ann_return,\n        \"Annualized Volatility (%)\": ann_vol,\n        \"Sharpe Ratio\": sharpe,\n        \"Max Drawdown (%)\": max_drawdown\n    }\n\n# --- 7b. Prepare All Strategy Returns for Comparison ---\n# We need to calculate the historical returns for each static portfolio to compare\n# them against the dynamic strategy and the VOO benchmark.\nstrategies = {\n    'VOO Benchmark': returns_final['VOO'],\n    'Static Raw (Sigma-Match)': (returns_final[etf_symbols] @ w_sigma_raw) if w_sigma_raw is not None else pd.Series(np.nan, index=returns_final.index),\n    'Static Shrunk (Sigma-Match)': (returns_final[etf_symbols] @ w_sigma_shrunk) if w_sigma_shrunk is not None else pd.Series(np.nan, index=returns_final.index),\n    'Static Resampled': (returns_final[etf_symbols] @ w_resampled) if w_resampled is not None else pd.Series(np.nan, index=returns_final.index),\n    'Black-Litterman': (returns_final[etf_symbols] @ w_bl_opt) if 'w_bl_opt' in globals() else pd.Series(np.nan, index=returns_final.index),\n    'Risk Parity Optimization': (returns_final[etf_symbols] @ rp_weights) if 'rp_weights' in globals() else pd.Series(np.nan, index=returns_final.index),\n    'Hierarchical Risk Parity': (returns_final[etf_symbols] @ hrp_weights) if 'hrp_weights' in globals() else pd.Series(np.nan, index=returns_final.index),\n    'Dynamic Regime Strategy': dynamic_returns_series\n}\n\n# --- 7c. Interactive Strategy Comparison Dashboard ---\ndef plot_strategy_comparison(selected_strategies):\n    plt.figure(figsize=(14, 8))\n    for name in selected_strategies:\n        series = strategies[name].dropna()\n        (1 + series).cumprod().plot(label=name, lw=2)\n\n    plt.title('Cumulative Performance Comparison of Selected Strategies', fontsize=16)\n    plt.xlabel('Date')\n    plt.ylabel('Growth of $1')\n    plt.yscale('log')\n    plt.legend(loc='upper left')\n    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n    plt.tight_layout()\n    plt.show()\n\n    display(all_perf_df.loc[selected_strategies])\n\nmulti_select = widgets.SelectMultiple(\n    options=list(strategies.keys()),\n    value=('VOO Benchmark', 'Black-Litterman', 'Hierarchical Risk Parity'),\n    description='Strategies:',\n    rows=7,\n    layout=widgets.Layout(width='50%')\n)\n\ninteractive_output = widgets.interactive_output(plot_strategy_comparison, {'selected_strategies': multi_select})\n\nprint(\"\\nSelect strategies to visualize interactively:\")\ndisplay(widgets.HBox([multi_select]), interactive_output)\n\n# --- 7d. Plot Cumulative Performance of All Strategies ---\nplt.figure(figsize=(14, 8))\nfor name, returns in strategies.items():\n    if not returns.isnull().all():\n        (1 + returns).cumprod().plot(label=name, lw=2)\n\nplt.title('Cumulative Performance Comparison of All Strategies', fontsize=16)\nplt.xlabel('Date')\nplt.ylabel('Growth of $1')\nplt.yscale('log') # Log scale is useful for comparing long-term growth rates.\nplt.legend(loc='upper left')\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\nplt.tight_layout()\nplt.show()\n\n# --- 7e. Display Final Performance Metrics Table ---\nall_perf_metrics = {\n    name: calculate_metrics(returns.dropna())\n    for name, returns in strategies.items()\n}\nall_perf_df = pd.DataFrame(all_perf_metrics).T\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"      COMPREHENSIVE STRATEGY PERFORMANCE METRICS\")\nprint(\"=\"*50)\nprint(all_perf_df)\nprint(\"=\"*50 + \"\\n\")\n\n# --- 7e. Visualize Regime Probabilities vs. Market Returns ---\nfig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True, gridspec_kw={'height_ratios': [3, 2]})\nsmoothed_probs.plot(ax=axes[0], kind='area', stacked=True, colormap='viridis', alpha=0.8)\naxes[0].set_title('Smoothed Probabilities of Each Market Regime Over Time', fontsize=14)\naxes[0].set_ylabel('Probability')\naxes[0].legend(title='Regime', loc='upper left')\naxes[0].grid(True, linestyle='--', alpha=0.5)\n\nreturns_final['VOO'].plot(ax=axes[1], color='black', label='VOO Monthly Return')\naxes[1].set_title('VOO Monthly Returns', fontsize=14)\naxes[1].set_ylabel('Return')\naxes[1].axhline(0, color='grey', lw=1)\naxes[1].grid(True, linestyle='--', alpha=0.5)\n\nplt.xlabel(\"Date\")\nplt.tight_layout()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}